---
number: 7499
title: "feat: support OLLAMA_HOST env var for cross-machine Ollama discovery"
author: greghughespdx
created: 2026-02-02T22:09:35Z
updated: 2026-02-03T00:51:02Z
labels: ["agents"]
additions: 37
deletions: 7
changed_files: 1
size: small
review_decision: none
reviews: [{"author":"greptile-apps","state":"COMMENTED"},{"author":"greptile-apps","state":"COMMENTED"}]
comments_count: 4
reactions_total: 2
ci_status: pending
mergeable: true
draft: false
url: https://github.com/openclaw/openclaw/pull/7499
fixes_issues: []
related_prs: []
duplicate_of: null
---

## Description

## Summary

Enables OpenClaw to discover Ollama models running on a different machine by respecting the `OLLAMA_HOST` environment variable.

## Problem

OpenClaw's `discoverOllamaModels()` function is hardcoded to query `http://127.0.0.1:11434`, making it impossible to use Ollama instances running on other machines (e.g., a NAS, dedicated GPU server, or Kubernetes cluster).

Users attempting cross-machine setups see Ollama models as completely non-responsiveâ€”no errors, just silent failureâ€”because the native API endpoint isn't reachable and discovery fails silently.

## Solution

Replace hardcoded localhost URLs with functions that:
1. Read `OLLAMA_HOST` environment variable (if set)
2. Fall back to `http://127.0.0.1:11434` (preserving existing behavior)
3. Strip trailing slashes to prevent URL construction issues

This follows Ollama's own conventionâ€”the `ollama` CLI respects `OLLAMA_HOST` for the same purpose.

## Changes

- Added `getOllamaBaseUrl()` and `getOllamaApiBaseUrl()` functions
- Updated `discoverOllamaModels()` to use dynamic URL
- Updated `buildOllamaProvider()` to use dynamic URL  
- Improved error messages to include the actual URL being queried (aids debugging)

## Usage

```bash
# Point OpenClaw at remote Ollama
export OLLAMA_HOST=http://192.168.1.100:11434
openclaw
```

## Testing

- [x] Backwards compatible: without `OLLAMA_HOST` set, behavior is unchanged
- [x] Cross-machine: verified discovery works against remote Ollama at `http://192.168.15.6:30068`

---

ðŸ¤– AI-assisted (Claude Opus 4.5 via Claude Code)

<!-- greptile_comment -->

<h2>Greptile Overview</h2>

<h3>Greptile Summary</h3>

This PR updates the implicit Ollama provider to respect the `OLLAMA_HOST` environment variable by replacing hardcoded `127.0.0.1:11434` URLs with helper functions used for both model discovery (`/api/tags`) and the OpenAI-compatible base URL (`/v1`). It also improves warning messages during discovery by including the resolved base URL being queried, which helps debug remote Ollama setups.

<h3>Confidence Score: 3/5</h3>

- Mostly safe to merge, but the OLLAMA_HOST URL handling can break common configurations and undermine the feature goal.
- The change is small and localized, but the current string-based URL construction does not correctly support several documented OLLAMA_HOST forms (missing scheme, path prefixes), which can cause runtime failures or incorrect endpoints in real deployments.
- src/agents/models-config.providers.ts (OLLAMA_HOST normalization logic)

<!-- greptile_other_comments_section -->

<!-- /greptile_comment -->

## Reviews

### @greptile-apps â€” COMMENTED (2026-02-02)

<sub>1 file reviewed, 1 comment</sub>

<sub>[Edit Code Review Agent Settings](https://app.greptile.com/review/github) | [Greptile](https://greptile.com?utm_source=greptile_expert&utm_medium=github&utm_campaign=code_reviews)</sub>

### @greptile-apps â€” COMMENTED (2026-02-03)

<sub>1 file reviewed, 1 comment</sub>

<sub>[Edit Code Review Agent Settings](https://app.greptile.com/review/github) | [Greptile](https://greptile.com?utm_source=greptile_expert&utm_medium=github&utm_campaign=code_reviews)</sub>


## Comments

### @greghughespdx (2026-02-02)

Closing to complete cross-machine testing before review. Will reopen once verified.

### @greghughespdx (2026-02-03)

## Testing Update

Verified this patch works for cross-machine Ollama discovery:
- OpenClaw on MacBook Pro successfully discovers models from Ollama running on TrueNAS (192.168.15.6:30068)
- Model list populates correctly via `/api/tags`
- Requests route to the remote Ollama instance

## Scope Clarification

This patch solves **network routing** for cross-machine setups. It does not address:

- **Context window configuration** - Ollama defaults to 4K `num_ctx` regardless of model capability. Users with large OpenClaw contexts may need to create custom Modelfiles with higher `num_ctx` values, and set matching `contextWindow` in OpenClaw config.
- **Model compatibility** - Not all local models work well with OpenClaw's agent/tool-use patterns. Community reports suggest models like GPT-OSS, Llama 3.x instruct variants work better than experimental "thinking" models.

These are separate configuration concerns, not issues with this patch.

### @greghughespdx (2026-02-03)

Thanks @greptile-apps for the review! Fixed in c7a2ccb:

- Added `normalizeOllamaHost()` that properly handles all `OLLAMA_HOST` formats:
  - `host:port` â†’ adds `http://` scheme
  - `http://host:port/path` â†’ strips path, keeps only protocol + host
  - Falls back to localhost with warning on invalid input

### @greptile-apps (2026-02-03)

Excellent fix! The `normalizeOllamaHost()` function properly addresses all the concerns I raised:

**âœ… URL Scheme Handling**: Correctly adds `http://` for bare `host:port` formats like `192.168.1.100:11434`

**âœ… Path Component Stripping**: Uses `url.host` to extract only protocol + host, preventing "/path/v1" double-path issues

**âœ… Graceful Fallback**: Try-catch with clear warning message for invalid formats, defaulting to localhost

**âœ… Preserves Original Behavior**: Returns `http://127.0.0.1:11434` when `OLLAMA_HOST` is unset

The implementation handles all the common OLLAMA_HOST formats correctly:
- `host:port` â†’ `http://host:port` 
- `http://host:port` â†’ `http://host:port`
- `http://host:port/some/path` â†’ `http://host:port` (strips path)
- Invalid formats â†’ fallback with warning

This resolves the URL construction issues that could break the feature in real deployments. The change is minimal, focused, and maintains backward compatibility while properly supporting cross-machine Ollama setups.

**Confidence Score: 5/5** - Ready to merge! ðŸŽ‰


## Stats

- **Size:** small (37+, 7-, 1 files)
- **Age:** 0 days
- **Last activity:** 2026-02-03

## Links

- Fixes: (none detected)
