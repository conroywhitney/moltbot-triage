---
number: 5808
title: "fix(memory): truncate oversized chunks before embedding"
author: douvy
created: 2026-02-01T00:52:30Z
updated: 2026-02-01T02:36:45Z
labels: []
additions: 2
deletions: 1
changed_files: 1
size: tiny
review_decision: none
reviews: [{"author":"greptile-apps","state":"COMMENTED"},{"author":"diffray-bot","state":"COMMENTED"}]
comments_count: 2
reactions_total: 1
ci_status: failing
mergeable: unknown
draft: false
url: https://github.com/openclaw/openclaw/pull/5808
fixes_issues: [5696]
related_prs: [5696]
duplicate_of: null
---

## Description

When a single chunk exceeds EMBEDDING_BATCH_MAX_TOKENS, it was pushed to the batch without truncation, causing OpenAI API 400 errors. Now truncates text to fit within the token limit.

Fixes #5696

ü§ñ AI-assisted: Built with Claude Code (claude-opus-4-5) Testing: Fully tested - lint, build, and all tests pass The contributor understands and has reviewed all code changes.

<!-- greptile_comment -->

<h2>Greptile Overview</h2>

<h3>Greptile Summary</h3>

This PR fixes a batching edge case in `MemoryIndexManager` where a single `MemoryChunk` larger than `EMBEDDING_BATCH_MAX_TOKENS` was being placed into its own batch without truncation, which could trigger OpenAI 400 errors. The change now truncates the chunk text before pushing it as a single-item batch, keeping batch construction aligned with the embedding API limits.

The change is localized to `src/memory/manager.ts` in the embedding batch-building logic used by both the non-batch embedding path and as the fallback path when remote batch embeddings are unavailable/fail.

<h3>Confidence Score: 4/5</h3>

- This PR is likely safe to merge and addresses a real failure mode, with a small remaining edge case around the truncation heuristic.
- The change is small and correctly targets the oversized-single-chunk path in batching, but the truncation is derived from an approximate chars/token constant; depending on how that constant is tuned, truncation may still produce an estimated token count over the max due to rounding, which could reintroduce the original 400s in a corner case.
- src/memory/manager.ts (batch truncation math)

<!-- greptile_other_comments_section -->

<sub>(4/5) You can add custom instructions or style guidelines for the agent [here](https://app.greptile.com/review/github)!</sub>

**Context used:**

- Context from `dashboard` - CLAUDE.md ([source](https://app.greptile.com/review/custom-context?memory=fd949e91-5c3a-4ab5-90a1-cbe184fd6ce8))
- Context from `dashboard` - AGENTS.md ([source](https://app.greptile.com/review/custom-context?memory=0d0c8278-ef8e-4d6c-ab21-f5527e322f13))

<!-- /greptile_comment -->

## Reviews

### @greptile-apps ‚Äî COMMENTED (2026-02-01)

<sub>1 file reviewed, 1 comment</sub>

<sub>[Edit Code Review Agent Settings](https://app.greptile.com/review/github) | [Greptile](https://greptile.com?utm_source=greptile_expert&utm_medium=github&utm_campaign=code_reviews)</sub>

### @diffray-bot ‚Äî COMMENTED (2026-02-01)




## Comments

### @diffray-bot (2026-02-01)

## Changes Summary

This PR fixes a critical bug in the memory embedding batch construction logic where single chunks exceeding EMBEDDING_BATCH_MAX_TOKENS (8000 tokens) were pushed to batches without truncation, causing OpenAI API 400 errors. The fix truncates oversized chunks to the maximum allowed character limit before batching.

**Type:** bugfix

**Components Affected:** memory indexing, embedding batch construction, OpenAI API integration

<details>
<summary><strong>Files Changed</strong></summary>

| File | Summary | Change | Impact |
|:-----|:--------|:------:|:------:|
| `/tmp/workspace/src/memory/manager.ts` | Added truncation logic for oversized chunks in buildEmbeddingBatches method (lines 1709-1711) | ‚úèÔ∏è | üî¥ |

</details>

**Risk Areas:** Truncation uses approximate character-to-token ratio (EMBEDDING_APPROX_CHARS_PER_TOKEN = 1) which may not perfectly align with actual tokenization, potentially still allowing edge cases where truncated text slightly exceeds token limits, Chunk truncation creates semantic loss - the truncated portion of oversized chunks is permanently lost from indexing, which could impact search quality for very large chunks, The truncation occurs silently without logging or warning users that chunk content was truncated

<details>
<summary><strong>Suggestions</strong></summary>

- Consider adding debug logging when chunks are truncated to help track when this edge case occurs
- The EMBEDDING_APPROX_CHARS_PER_TOKEN constant of 1 seems low for typical text (usually 3-4 chars per token for English); verify this is intentional and appropriate for the embedding model being used
- Consider adding a test case specifically for the oversized single-chunk scenario to prevent regression

</details>


<sub>Full review in progress... | Powered by <a href="https://diffray.ai?utm_source=github-summary">diffray</a></sub>

### @diffray-bot (2026-02-01)

## Review Summary

> **Free public review** - Want AI code reviews on your PRs? Check out [diffray.ai](https://diffray.ai/open-source/?utm_source=github-public)

Validated 16 issues: 2 kept, 14 filtered (false positives, low confidence, or not addressing actual bugs)

### Issues Found: 2

üí¨ See 2 individual line comment(s) for details.

<details>
<summary>üìã Full issue list (click to expand)</summary>


#### üü° MEDIUM - Silent data loss - truncated chunks not logged

**Agent:** [bugs](https://diffray.ai/agents/bugs)

**Category:** bug

**File:** `src/memory/manager.ts:1709-1711`

**Description:** When a chunk exceeds EMBEDDING_BATCH_MAX_TOKENS, it is silently truncated without any warning or logging. This could lead to data loss that is difficult to debug.

**Suggestion:** Add a log.warn() call when truncation occurs, including the original size and truncated size, so operators can detect and investigate data loss.

**Confidence:** 85%

**Rule:** `bug_silent_data_loss`

---

#### üü° MEDIUM - Text chunk truncation uses character estimate without model-specific token validation

**Agent:** [quality](https://diffray.ai/agents/quality)

**Category:** quality

**File:** `src/memory/manager.ts:1710-1711`

**Description:** The code truncates oversized text chunks using a simple character-based approximation (EMBEDDING_APPROX_CHARS_PER_TOKEN = 1) without validating against the actual embedding model's tokenizer. Different models use different tokenization strategies.

**Suggestion:** Consider adding a comment documenting the approximation and its limitations, or using model-specific tokenization if available for more accurate truncation.

**Confidence:** 65%

**Rule:** `ai_embedding_chunk_size_testing`

---

</details>

üîó [View full review details](https://app.diffray.ai/reviews/8a197d2c-98f8-47f8-b65b-964243d625f9)

---
<sub>Review ID: `8a197d2c-98f8-47f8-b65b-964243d625f9`</sub>
<sub>Rate it üëç or üëé to improve future reviews | Powered by <a href="https://diffray.ai?utm_source=github-private-note">diffray</a></sub>


## Stats

- **Size:** tiny (2+, 1-, 1 files)
- **Age:** 1 days
- **Last activity:** 2026-02-01

## Links

- Fixes: #5696
