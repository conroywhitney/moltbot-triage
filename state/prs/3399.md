---
number: 3399
title: "feat(memory-lancedb): support custom embedding endpoints"
author: mike-nott
created: 2026-01-28T15:06:33Z
updated: 2026-01-28T23:25:09Z
labels: ["extensions: memory-lancedb"]
additions: 64
deletions: 15
changed_files: 3
size: medium
review_decision: none
reviews: []
comments_count: 0
reactions_total: 0
ci_status: failing
mergeable: unknown
draft: false
url: https://github.com/openclaw/openclaw/pull/3399
fixes_issues: []
related_prs: []
duplicate_of: null
---

## Description

## Summary

Add support for self-hosted OpenAI-compatible embedding servers in memory-lancedb.

## Changes

- Add `embedding.baseUrl` config option for custom endpoint URL
- Add `embedding.dimensions` config option to override vector dimensions  
- Remove model enum restriction to allow any model name
- Update Embeddings class to pass baseURL to OpenAI client

## Motivation

Many users want to run local embedding models instead of using OpenAI API:
- Privacy: keep data local
- Cost: avoid API charges for high-volume use
- Latency: local inference can be faster
- Flexibility: use specialized models

This enables servers like:
- llama.cpp with `--embedding` flag
- Hugging Face text-embeddings-inference
- Any OpenAI-compatible embedding endpoint

## Example Config

```json
{
  "embedding": {
    "apiKey": "not-needed",
    "baseUrl": "http://localhost:8080/v1",
    "model": "Qwen3-Embedding-8B",
    "dimensions": 4096
  }
}
```

## Testing

Tested with:
- llama.cpp running Qwen3-Embedding-8B (4096 dims)
- Successfully indexes and recalls memories
- Works with autoCapture and autoRecall enabled

## Reviews


## Comments


## Stats

- **Size:** medium (64+, 15-, 3 files)
- **Age:** 1 days
- **Last activity:** 2026-01-28

## Links

- Fixes: (none detected)
