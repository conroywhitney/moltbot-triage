---
number: 7278
title: "feat(ollama): optimize local LLM support with auto-discovery and timeouts"
author: alltomatos
created: 2026-02-02T16:40:52Z
updated: 2026-02-02T17:19:32Z
labels: ["docs", "docker", "agents"]
additions: 156
deletions: 35
changed_files: 10
size: medium
review_decision: none
reviews: [{"author":"greptile-apps","state":"COMMENTED"}]
comments_count: 1
reactions_total: 2
ci_status: failing
mergeable: false
draft: false
url: https://github.com/openclaw/openclaw/pull/7278
fixes_issues: []
related_prs: []
duplicate_of: null
---

## Description

## Summary
Optimizes the Ollama provider integration to better support local LLM workflows without requiring manual configuration or API keys.

## Changes
- **Auto-discovery**: Automatically detects local Ollama instances running on default ports (`11434`) without requiring `OLLAMA_API_KEY`.
- **Performance**: Eliminated double network calls during model discovery.
- **Reliability**: Increased discovery timeout from 5s to 10s to accommodate slower local model loading.
- **Configuration**: Added support for `OLLAMA_HOST` and `OLLAMA_BASE_URL` environment variables to override defaults.
- **UX**: Added silent failure mode for unconfigured instances to reduce console noise.

## Testing
- Added unit tests for auto-discovery logic (with and without API keys).
- Validated `OLLAMA_HOST` environment variable overrides.
- Verified that existing tests pass.

## Checklist
- [x] Code matches the existing style.
- [x] Tests added/updated.
- [x] Documentation updated.

<!-- greptile_comment -->

<h2>Greptile Overview</h2>

<h3>Greptile Summary</h3>

This PR improves the Ollama provider experience by enabling implicit provider auto-discovery against a local Ollama instance (defaulting to `127.0.0.1:11434`), adding env overrides (`OLLAMA_HOST`, `OLLAMA_BASE_URL`), increasing discovery timeouts, and avoiding duplicate discovery calls when building the provider config. It also adds unit tests for the new discovery behavior and updates docs and docker examples to better support local-model workflows.

Key interactions: `src/agents/models-config.providers.ts` now probes Ollama during `resolveImplicitProviders()` and conditionally adds the `ollama` provider when either an API key is present or models are discovered; docs and docker config changes aim to make local Ollama use easier out of the box.

<h3>Confidence Score: 3/5</h3>

- This PR is likely safe to merge, but it has a few correctness/documentation mismatches and a misleading runtime version message that should be fixed first.
- Core Ollama changes are straightforward and covered by new tests, but there are a couple of user-facing correctness issues (Node version guard message mismatch, docs contradicting new behavior) and some brittleness/noise in auto-discovery and tests that could lead to confusing behavior or flaky CI.
- src/infra/runtime-guard.ts, src/agents/models-config.providers.ts, docs/providers/ollama.md, src/agents/models-config.providers.ollama.test.ts

<!-- greptile_other_comments_section -->

<sub>(2/5) Greptile learns from your feedback when you react with thumbs up/down!</sub>

<!-- /greptile_comment -->

## Reviews

### @greptile-apps — COMMENTED (2026-02-02)

<sub>5 files reviewed, 7 comments</sub>

<sub>[Edit Code Review Agent Settings](https://app.greptile.com/review/github) | [Greptile](https://greptile.com?utm_source=greptile_expert&utm_medium=github&utm_campaign=code_reviews)</sub>


## Comments

### @greptile-apps (2026-02-02)

<details>
<summary>Additional Comments (2)</summary>

**`src/agents/models-config.providers.ts`**
[P1] Auto-discovery is intended to be silent unless explicitly configured, but the “no models found” path always logs `console.warn("No Ollama models found on local instance")` (`src/agents/models-config.providers.ts:107-109`). This still produces console noise for users without Ollama running/loaded (and contradicts the silent-failure behavior in the catch block). Consider making this warning conditional on explicit configuration too.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/agents/models-config.providers.ts
Line: 107:110

Comment:
[P1] Auto-discovery is intended to be silent unless explicitly configured, but the “no models found” path always logs `console.warn("No Ollama models found on local instance")` (`src/agents/models-config.providers.ts:107-109`). This still produces console noise for users without Ollama running/loaded (and contradicts the silent-failure behavior in the catch block). Consider making this warning conditional on explicit configuration too.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>

---

**`docs/providers/ollama.md`**
[P1] Docs are now inconsistent with the implementation: troubleshooting still says you must set `OLLAMA_API_KEY` and that only tool-capable models are auto-discovered (`docs/providers/ollama.md:178-195`), but the code now auto-discovers without a key and does not filter by tool support. This will send users down the wrong path when Ollama isn’t detected or when models don’t show up.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: docs/providers/ollama.md
Line: 176:195

Comment:
[P1] Docs are now inconsistent with the implementation: troubleshooting still says you must set `OLLAMA_API_KEY` and that only tool-capable models are auto-discovered (`docs/providers/ollama.md:178-195`), but the code now auto-discovers without a key and does not filter by tool support. This will send users down the wrong path when Ollama isn’t detected or when models don’t show up.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>


## Stats

- **Size:** medium (156+, 35-, 10 files)
- **Age:** 0 days
- **Last activity:** 2026-02-02

## Links

- Fixes: (none detected)
