---
number: 2353
title: "fix: ensure api field is set for inline provider models"
author: sbknana
created: 2026-01-26T19:44:18Z
updated: 2026-01-28T23:25:12Z
labels: []
additions: 7
deletions: 1
changed_files: 1
size: tiny
review_decision: none
reviews: []
comments_count: 0
reactions_total: 1
ci_status: failing
mergeable: unknown
draft: false
url: https://github.com/moltbot/moltbot/pull/2353
fixes_issues: []
related_prs: []
duplicate_of: null
---

## Description

## Summary

When a model is found in the inline provider config (`models.providers.*.models`), the `api` field was not being set, causing `"Unhandled API in mapOptionsForApi: undefined"` errors when using custom OpenAI-compatible providers like Ollama or LM Studio.

This fix ensures the `api` field is inherited from:
1. The model config itself (if specified)
2. The provider config's `api` field  
3. Falls back to `"openai-responses"` (consistent with the fallback model behavior on line 73)

## Problem

When configuring a custom provider like this:
```json
{
  "models": {
    "providers": {
      "openai": {
        "baseUrl": "http://localhost:1234/v1",
        "apiKey": "lm-studio",
        "models": [
          {
            "id": "qwen/qwen3-vl-8b",
            "name": "Qwen3 VL 8B",
            "contextWindow": 16384
          }
        ]
      }
    }
  }
}
```

The model resolution would find `inlineMatch` but return it without an `api` field, causing the pi-ai library's `mapOptionsForApi` function to throw an error.

## Solution

Added logic to ensure the `api` field is set when returning an inline model match, mirroring the existing fallback behavior.

## Test plan

- [x] Tested with LM Studio as OpenAI-compatible backend
- [x] Verified Discord channel receives responses correctly after fix

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

## Reviews


## Comments


## Stats

- **Size:** tiny (7+, 1-, 1 files)
- **Age:** 2 days
- **Last activity:** 2026-01-28

## Links

- Fixes: (none detected)
