---
number: 4628
title: "feat(memory-lancedb): add local embedding support via node-llama-cpp"
author: solofberlin
created: 2026-01-30T12:23:07Z
updated: 2026-01-30T12:23:19Z
labels: ["extensions: memory-lancedb"]
additions: 298
deletions: 31
changed_files: 4
size: large
review_decision: none
reviews: []
comments_count: 0
reactions_total: 0
ci_status: passing
mergeable: true
draft: false
url: https://github.com/openclaw/openclaw/pull/4628
fixes_issues: []
related_prs: []
duplicate_of: null
---

## Description

Adds support for running embeddings locally using node-llama-cpp, eliminating the need for OpenAI API keys when using the memory-lancedb plugin.

Changes:
- Add embedding.provider config: 'openai' | 'local' (default: 'openai')
- Make embedding.apiKey optional when using local provider
- Add embedding.local.modelPath for custom GGUF model paths
- Add embedding.local.modelCacheDir for model caching
- Default local model: embeddinggemma-300M-Q8_0.gguf (768-dim vectors)
- Add node-llama-cpp as optional peer dependency
- Port local embedding logic from core memorySearch implementation
- Maintain backwards compatibility (existing configs work unchanged)

Usage:
  embedding: provider: local # Optional custom model: local: modelPath: hf:ggml-org/embeddinggemma-300M-GGUF/embeddinggemma-300M-Q8_0.gguf

## Reviews


## Comments


## Stats

- **Size:** large (298+, 31-, 4 files)
- **Age:** 0 days
- **Last activity:** 2026-01-30

## Links

- Fixes: (none detected)
