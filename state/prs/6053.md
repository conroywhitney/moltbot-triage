---
number: 6053
title: "fix: use 400K context window instead of 200K if the model allows (gpt-5.2)"
author: icedac
created: 2026-02-01T07:05:48Z
updated: 2026-02-01T07:19:12Z
labels: ["agents"]
additions: 16
deletions: 2
changed_files: 3
size: small
review_decision: none
reviews: [{"author":"greptile-apps","state":"COMMENTED"}]
comments_count: 1
reactions_total: 2
ci_status: failing
mergeable: unknown
draft: false
url: https://github.com/openclaw/openclaw/pull/6053
fixes_issues: []
related_prs: []
duplicate_of: null
---

## Description

## Summary
- OpenAI Codex models (`openai-codex/gpt-5.2`) have 400k context windows
- Sessions were auto-clearing at ~200k because `lookupContextTokens()` returned `undefined` for prefixed model IDs
- Added static fallback lookup using `MODEL_CONTEXT_WINDOWS` from `opencode-zen-models.ts`

## Changes
- Export `MODEL_CONTEXT_WINDOWS` from `opencode-zen-models.ts`
- Add static fallback in `lookupContextTokens()` for cache misses
- Handle prefixed model IDs by extracting bare model ID (e.g., `openai-codex/gpt-5.2` ‚Üí `gpt-5.2`)

## Test plan
- [x] `pnpm build` passes
- [x] `pnpm test` passes (4924 tests)
- [x] `pnpm lint` passes for changed files
- [ ] Manual: verify `openai-codex/gpt-5.2` sessions don't clear at 200k

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

<!-- greptile_comment -->

<h2>Greptile Overview</h2>

<h3>Greptile Summary</h3>

This PR fixes incorrect context window inference for OpenAI Codex model refs like `openai-codex/gpt-5.2` by adding a static fallback lookup in `lookupContextTokens()` when the dynamic models cache misses. It also exports `MODEL_CONTEXT_WINDOWS` from the OpenCode Zen model catalog so that context window defaults can be reused, and updates the xhigh-thinking allowlist to include the new `openai-codex/gpt-5.2` ref.

Overall, the change is consistent with the existing ‚Äúbest-effort cache, then fallback‚Äù approach used in model discovery, and should prevent sessions from auto-clearing early when the configured model id includes a provider prefix.

<h3>Confidence Score: 4/5</h3>

- This PR is low-risk and likely safe to merge, with a minor normalization edge case to consider.
- Changes are small, covered by existing test suite per the PR description, and primarily add a fallback read path. The main residual risk is that model IDs may include more complex prefixes/suffixes than a single `/` segment, in which case the fallback may still miss.
- src/agents/context.ts

<!-- greptile_other_comments_section -->

<!-- /greptile_comment -->

## Reviews

### @greptile-apps ‚Äî COMMENTED (2026-02-01)

<sub>1 file reviewed, 1 comment</sub>

<sub>[Edit Code Review Agent Settings](https://app.greptile.com/review/github) | [Greptile](https://greptile.com?utm_source=greptile_expert&utm_medium=github&utm_campaign=code_reviews)</sub>


## Comments

### @icedac (2026-02-01)

Thanks for the review!

Regarding the `:tag` suffix concern ‚Äî I searched the codebase and found no usage of model IDs with `:tag` or `@rev` suffixes (e.g., `gpt-5.2:latest`). The current pattern used is `provider/model-id` (e.g., `openai-codex/gpt-5.2`).

The implementation using `split("/").pop()` already handles multiple path segments correctly:
- `openai-codex/gpt-5.2` ‚Üí `gpt-5.2` ‚úÖ
- `provider/openai-codex/gpt-5.2` ‚Üí `gpt-5.2` ‚úÖ

Since `:tag` suffixes aren't used in this codebase, adding that handling would be premature (YAGNI). If we add tagged model references in the future, we can extend the normalization then.


## Stats

- **Size:** small (16+, 2-, 3 files)
- **Age:** 1 days
- **Last activity:** 2026-02-01

## Links

- Fixes: (none detected)
