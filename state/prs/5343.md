---
number: 5343
title: "fix(memoryFlush): fix prompt token tracking for flush threshold"
author: jarvis-medmatic
created: 2026-01-31T11:08:47Z
updated: 2026-02-02T13:43:34Z
labels: []
additions: 477
deletions: 14
changed_files: 4
size: large
review_decision: none
reviews: [{"author":"greptile-apps","state":"COMMENTED"}]
comments_count: 2
reactions_total: 1
ci_status: pending
mergeable: unknown
draft: false
url: https://github.com/openclaw/openclaw/pull/5343
fixes_issues: [4836]
related_prs: [4836]
duplicate_of: null
---

## Description

Fixes #4836

## Summary

The memory flush feature was rarely triggering because `totalTokens` was not being tracked correctly. The session entry's `totalTokens` was often undefined or stale, causing the threshold check to always fail.

## Changes

- **Token estimation**: Use `estimateMessagesTokens()` to estimate current prompt tokens
- **Transcript fallback**: Read last usage entry from session log when session entry lacks data
- **Include output tokens**: Total now includes output tokens (`input + cacheRead + cacheWrite + output`)
- **agentId derivation**: Properly derive agentId when sessionEntry lacks sessionFile/transcriptPath
- **Performance**: Gate transcript reading behind missing `baseTotalTokens`
- **Async I/O**: Use `fs.promises.readFile` instead of sync read
- **Simplified helpers**: Removed redundant code, cleaner token tracking logic

## Testing

```bash
npx vitest run --config vitest.unit.config.ts memory-flush
# 28 tests pass âœ…
```

**Note:** Unit tests only - not tested with a full development instance.

---

## AI Disclosure ðŸ¤–

- **AI-assisted**: Yes - developed with Claude (Opus) and OpenAI Codex
- **Degree of testing**: Unit tests only (28 pass) - not tested with full dev instance
- **Human oversight**: [@ManuelHettich](https://github.com/ManuelHettich) reviewed all changes
- **Understanding**: Yes - we understand the token tracking logic and how it integrates with the compaction system

Co-authored-by: Jarvis <jarvis@medmatic.ai>

## Reviews

### @greptile-apps â€” COMMENTED (2026-01-31)

<sub>2 files reviewed, 2 comments</sub>

<sub>[Edit Code Review Agent Settings](https://app.greptile.com/review/github) | [Greptile](https://greptile.com?utm_source=greptile_expert&utm_medium=github&utm_campaign=code_reviews)</sub>


## Comments

### @greptile-apps (2026-01-31)

<details>
<summary>Additional Comments (1)</summary>

**`src/auto-reply/reply/session-usage.ts`**
[P0] `totalTokens` is now set to `promptTokens` (input + cacheRead + cacheWrite), which drops `output` tokens even though the PR description says total should include output. That will undercount usage and can keep memory flush from triggering on long responses.

This looks like it should be `input + output + cacheRead + cacheWrite` (or use `params.usage.total` if that already includes all components).

Also appears in `src/auto-reply/reply/agent-runner-memory.ts` where `readPromptTokensFromSessionLog()` computes `totalTokens` as `inputTokens + outputTokens` but `inputTokens` is derived from prompt-only usage.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/auto-reply/reply/session-usage.ts
Line: 41:49

Comment:
[P0] `totalTokens` is now set to `promptTokens` (input + cacheRead + cacheWrite), which drops `output` tokens even though the PR description says total should include output. That will undercount usage and can keep memory flush from triggering on long responses.

This looks like it should be `input + output + cacheRead + cacheWrite` (or use `params.usage.total` if that already includes all components).

Also appears in `src/auto-reply/reply/agent-runner-memory.ts` where `readPromptTokensFromSessionLog()` computes `totalTokens` as `inputTokens + outputTokens` but `inputTokens` is derived from prompt-only usage.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>

### @jarvis-medmatic (2026-01-31)

Thanks for the review! This was already addressed in commit a8cf37a1a (`fix(session-usage): include output tokens in totalTokens calculation`).

The current formula is:
```javascript
const promptTokens = input + cacheRead + cacheWrite;
totalTokens: promptTokens > 0 ? promptTokens + output : (params.usage?.total ?? input + output)
```

So `totalTokens = input + cacheRead + cacheWrite + output` â€” output tokens are included.


## Stats

- **Size:** large (477+, 14-, 4 files)
- **Age:** 2 days
- **Last activity:** 2026-02-02

## Links

- Fixes: #4836
