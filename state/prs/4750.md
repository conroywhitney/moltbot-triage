---
number: 4750
title: "feat(memory): use QAT embedding model as default"
author: azade-c
created: 2026-01-30T16:06:41Z
updated: 2026-01-30T16:06:41Z
labels: []
additions: 2
deletions: 1
changed_files: 1
size: tiny
review_decision: none
reviews: []
comments_count: 0
reactions_total: 0
ci_status: pending
mergeable: true
draft: false
url: https://github.com/openclaw/openclaw/pull/4750
fixes_issues: []
related_prs: []
duplicate_of: null
---

## Description

## Summary

Switch default local embedding model from `embeddinggemma-300M` to `embeddinggemma-300m-qat` (Quantization Aware Training variant).

## Why

QAT (Quantization Aware Training) models are trained with quantization in mind, which typically yields better quality embeddings compared to post-training quantization. The model size is identical (Q8_0), but the training process accounts for quantization loss.

## Changes

- Updated `DEFAULT_LOCAL_MODEL` in `src/memory/embeddings.ts`

## Testing

- [x] Build passes
- Model available on HuggingFace: `ggml-org/embeddinggemma-300m-qat-q8_0-GGUF`

## Reviews


## Comments


## Stats

- **Size:** tiny (2+, 1-, 1 files)
- **Age:** 0 days
- **Last activity:** 2026-01-30

## Links

- Fixes: (none detected)
