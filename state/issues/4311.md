---
number: 4311
title: "[Critical] Config Self-Mutation Bug + Safe Network Change Process"
author: wdi-dave-roberts
created: 2026-01-30T01:58:59Z
updated: 2026-01-31T02:32:30Z
labels: ["bug"]
assignees: []
comments_count: 15
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/4311
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Summary

**Critical reliability issue:** Clawdbot can self-mutate its config file, causing unexpected gateway crashes. This creates business continuity risk for production deployments.

## Timeline of Events (2026-01-29)

1. **Port conflict**: Gateway crash loop on 18789 (kernel TIME-WAIT state)
2. **Emergency port change**: Moved to 18790 to restore service  
3. **Config mutations**: Direct JSON edits triggered background process that rewrote config
4. **System instability**: Gateway crashed multiple times due to invalid config values
5. **Business impact**: Unreliable remote access to critical business systems

## The Config Self-Mutation Bug

**Symptom:** Config file gets rewritten by background Clawdbot process, overriding manual changes.

**Dangerous sequence:**
- User makes rapid config changes (bypassing proper API)
- Background validation/reconciliation process detects 'inconsistency'  
- Process overwrites manual config with computed values
- Computed values may be invalid format, causing crash loop

**Business risk:** A working config can spontaneously become invalid, causing service outages.

## Safe Network Change Process (Needed)

**Current gap:** No documented safe process for network configuration changes that could disrupt business operations.

**Requirements:**
- Backup/rollback capability
- Validation at each step  
- Proper use of clawdbot config API (not direct JSON edits)
- Business continuity considerations

## Immediate Action

We're about to attempt port reversion (18790 → 18789) using ultra-conservative process with:
- Pre-flight safety checks
- Step-by-step validation
- Full rollback plan
- Documentation of lessons learned

## Request

1. **Fix the config mutation bug** - background processes should not overwrite user config
2. **Document safe network change procedures** - especially for production deployments  
3. **Add config file locking** or validation to prevent this failure mode
4. **Improve error messages** when config conflicts occur

## Business Context

This isn't just a technical issue - it's about trust. Businesses depend on reliable systems. Self-mutating configs that cause surprise outages make AI assistants a liability instead of an asset.

For businesses running critical operations through Clawdbot, config reliability is essential.

## Comments

### @wdi-dave-roberts (2026-01-30)

Starting port reversion process (18790 → 18789) - conducting pre-flight checks

### @wdi-dave-roberts (2026-01-30)

Pre-flight complete - all systems healthy, backups created.

- Port 18789: clear (no listeners)
- Gateway: running, pid 8622, RPC ok on 18790
- Backups: `clawdbot.json.backup-20260129-210234`, `clawdbot-gateway.service.backup-20260129-210234`
- Tailscale: proxying to 127.0.0.1:18790

Beginning execution sequence.

### @wdi-dave-roberts (2026-01-30)

Service restarted on port 18789 - beginning validation

### @wdi-dave-roberts (2026-01-30)

**ROLLBACK INITIATED** - Port 18789 still exhibits the canvas self-conflict bug (Issue 1 from diagnostic log).

The gateway's canvas sub-server binds to 18789, then the startup check connects to that port and detects "another gateway instance." This is a clawdbot v2026.1.24-3 bug, NOT a TIME-WAIT issue — it persists 3+ hours later.

Rolling back to port 18790.

### @wdi-dave-roberts (2026-01-30)

**ROLLBACK COMPLETE** - System restored to working port 18790 configuration.

- Gateway: running, pid 10477, RPC ok
- Port: 18790, bind=loopback
- Tailscale: proxying to 127.0.0.1:18790
- Config: locked (chmod 444)

## Findings

Port 18789 is **not usable** with clawdbot v2026.1.24-3 on this system. The bug is not TIME-WAIT related — it persists 3+ hours after the original incident. The canvas sub-server binds to the port before the gateway's startup conflict check, causing it to detect itself as "another instance."

**Recommendation:** Stay on port 18790 until clawdbot upstream fixes the canvas/gateway startup race condition. Update CLAUDE.md to reflect port 18790 as the standard.

### @ZhongHaohua (2026-01-30)

你好，我已收到邮件

### @wdi-dave-roberts (2026-01-30)

Starting Tailscale interference test - capturing pre-test state

### @wdi-dave-roberts (2026-01-30)

**Pre-test state captured:**
- Tailscale: proxying to 127.0.0.1:18790
- Gateway: running pid 10941, RPC ok on 18790
- Port 18789: clear (nothing listening)

Isolating Tailscale to dummy port.

### @wdi-dave-roberts (2026-01-30)

Tailscale isolated to dummy port 19999 - proceeding with 18789 test (direct launch, no service change)

### @wdi-dave-roberts (2026-01-30)

**New finding:** Direct launch failed with a different error:
```
gateway already running (pid 10948); lock timeout after 5000ms
```

The running production gateway on 18790 holds a **process lock** that blocks any second instance regardless of port. The self-conflict errors on 18789 may have been this lock all along — not a canvas race condition.

Need to stop production gateway briefly to test 18789 in isolation. Will restore immediately after.

### @wdi-dave-roberts (2026-01-30)

**Tailscale interference hypothesis: REJECTED**

## Test Results

| Port | Tailscale | Production GW | Result |
|------|-----------|---------------|--------|
| 18789 | Isolated (dummy port) | Stopped | **FAIL** — canvas self-conflict |
| 18790 | Isolated (dummy port) | Stopped | **PASS** — starts normally |
| 18791 | Isolated (dummy port) | Stopped | **PASS** — starts normally |

All three tests ran with identical conditions: same binary, same config (except port), Tailscale pointed at dummy port 19999, no production gateway running.

## Additional Finding

When the production gateway was still running and we tried a direct launch on 18789, we got a **different** error: `gateway already running (pid 10948); lock timeout after 5000ms`. This means the original crash-loop errors during the port reversion (when systemd was restarting the service) were likely **lock contention from rapid restarts**, not just the canvas issue.

However, when tested completely clean (no other gateway, no Tailscale), port 18789 still fails with the canvas self-conflict. **Something is uniquely wrong with port 18789 on this system.**

## Root Cause: Still Unknown

Not Tailscale. Not TIME-WAIT. Not lock contention. Not any visible listener (ss shows nothing). Port 18789 is specifically broken while 18790 and 18791 work fine with identical conditions.

Possible remaining explanations:
- Kernel-level socket state not visible to unprivileged `ss` (need root/lsof)
- iptables/nftables rule affecting 18789 specifically
- WSL2 port forwarding reservation from Windows side

## Production State Restored
- Gateway: running pid 11821, RPC ok on 18790
- Tailscale: proxying to 127.0.0.1:18790
- Config: locked (chmod 444)

Test complete.

### @wdi-dave-roberts (2026-01-30)

**Handoff to Windows-side Claude Code**

WSL2 investigation exhausted. Port 18789 is uniquely broken — not Tailscale, not TIME-WAIT, not lock contention, not any visible listener. Suspect Hyper-V excluded port range.

Handoff doc committed: `docs/windows-port-check.md`

---

**Prompt for Windows-side Claude Code:**

```
Pull the latest from the clawdbot repo and read docs/windows-port-check.md. Run the three Windows PowerShell diagnostic commands listed there, paste the output into the file replacing the command blocks, and follow the "What to do with results" decision tree. Commit and push your findings so the WSL2 side can pick them up. Update GitHub issue https://github.com/moltbot/moltbot/issues/4311 with what you find.
```

### @wdi-dave-roberts (2026-01-30)

## Root Cause Found — Stale Windows Port Forwarding Rule

Ran the Windows-side diagnostics from `docs/windows-port-check.md`. Results:

1. **Hyper-V excluded port ranges:** Port 18789 is NOT in any excluded range — ruled out.
2. **Port forwarding (portproxy):** A stale `netsh interface portproxy` rule was forwarding `0.0.0.0:18789 → 127.0.0.1:18789`. This caused `svchost.exe` (Windows IP Helper service, PID 3412) to hold the port, making it unavailable to WSL2.
3. **netstat:** Confirmed `svchost.exe` was LISTENING on 18789.

### Resolution

Removed the stale rule:
```powershell
netsh interface portproxy delete v4tov4 listenport=18789 listenaddress=0.0.0.0
```

Port 18789 is now free. **Next step:** Retest gateway binding from WSL2.

Full diagnostics committed to whitedoeinn/clawdbot@d40f9fa.


### @wdi-dave-roberts (2026-01-30)

## Root Cause Confirmed — Stale Windows portproxy Rule

Windows-side investigation (d40f9fa) found the root cause: a persistent `netsh interface portproxy` rule was forwarding `0.0.0.0:18789 → 127.0.0.1:18789`. This caused Windows IP Helper service (`svchost.exe`, PID 3412) to bind the port at the OS level, making it unavailable to WSL2.

Key insight: `netsh interface portproxy` rules are **registry-persistent** — they survive reboots. A Windows reboot would NOT have fixed this; only explicitly deleting the rule resolves it.

Rule has been removed. Port 18789 is now free for WSL2 retest.

### Full investigation chain
1. Port 18789 canvas self-conflict → moved to 18790 (workaround)
2. Reid agent config mutation → chmod 444 lockdown (mitigation)
3. Tailscale interference hypothesis → rejected by controlled test
4. Hyper-V excluded port range → ruled out by Windows diagnostics
5. **Stale portproxy rule** → confirmed root cause, removed

### Next steps
- Retest port 18789 from WSL2
- If successful, decide whether to revert to 18789 or stay on 18790

### @wdi-dave-roberts (2026-01-30)

## Separate Issue: Recurring `TypeError: fetch failed` crashes

While investigating the port issue, discovered a separate crash-loop bug. Gateway crashes every 60-90 seconds with:

```
Unhandled promise rejection: TypeError: fetch failed
    at node:internal/deps/undici/undici:15845:13
    at processTicksAndRejections (node:internal/process/task_queues:103:5)
```

Systemd auto-restarts it, but agent state is lost each time. This is why the bot appears unresponsive — it's either mid-crash or freshly restarted with no context.

### Investigation findings

- Network is fine (Anthropic API, Telegram API, npm registry all reachable)
- The stack trace has zero context — no URL, no subsystem, just undici internals
- The binary (v2026.1.24-3) contains 39 hardcoded `clawd.bot` URLs and fetches `registry.npmjs.org/clawdbot`
- The fetch wrapper (`infra/fetch.js`) has no error handling — it passes through raw
- Some internal periodic fetch (Telegram polling, heartbeat, or plugin check) is failing without a `.catch()`, which kills the process via unhandled promise rejection

Filing separate bug.


## Links

- None detected yet
