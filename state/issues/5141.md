---
number: 5141
title: "[Bug]: use vision model  in ollama will cause respone very long."
author: acer1204
created: 2026-01-31T03:34:45Z
updated: 2026-01-31T03:34:45Z
labels: ["bug"]
assignees: []
comments_count: 0
reactions_total: 1
url: https://github.com/openclaw/openclaw/issues/5141
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Summary
What went wrong?

## Steps to reproduce
1. setup the opebclaw in ollama config
2.use qwen3 vl 32B
3.use line to pass a image

## Expected behavior
What did you expect to happen?
every ai response return very long (about 10mins)
but use not vision model is good. like gpt-oss.

## Actual behavior
What actually happened?

## Environment
- Clawdbot version: newest
- OS: ubuntu 22.04
- Install method (pnpm/npx/docker/etc): curl

## Comments


## Links

- None detected yet
