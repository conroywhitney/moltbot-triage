---
number: 2977
title: "codex-cli backend responses never received, messages disappear"
author: vulcan2018
created: 2026-01-27T21:46:19Z
updated: 2026-01-29T18:49:51Z
labels: ["bug"]
assignees: []
comments_count: 2
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/2977
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

# Bug: codex-cli backend responses never received, messages disappear

## Summary

When using `codex-cli` as the primary model provider, chat messages disappear after being sent. The `cli exec` is logged but no completion/response is ever received. The WebSocket connection eventually times out (code 1006) and the message vanishes from the UI.

## Environment

- **Clawdbot version:** 2026.1.24-3
- **OS:** Ubuntu/WSL2 (Linux 6.6.87.2-microsoft-standard-WSL2)
- **Node:** v22.22.0 (via nvm)
- **codex-cli version:** 0.91.0
- **Install method:** npm global

## Configuration

```json
{
  "agents": {
    "defaults": {
      "model": {
        "primary": "codex-cli/gpt-5.1-codex-max"
      },
      "cliBackends": {
        "codex-cli": {
          "command": "codex",
          "args": ["exec", "--json", "--full-auto"],
          "resumeArgs": ["exec", "resume", "{sessionId}"],
          "output": "jsonl"
        }
      }
    }
  }
}
```

## Steps to Reproduce

1. Configure clawdbot with `codex-cli/gpt-5.1-codex-max` as primary model
2. Authenticate codex via `codex auth login`
3. Verify auth is synced (check `auth-profiles.json` shows valid `openai-codex:codex-cli`)
4. Open webchat at `http://127.0.0.1:18789/`
5. Send any message (e.g., "hello")
6. Observe message appears briefly then disappears after ~1-2 minutes

## Expected Behavior

- Message is sent to codex-cli
- Response is received and displayed in chat
- Conversation persists

## Actual Behavior

- Message is sent, `cli exec` is logged
- **No completion event is ever logged**
- WebSocket disconnects with code 1006 after timeout
- Message disappears from chat history

## Log Evidence

```
21:34:42 cli exec: provider=codex-cli model=gpt-5.1-codex-max promptChars=63
21:34:55 chat.history 191ms  (UI polling)
21:35:15 chat.history 1395ms (UI polling)
21:35:26 chat.history 370ms  (UI polling)
21:35:31 webchat disconnected code=1006 reason=n/a
```

Note: There is NO `cli completed`, `cli response`, or similar event between `cli exec` and the disconnect.

## Verification: codex-cli Works Standalone

Running the same command manually works perfectly:

```bash
$ echo "say hello" | codex exec --json --full-auto
Reading prompt from stdin...
{"type":"thread.started","thread_id":"019c0163-472c-7fe1-9b24-3be36c4d94af"}
{"type":"turn.started"}
{"type":"item.completed","item":{"id":"item_0","type":"reasoning","text":"**Saying hello**\n\nHi there!"}}
{"type":"item.completed","item":{"id":"item_1","type":"agent_message","text":"Hello!"}}
{"type":"turn.completed","usage":{"input_tokens":7937,"cached_input_tokens":6912,"output_tokens":82}}
```

This confirms the codex CLI itself works and authentication is valid.

## Root Cause Hypothesis

The issue appears to be in how clawdbot spawns and reads from the codex-cli process. Possible causes:

1. **stdout buffering issue** - output may not be flushed/received properly
2. **JSONL parsing issue** - the response format may not match what clawdbot expects
3. **Process spawn issue on WSL/Linux** - pipe handling differences
4. **Missing stdin closure** - codex waits for EOF on stdin

## Workaround

Switch to `anthropic/claude-sonnet-4-5` as the primary model (which uses different backend).

## Related

- The `anthropic:claude-cli` backend works correctly with the same setup
- Issue may be specific to the `codex-cli` cliBackend integration

## Additional Context

- OAuth token was initially expired, causing similar symptoms
- After re-authentication (`codex auth login`), token is valid but issue persists
- Auth profile shows: `"expires": "2026-01-27T22:30:13.276Z"` (valid at time of testing)

## Comments

### @vulcan2018 (2026-01-28)

Has anyone successfully used codex (via cli in wsl) as main language for moltbot? Claude thinks it's a moltbot bug...

### @vulcan2018 (2026-01-29)

## Fix: 3 patches + config change (confirmed working)

I hit this exact issue and traced it through the source. There are **two bugs** in how the webchat path handles CLI agent backends, plus a config problem with `resumeArgs`. Here's the full fix.

---

### Root Cause

**Issue 1 — No `assistant` stream events from CLI agents.**
The webchat response delivery relies on `assistant` stream events to populate a per-run buffer. When `lifecycle:end` fires, `emitChatFinal()` reads that buffer and broadcasts to WebSocket clients. CLI agents (codex-cli) never emit these events — the runner calls `codex exec --json --full-auto` synchronously and collects output at once. So the buffer is empty and the final broadcast sends `{ state: "final", message: undefined }`.

**Issue 2 — CLI agent responses not persisted to transcript.**
The webchat client ignores the message content in the "final" payload. Instead, it clears streaming state and re-fetches `chat.history` from the server, which reads from a JSONL transcript file. The embedded PI agent writes to this transcript during its run. The CLI agent does not — neither the user message nor the assistant response are persisted. So the history fetch returns empty and the message vanishes.

This is why WhatsApp works fine (completely different delivery path that doesn't depend on stream events or transcript persistence).

---

### Config Fix: `resumeArgs`

Your config has:
```json
"resumeArgs": ["exec", "resume", "{sessionId}"]
```

This is broken — it lacks `--json` and `--full-auto`, so codex enters TUI mode headlessly and hangs. Change to:
```json
"resumeArgs": []
```

Also clear any stale `cliSessionIds` from `sessions.json`.

---

### Patch 1: Emit `assistant` event for CLI agents

**File:** `dist/auto-reply/reply/agent-runner-execution.js`

In the `.then((result) => {...})` handler after `runCliAgent()` returns, add this **before** `lifecycle:end` is emitted:

```javascript
if (result.payloads?.length) {
    const cliText = result.payloads
        .map(p => (typeof p === "string" ? p : p?.text))
        .filter(Boolean)
        .join("\n");
    if (cliText) {
        emitAgentEvent({
            runId,
            stream: "assistant",
            data: { text: cliText },
        });
    }
}
// Then lifecycle:end is emitted as before
```

This populates the chat buffer so `emitChatFinal()` sends actual content.

---

### Patch 2: Persist user + assistant messages to transcript

**File:** `dist/gateway/server-methods/chat.js`

In the `.then()` handler of `dispatchInboundMessage()`, when `agentRunStarted` is true and `combinedReply` has content, add persistence logic:

```javascript
else if (combinedReply) {
    const { storePath: latestStorePath, entry: latestEntry } = loadSessionEntry(p.sessionKey);
    const sessionId = latestEntry?.sessionId ?? entry?.sessionId ?? clientRunId;
    const transcriptPath = resolveTranscriptPath({
        sessionId,
        storePath: latestStorePath,
        sessionFile: latestEntry?.sessionFile,
    });
    if (transcriptPath) {
        // Ensure header exists for 0-byte transcript files
        try {
            const stat = fs.statSync(transcriptPath);
            if (stat.size === 0) {
                const header = {
                    type: "session",
                    version: CURRENT_SESSION_VERSION,
                    id: sessionId,
                    timestamp: new Date().toISOString(),
                    cwd: process.cwd(),
                };
                fs.writeFileSync(transcriptPath, `${JSON.stringify(header)}\n`, "utf-8");
            }
        } catch {}
        // Persist user message
        const userEntry = {
            type: "message",
            id: randomUUID().slice(0, 8),
            timestamp: new Date(now).toISOString(),
            message: {
                role: "user",
                content: [{ type: "text", text: parsedMessage }],
                timestamp: now,
            },
        };
        try {
            fs.appendFileSync(transcriptPath, `${JSON.stringify(userEntry)}\n`, "utf-8");
        } catch {}
    }
    // Persist assistant response
    appendAssistantTranscriptMessage({
        message: combinedReply,
        sessionId,
        storePath: latestStorePath,
        sessionFile: latestEntry?.sessionFile,
        createIfMissing: true,
    });
}
```

This ensures the webchat client's `chat.history` re-fetch finds both messages.

---

### Patch 3: Emit `assistant` event in agent command path

**File:** `dist/commands/agent.js` (~line 380)

Same pattern as Patch 1 but for the `agentCommand()` code path (used by the bridge handler). Belt-and-suspenders — add the same `emitAgentEvent` block after CLI agent payloads are collected.

---

### Why the timing works

The `.then()` handler runs as a microtask after `dispatchInboundMessage` resolves. `lifecycle:end` triggers the first "final" broadcast (frame written to OS buffer). The client receives it, processes it, and sends `chat.history` RPC (I/O event). The microtask (transcript persistence) always completes before the I/O callback processes the history request.

---

### Important note

All 3 patches modify files in `node_modules/.../dist/`. They will be lost on `npm update`. The core issue is that the webchat path assumes agents write to the gateway transcript and emit assistant stream events — CLI agents do neither.

A proper upstream fix should have `runCliAgent()` persist results and emit stream events itself, or have the `chat.send` handler always persist for non-PI agents.


## Links

- None detected yet
