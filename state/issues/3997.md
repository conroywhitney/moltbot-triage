---
number: 3997
title: "[Bug] LM Studio local model processes requests but doesn't send responses to Telegram"
author: mictsi1975
created: 2026-01-29T13:58:43Z
updated: 2026-01-29T13:58:43Z
labels: ["bug"]
assignees: []
comments_count: 0
reactions_total: 0
url: https://github.com/moltbot/moltbot/issues/3997
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description


### Description
When using LM Studio as the primary model provider, Moltbot/Clawdbot receives Telegram messages and successfully sends requests to LM Studio, but the responses are never sent back to Telegram.

### Environment
- **Moltbot/Clawdbot version**: 2026.1.24-3
- **Moltbot host**: Ubuntu 24.04 LTS (Linux PC)
- **LM Studio version**: 0.4.0 (Build 17)
- **LM Studio host**: Windows 11 (separate PC on same network)
- **Network**: Both PCs on same LAN (192.168.1.x)
- **Node.js version**: 24.13.0
- **Model**: qwen2.5-coder-7b-instruct

**Note**: LM Studio runs on a Windows PC (192.168.1.159) while Moltbot/Clawdbot runs on a separate Linux PC. Both are on the same local network.

**Important**: Ollama also runs on the **same Windows PC** (192.168.1.159) as LM Studio, and Ollama works perfectly with Moltbot. This proves the network configuration is correct and the issue is specific to LM Studio integration.

### Configuration
```json
{
  "models": {
    "providers": {
      "lmstudio": {
        "baseUrl": "http://192.168.1.159:1234/v1",
        "apiKey": "lmstudio",
        "api": "openai-responses",
        "models": [{"id": "qwen2.5-coder-7b-instruct", "name": "LM Studio Local"}]
      }
    }
  },
  "agents": {
    "defaults": {
      "model": {"primary": "lmstudio/qwen2.5-coder-7b-instruct"}
    }
  }
}
```

### Steps to Reproduce
1. Configure Moltbot with LM Studio as primary model
2. Start the gateway: `clawdbot daemon start`
3. Send a message to the Telegram bot
4. Observe: Telegram shows "typing..." but no response is sent

### Expected Behavior
The bot should respond with the LM Studio model's output in Telegram.

### Actual Behavior
- LM Studio receives the request (confirmed via Developer Logs)
- Prompt processing completes (0% â†’ 100%)
- No token generation occurs
- No response is sent to Telegram
- Run completes in ~500-700ms (too fast for actual generation)

### LM Studio Developer Logs
```
[WARN] [qwen2.5-coder-7b-instruct] The provided unsupported field(s) prompt_cache_key are not supported and will be ignored.
[INFO] [qwen2.5-coder-7b-instruct] Streaming response...
[INFO] [qwen2.5-coder-7b-instruct] Prompt processing progress: 0.0%
[INFO] [qwen2.5-coder-7b-instruct] Prompt processing progress: 100.0%
```
Note: No token generation logs appear after prompt processing.

### Moltbot Logs
```
embedded run start: provider=lmstudio model=qwen2.5-coder-7b-instruct
embedded run agent start
embedded run agent end
embedded run done: durationMs=538 aborted=false
```
Note: Run completes very quickly without errors.

### Verification: Direct API Call Works
```bash
curl -s http://192.168.1.159:1234/v1/responses \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen2.5-coder-7b-instruct", "input": "Say hello"}' | jq '.output[0].content[0].text'
# Returns: "Hello! How can I assist you today?"
```

### Workaround
Using **Ollama** instead of LM Studio works correctly with the same configuration:
```json
"primary": "ollama/qwen2.5-coder-32k:7b"
```

### Additional Context
- Both providers use `api: "openai-responses"`
- LM Studio's `/v1/responses` endpoint works correctly with direct curl
- The issue appears to be in how Moltbot handles LM Studio's response format

## Comments


## Links

- None detected yet
