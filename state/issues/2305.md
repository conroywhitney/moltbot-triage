---
number: 2305
title: "Feature Request: Custom baseUrl for model providers (OpenAI-compatible proxies)"
author: zaNevar-dev
created: 2026-01-26T17:36:55Z
updated: 2026-01-28T06:02:58Z
labels: ["enhancement"]
assignees: []
comments_count: 0
reactions_total: 10
url: https://github.com/openclaw/openclaw/issues/2305
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Summary
I'd like to route model requests through a local LiteLLM proxy for smart model routing (local Ollama â†’ Gemini â†’ Claude), but the current `openai` provider doesn't respect custom `baseUrl` configuration.

## Use Case
- Run LiteLLM proxy locally on port 4000
- Route family/work agents through it for cost savings
- LiteLLM handles fallbacks: Local Ollama GPUs â†’ Gemini free tier â†’ Claude premium
- Main agent stays on Claude for quality

## Current Behavior
Setting `OPENAI_BASE_URL` environment variable or adding `models.providers.openai.baseUrl` in config is ignored. The openai provider always goes to `api.openai.com`.

## Desired Behavior
Allow configuring a custom `baseUrl` for the `openai` provider (or any provider), so requests can be routed through local proxies like LiteLLM, Ollama, or other OpenAI-compatible endpoints.

## Proposed Config
```json5
{
  models: {
    providers: {
      openai: {
        baseUrl: "http://localhost:4000/v1",  // LiteLLM proxy
        apiKey: "sk-local-key"
      }
    }
  }
}
```

Or per-agent:
```json5
{
  agents: {
    list: [{
      id: "family",
      model: {
        primary: "openai/gpt-4o",
        providerConfig: {
          baseUrl: "http://localhost:4000/v1"
        }
      }
    }]
  }
}
```

## Environment
- Clawdbot version: 2026.1.23-1
- LiteLLM proxy running with Ollama backend (3 local GPUs)

## Workarounds Attempted
1. Setting `OPENAI_BASE_URL` in systemd service - ignored
2. Adding `models.providers.openai.baseUrl` in config - invalid config error
3. Using `models.providers.custom.baseUrl` - only works for tools, not model providers

Thanks for considering! This would enable significant cost savings by routing bulk/simple requests to local models. ðŸ¦Š

## Comments


## Links

- None detected yet
