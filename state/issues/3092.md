---
number: 3092
title: "Session lock timeout causes channel handler failures during long operations"
author: delonsp
created: 2026-01-28T02:36:28Z
updated: 2026-02-01T12:52:29Z
labels: ["bug"]
assignees: []
comments_count: 5
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/3092
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Description

Channel handlers (especially Telegram) frequently fail with lock timeout when the agent is processing any long operation (exec commands, HTTP requests, tool calls, etc):

```
error gateway/channels/telegram handler failed: Error: timeout acquiring session store lock: /root/.clawdbot/agents/main/sessions/sessions.json.lock
```

This happens even with a single channel active - it's not specific to multi-channel scenarios.

## Steps to Reproduce

1. Start a conversation that triggers a long operation (e.g., slow exec command, HTTP request that hangs)
2. While processing, send another message from the same or different channel
3. The new message fails with lock timeout

## Expected Behavior

Messages should queue and process sequentially, or the lock acquisition should have a longer timeout with retries.

## Actual Behavior

The channel handler fails immediately when it can't acquire the lock within the timeout period. The user sees no response and has to retry.

## Environment

- Clawdbot version: 2026.1.24-3
- OS: Linux (Ubuntu)
- Channels: Telegram, WhatsApp
- Agent config: maxConcurrent: 4

## Current Workaround

Cron job to clean stale locks every 2 minutes:
```bash
*/2 * * * * find /root/.clawdbot/agents/main/sessions -name "*.lock" -mmin +2 -delete
```

This helps recover from stuck locks but doesn't prevent the failures.

## Suggestions

1. Increase lock acquisition timeout for channel handlers
2. Implement automatic retry with backoff
3. Use a message queue instead of failing immediately
4. Consider async/non-blocking lock mechanism
5. Separate lock scopes for different operations (read vs write)

## Comments

### @nicolasmertens (2026-01-29)

## Additional Analysis: RPC Timeout During Session Processing

We're experiencing the same issue on our multi-agent setup (Clawdbot 2026.1.24-0, macOS arm64, Node v25.3.0).

### Our Findings

**Root cause confirmed:** When an agent session is in "processing" state (waiting for Claude API response, which can take 2-3 minutes), the gateway cannot handle new WebSocket RPC calls within the 10-second timeout.

**Evidence from logs:**
```
lane task done: lane=main durationMs=170756 active=1 queued=0
lane task done: lane=session:agent:main:main durationMs=170761 active=0 queued=0
```
Lane tasks taking 170+ seconds (almost 3 minutes) while cron tool calls timeout after 10 seconds.

**Diagnosis:**
1. Session starts processing → holds resources
2. Cron tool (or any RPC call) tries to connect via WebSocket
3. Gateway event loop is blocked by session processing
4. RPC call times out after 10 seconds
5. Error: `gateway timeout after 10000ms`

**Key insight:** This is NOT just a file lock issue - it's event loop contention. Even when we cleared all .lock files, the timeouts continued until the session finished processing.

### Workaround Implemented

```cron
# Clawdbot stale lock cleanup (workaround for #3030/#3092)
*/2 * * * * find ~/.clawdbot/agents -name "*.lock" -mmin +2 -delete 2>/dev/null
```

This helps with stale locks but does NOT solve the RPC timeout during active session processing.

### Suggested Fix

The gateway should handle RPC calls (especially cron management) on a separate path that doesn't wait for session processing to complete. Cron operations don't need the session lock - they operate on the cron store, not session state.

Related: #2654, #3030, #3992

### @itsahedge (2026-01-30)

## Additional Evidence: Discord Channel with Stuck Sessions

We experienced the same root cause on Discord channels (Clawdbot on macOS arm64, Node v25.4.0).

### Symptoms

**Stuck sessions:**
```
[diagnostic] stuck session: sessionId=<redacted> state=processing age=515s queueDepth=0
```

**Slow Discord message processing:**
```
[discord] Slow listener detected: DiscordMessageListener took 289 seconds for event MESSAGE_CREATE
```

**Agent run timeouts:**
```
[agent/embedded] embedded run timeout: runId=<redacted> timeoutMs=600000
[agent/embedded] Profile anthropic:personal timed out (possible rate limit). Trying next account...
```

### Impact

Discord channels stopped responding to messages. Messages were being received by the gateway, but the agent sessions were stuck processing for 8+ minutes, causing:
- Message listener delays of 40-289 seconds (should be instant)
- Multiple concurrent sessions all stuck simultaneously
- Gateway had to be manually restarted to clear stuck sessions

### Diagnosis

Same root cause as described in the comments above: **Event loop contention during long session processing**. When agent sessions take 2-3+ minutes to process (waiting for Claude API), the gateway cannot handle new Discord events or RPC calls.

This manifests differently depending on the channel:
- **Telegram/WhatsApp:** Lock timeout errors (as reported above)
- **Discord:** Slow listener warnings and stuck sessions

But it's the same underlying issue: gateway event loop blocked by long-running session processing.

### Related Issues

- #3639 - Discord messages not sent after tool calls (possibly same root cause)
- #4173 - Sub-agents hang after tool results (similar stuck session pattern)
- #4410 - Feature request for auto-restart on stuck sessions (treating symptom, not cause)

Manual gateway restart cleared all stuck sessions immediately.

### @xrf9268-hue (2026-01-31)

I encountered a related issue: after an embedded run timed out (600000ms), the `.jsonl.lock` file was not cleaned up, causing all subsequent messages to be permanently blocked until manually deleted.

**Timeline:**
- `02:25:19` - Embedded agent run started, lock file created
- `02:27:55` - Typing indicator TTL reached (2m), stopped
- `02:35:19` - `[agent/embedded] embedded run timeout: runId=e75a8f75-... sessionId=3a24e781-... timeoutMs=600000`
- Lock file remained: `sessions/3a24e781-....jsonl.lock` (PID still valid, gateway process still running)
- All subsequent messages silently dropped - no error logs, just health checks passing

**Lock file content:**
```json
{
  "pid": 1256,
  "createdAt": "2026-01-31T02:25:19.694Z"
}
```

**Workaround:** `rm sessions/xxx.jsonl.lock`

This seems like the timeout handler is not properly releasing the lock when the embedded run times out.

### @omniscient (2026-02-01)

I also have had a similar problem induced by the brower relay. I was trying to train the bot on a specific website to develop a skill and it seems the it got some errors trying to locate and HTML element and got the following logs:

2026-02-01T03:00:18.912Z [tools] browser failed: Error: Error: locator.ariaSnapshot: Error: strict mode violation: locator('[class*=\'compose\'], [class*=\'message-input\'], [class*=\'toolbar\'], [role=\'toolbar\'], [class*=\'quick\']') resolved to 4 elements:

    1) <div dir="ltr" class="redactor-box redactor-blur redactor-styles-on redactor-toolbar-on">…</div> aka locator('div').filter({ hasText: /^Rich text editor$/ })

    2) <div class="redactor-toolbar-wrapper">…</div> aka locator('.redactor-toolbar-wrapper')

    3) <div class="redactor-toolbar">…</div> aka locator('.redactor-toolbar')

    4) <div dir="ltr" domtargetshow="" id="redactor-context-toolbar-0" class="redactor-context-toolbar"></div> aka locator('#redactor-context-toolbar-0')


Call log:

  - waiting for locator('[class*=\'compose\'], [class*=\'message-input\'], [class*=\'toolbar\'], [role=\'toolbar\'], [class*=\'quick\']')


2026-02-01T03:02:18.732Z typing TTL reached (2m); stopping typing indicator

2026-02-01T03:08:59.067Z [agent/embedded] embedded run timeout: runId=cc987e6a-a600- sessionId=97a54747-52d7- timeoutMs=600000 

All subsequent messages were ignored from my telegram prompts or the gateway web interface. Then 10 minutes later I got the error message in Telegram:

Browser: snapshot failed: Error: Error: locator.ariaSnapshot: Error: strict mode violation: locator('[class*='compose'], [class*='message-input'], [class*='toolbar'], [role='toolbar'], [class*='quick']') resolved to 4 elements:

But even then, could not actually get the bot to answer. Tried removing the .lock file as suggested and then restarted Docker completely but I was still stuck. Went through the setup again, was still stuck. I must say I am 3 hours in on this project so it's probably a code 18, but somehow I saw this in the log

2026-02-01T04:16:40.981Z [diagnostic] lane wait exceeded: lane=session:agent:main:main waitedMs=101778 queueAhead=0

and hour+ later I was back on track.

I know there's not a whole lot of actionnable thiings here, but if anything, this just re-inforce that somethings locking up the main thread from processing messages when long running processes timeouts.



### @bugsbuster (2026-02-01)

## Confirmed - Happens with normal message flow (no long operations needed)

**Environment:**
- OpenClaw version: `2026.1.30`
- OS: Linux (Ubuntu on AWS EC2)
- Channel: Telegram (long-polling)
- Model: `github-copilot/claude-sonnet-4.5`

**Reproduction (extremely simple):**
1. Send a message to the agent
2. While agent is still generating response (2-3 min for longer responses)
3. Send another message
4. Gateway crashes with lock timeout

**Key finding:** This happens with **zero special operations** - just normal text conversations. No exec commands, no browser automation, no background tasks. Simply sending a second message while the agent is processing the first one triggers the crash.

**Workaround:** Wait for the current response to complete before sending the next message.

**Analysis:**
The root cause appears to be Node.js event loop blocking during LLM API calls (Claude streaming responses can take 2-3 minutes). During this time:
- Gateway cannot accept new WebSocket connections
- Lock acquisition fails immediately (10s timeout)
- Session store becomes unreachable

This is **not** a background task issue - it's the agent's own API call blocking the event loop.

**Impact:** Makes the bot unusable in conversational scenarios where users naturally send follow-up messages without waiting for completion.

Related: Tested `background: true` for Python exec tasks - does **not** help, confirming the issue is in the agent runtime itself, not tool execution.


## Links

- None detected yet
