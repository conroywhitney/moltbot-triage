---
number: 3092
title: "Session lock timeout causes channel handler failures during long operations"
author: delonsp
created: 2026-01-28T02:36:28Z
updated: 2026-01-30T06:08:35Z
labels: ["bug"]
assignees: []
comments_count: 2
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/3092
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Description

Channel handlers (especially Telegram) frequently fail with lock timeout when the agent is processing any long operation (exec commands, HTTP requests, tool calls, etc):

```
error gateway/channels/telegram handler failed: Error: timeout acquiring session store lock: /root/.clawdbot/agents/main/sessions/sessions.json.lock
```

This happens even with a single channel active - it's not specific to multi-channel scenarios.

## Steps to Reproduce

1. Start a conversation that triggers a long operation (e.g., slow exec command, HTTP request that hangs)
2. While processing, send another message from the same or different channel
3. The new message fails with lock timeout

## Expected Behavior

Messages should queue and process sequentially, or the lock acquisition should have a longer timeout with retries.

## Actual Behavior

The channel handler fails immediately when it can't acquire the lock within the timeout period. The user sees no response and has to retry.

## Environment

- Clawdbot version: 2026.1.24-3
- OS: Linux (Ubuntu)
- Channels: Telegram, WhatsApp
- Agent config: maxConcurrent: 4

## Current Workaround

Cron job to clean stale locks every 2 minutes:
```bash
*/2 * * * * find /root/.clawdbot/agents/main/sessions -name "*.lock" -mmin +2 -delete
```

This helps recover from stuck locks but doesn't prevent the failures.

## Suggestions

1. Increase lock acquisition timeout for channel handlers
2. Implement automatic retry with backoff
3. Use a message queue instead of failing immediately
4. Consider async/non-blocking lock mechanism
5. Separate lock scopes for different operations (read vs write)

## Comments

### @nicolasmertens (2026-01-29)

## Additional Analysis: RPC Timeout During Session Processing

We're experiencing the same issue on our multi-agent setup (Clawdbot 2026.1.24-0, macOS arm64, Node v25.3.0).

### Our Findings

**Root cause confirmed:** When an agent session is in "processing" state (waiting for Claude API response, which can take 2-3 minutes), the gateway cannot handle new WebSocket RPC calls within the 10-second timeout.

**Evidence from logs:**
```
lane task done: lane=main durationMs=170756 active=1 queued=0
lane task done: lane=session:agent:main:main durationMs=170761 active=0 queued=0
```
Lane tasks taking 170+ seconds (almost 3 minutes) while cron tool calls timeout after 10 seconds.

**Diagnosis:**
1. Session starts processing â†’ holds resources
2. Cron tool (or any RPC call) tries to connect via WebSocket
3. Gateway event loop is blocked by session processing
4. RPC call times out after 10 seconds
5. Error: `gateway timeout after 10000ms`

**Key insight:** This is NOT just a file lock issue - it's event loop contention. Even when we cleared all .lock files, the timeouts continued until the session finished processing.

### Workaround Implemented

```cron
# Clawdbot stale lock cleanup (workaround for #3030/#3092)
*/2 * * * * find ~/.clawdbot/agents -name "*.lock" -mmin +2 -delete 2>/dev/null
```

This helps with stale locks but does NOT solve the RPC timeout during active session processing.

### Suggested Fix

The gateway should handle RPC calls (especially cron management) on a separate path that doesn't wait for session processing to complete. Cron operations don't need the session lock - they operate on the cron store, not session state.

Related: #2654, #3030, #3992

### @itsahedge (2026-01-30)

## Additional Evidence: Discord Channel with Stuck Sessions

We experienced the same root cause on Discord channels (Clawdbot on macOS arm64, Node v25.4.0).

### Symptoms

**Stuck sessions:**
```
[diagnostic] stuck session: sessionId=<redacted> state=processing age=515s queueDepth=0
```

**Slow Discord message processing:**
```
[discord] Slow listener detected: DiscordMessageListener took 289 seconds for event MESSAGE_CREATE
```

**Agent run timeouts:**
```
[agent/embedded] embedded run timeout: runId=<redacted> timeoutMs=600000
[agent/embedded] Profile anthropic:personal timed out (possible rate limit). Trying next account...
```

### Impact

Discord channels stopped responding to messages. Messages were being received by the gateway, but the agent sessions were stuck processing for 8+ minutes, causing:
- Message listener delays of 40-289 seconds (should be instant)
- Multiple concurrent sessions all stuck simultaneously
- Gateway had to be manually restarted to clear stuck sessions

### Diagnosis

Same root cause as described in the comments above: **Event loop contention during long session processing**. When agent sessions take 2-3+ minutes to process (waiting for Claude API), the gateway cannot handle new Discord events or RPC calls.

This manifests differently depending on the channel:
- **Telegram/WhatsApp:** Lock timeout errors (as reported above)
- **Discord:** Slow listener warnings and stuck sessions

But it's the same underlying issue: gateway event loop blocked by long-running session processing.

### Related Issues

- #3639 - Discord messages not sent after tool calls (possibly same root cause)
- #4173 - Sub-agents hang after tool results (similar stuck session pattern)
- #4410 - Feature request for auto-restart on stuck sessions (treating symptom, not cause)

Manual gateway restart cleared all stuck sessions immediately.


## Links

- None detected yet
