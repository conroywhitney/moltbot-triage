---
number: 6202
title: "[Bug]: Read tool inlines base64 images in session transcripts, causing context overflow"
author: tsangha
created: 2026-02-01T11:38:11Z
updated: 2026-02-01T20:58:58Z
labels: ["bug"]
assignees: []
comments_count: 1
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/6202
duplicate_of: null
related_issues: [1210]
blocks: []
blocked_by: []
---

## Description

## Description

The `read` tool stores base64-encoded image data directly in session JSONL files when reading images. This causes rapid context bloat and eventual model failures.

This also affects image outputs from LLM model tool invocations (e.g., image generation/analysis tools) â€” the base64 response gets persisted in the session transcript.

### Reproduction

1. Start a Telegram session
2. Read images or invoke image-generating model tools
3. After 5-7 images, session fails with capacity errors

### Evidence

Session file analysis:
```
$ awk "{print length, NR}" session.jsonl | sort -rn | head -3
474137 18   # 474KB - single image
400444 28   # 400KB - another image
9457 14
```

### Impact

- Sessions hit 200K token limit after ~5-7 images
- Session JSONL files grow to 2-5MB
- Model API calls fail with capacity/timeout errors
- `/reset` is the only recovery, losing all session context

### Expected Behavior

Similar to #1210, image tool results should:
1. Store image reference in transcript (path, hash, size)
2. Hydrate from disk when building API context
3. Apply `limitHistoryImages()` to tool results, not just inbound channel images

### Workaround

None currently. The Phase 1 fix (`limitHistoryImages`) only applies to inbound channel images, not to tool results.

### Environment

- OpenClaw version: 2026.1.30
- Channel: Telegram

### Related

- #1210 - Images from Discord stored as base64 in session transcripts

## Comments

### @abmccull (2026-02-01)

### Related Analysis

This is the same root cause pattern as #6190 (gateway tool bloat). Both are caused by large tool results being stored in session JSONL without size limits.

**Unified Fix Suggestion:**

Add a global tool result size limiter in session serialization:

```javascript
// In session serialization (before writing to JSONL)
const MAX_TOOL_RESULT_SIZE = 50_000; // 50KB

function serializeToolResult(result) {
    const serialized = JSON.stringify(result);
    if (serialized.length > MAX_TOOL_RESULT_SIZE) {
        // For images: store reference only
        if (result.type === 'image' || serialized.includes('base64')) {
            return {
                type: 'image_ref',
                truncated: true,
                originalSize: serialized.length,
                hint: 'Image stored separately - reference only in transcript'
            };
        }
        // For other large results: truncate
        return {
            truncated: true,
            originalSize: serialized.length,
            preview: serialized.slice(0, 1000) + '...'
        };
    }
    return result;
}
```

This would fix both #6190 (schema bloat) and #6202 (image bloat) with a single change.

cc @issue-6190


## Links

- None detected yet
