---
number: 6658
title: "Feature Request: Pre-Compaction Hook / Post-Execution Token Tracking"
author: AmbitiousRealism2025
created: 2026-02-01T22:50:10Z
updated: 2026-02-02T00:16:11Z
labels: ["enhancement"]
assignees: []
comments_count: 0
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/6658
duplicate_of: null
related_issues: [9239]
blocks: []
blocked_by: []
---

## Description

# OpenClaw Feature Request: Post-Execution Token Tracking

**Date:** 2026-02-01  
**Author:** Duncan (via Sean @ Ambitious Realism)  
**Priority:** High  
**Type:** Architecture Enhancement

---

## Summary

OpenClaw's current token tracking uses **pre-execution estimates** that are fundamentally unreliable. This causes context overflow without memory flush, resulting in lost work context during compaction.

Request: Implement **post-execution token tracking** similar to Claude Code's approach.

---

## Current Behavior (Problem)

```
1. Session loads with totalTokens from LAST run (e.g., 150K)
2. Memory flush check: 150K < 160K threshold → NO FLUSH
3. Agent runs, browser snapshot adds 25K tokens
4. API call fails: 175K > context limit → OVERFLOW ERROR
5. Auto-compaction runs WITHOUT memory flush
6. Context lost
```

**Root cause:** Token count is checked BEFORE tool execution, using stale data from the previous run. Tool output size is unpredictable.

**Code locations:**
- `agent-runner.js` line ~104: Calls `runMemoryFlushIfNeeded` with stale `totalTokens`
- `memory-flush.js` lines 52-63: Checks `params.entry?.totalTokens` (stale)
- `run.js` lines 287-312: Detects overflow AFTER API fails, triggers compaction without flush

---

## Proposed Behavior (Solution)

### Option A: Pre-Compaction Hook (Minimal Change)

Add a `compaction:before` hook that fires when `isContextOverflowError()` returns true, BEFORE auto-compaction:

```javascript
// In run.js, around line 287
if (isContextOverflowError(errorText)) {
    // NEW: Fire pre-compaction hook
    await fireHook('compaction:before', { 
        sessionKey, 
        reason: 'overflow',
        tokenEstimate: estimatedTokens
    });
    
    // THEN run auto-compaction
    const compactResult = await compactEmbeddedPiSessionDirect({...});
    
    // NEW: Fire post-compaction hook
    await fireHook('compaction:after', {
        sessionKey,
        summaryLength: compactResult.summaryTokens
    });
}
```

**Effort:** Small code change  
**Benefit:** Users can implement memory flush in the hook

### Option B: Post-Execution Token Injection (Claude Code Pattern)

Track actual token usage from API response metadata and inject state into conversation:

```javascript
// After each API response
const actualUsage = response.usage.input_tokens + response.usage.output_tokens;
const stateInjection = `[TOKEN_STATE: ${actualUsage}/${contextLimit} (${(actualUsage/contextLimit*100).toFixed(1)}%)]`;

// Inject into next turn so model knows its state
```

**Effort:** Medium architectural change  
**Benefit:** Model self-adjusts behavior based on real context state

### Option C: Flush Inside Compaction Path

Modify `compactEmbeddedPiSessionDirect` to run memory flush as part of compaction:

```javascript
// In compact.js
export async function compactEmbeddedPiSessionDirect(params) {
    // NEW: Run memory flush before summarizing
    await runMemoryFlushDirect(params);
    
    // Then continue with compaction
    ...
}
```

**Effort:** Medium  
**Benefit:** Guaranteed flush before any compaction

---

## Why This Matters

### Current User Experience
- Work in progress → large tool output → overflow → compaction → context lost
- No opportunity to save important state
- "Groundhog day" effect: agent forgets recent decisions

### Research Context

Analysis of Claude Code (Anthropic) and OpenCode reveals:

| System | Token Tracking | Result |
|--------|----------------|--------|
| Claude Code | Post-execution, injected warnings | Model always knows real state |
| OpenCode | Threshold at 95%, post-execution | Graceful degradation |
| OpenClaw | Pre-execution estimates | Unpredictable overflow |

**Key insight from Claude Code:** They inject `<<system_warning>>Token usage: X/Y; Z remaining<</system_warning>>` after EVERY tool result. The model self-adjusts because it always knows its budget.

---

## Recommended Implementation

**Phase 1 (Quick Win):** Implement Option A - `compaction:before` hook
- Smallest change
- Enables user-space solutions (plugins can flush memory)
- Non-breaking

**Phase 2 (Full Fix):** Implement Option B - Post-execution tracking
- Track actual usage from API response `usage` field
- Multi-threshold warnings (70%, 85%, 95%)
- Model sees real context state

---

## References

- Research report: `memory-token-tracking-analysis.md`
- Root cause analysis: `memory-flush-bug-analysis.md`
- Claude Code monitoring docs: https://code.claude.com/docs/en/monitoring-usage
- Claude Code GitHub Issue #9239 (token warning behavior)
- OpenCode context management: https://deepwiki.com/sst/opencode/2.4-context-management-and-compaction

---

## Contact

Sean @ Ambitious Realism  
Discord: Ambitious Realism community  
GitHub: (to be filed as issue)

---

*This feature request was researched by deploying parallel sub-agents (Atreides/Claude Code and Sisyphus/OpenCode) to analyze token tracking implementations across major coding agent platforms.*

## Comments


## Links

- None detected yet
