---
number: 5696
title: "[Bug]: Memory sync fails exceed OpenAI's 8192 token limit."
author: DiNaSoR
created: 2026-01-31T21:13:18Z
updated: 2026-02-02T08:25:12Z
labels: ["bug"]
assignees: []
comments_count: 2
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/5696
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Summary

  Memory sync fails when session content creates chunks that exceed OpenAI's `text-embedding-3-small` 8192 token limit.
  The `buildEmbeddingBatches()` function allows oversized single chunks to be sent without truncation.

  ## Steps to reproduce

  1. Enable session memory in config:
     ```json
     "memorySearch": {
       "enabled": true,
       "sources": ["memory", "sessions"],
       "experimental": {
         "sessionMemory": true
       }
     }
  2. Have a long conversation with large code blocks or JSON content
  3. Wait for memory sync to trigger (session-delta)

  Expected behavior

  Chunks exceeding the embedding model's token limit should be truncated or split further before being sent to the API.

  Actual behavior

  Memory sync fails with error:

  [memory] sync failed (session-delta): Error: openai embeddings failed: 400 {
    "error": {
      "message": "This model's maximum context length is 8192 tokens, however you requested 8207 tokens (8207 in your
  prompt; 0 for the completion). Please reduce your prompt; or completion length.",
      "type": "invalid_request_error"
    }
  }

  The issue is in manager.js line ~1389:

  if (current.length === 0 && estimate > EMBEDDING_BATCH_MAX_TOKENS) {
      batches.push([chunk]);  // Oversized chunk still sent without truncation
      continue;
  }

  Environment

  - OpenClaw version: 2026.1.30
  - OS: Linux (Ubuntu)
  - Install method: npm global

  Logs or screenshots

  Jan 31 18:56:49 openclaw[311990]: 2026-01-31T18:56:49.766Z [memory] sync failed (session-delta): Error: openai
  embeddings failed: 400 {"error":{"message":"This model's maximum context length is 8192 tokens, however you requested
  8207 tokens..."}}

## Comments

### @tonydehnke (2026-02-01)

Confirming this same issue with **local embeddings** (`embeddinggemma-300M-GGUF`), not just OpenAI.

**Environment:**
- OpenClaw 2026.1.30
- Provider: `local` (node-llama-cpp with embeddinggemma-300M-Q8_0.gguf)
- Chunk size configured: 256 tokens (also tried 401)
- Linux (Hetzner CX33, 8GB RAM)

**Error:**
```
Memory index failed (main): Input is longer than the context size. Try to increase the context size or use another model that supports longer contexts.
```

**Session files involved:** Multiple 1-2MB session files from extended conversations.

**Observations:**
- `memory index --force --verbose` shows endless `[memory] embeddings: batch start` loops before failing
- Index stays stuck at 24/169 files (13/158 sessions)
- Reducing `memorySearch.chunking.tokens` from 401 to 256 didn't help
- The chunking config appears to be ignored for session-delta sync

This blocks session memory indexing entirely for anyone with large sessions, regardless of embedding provider.

### @humphreyyy (2026-02-02)

> Confirming this same issue with **local embeddings** (`embeddinggemma-300M-GGUF`), not just OpenAI.
> 
> **Environment:**
> 
> * OpenClaw 2026.1.30
> * Provider: `local` (node-llama-cpp with embeddinggemma-300M-Q8_0.gguf)
> * Chunk size configured: 256 tokens (also tried 401)
> * Linux (Hetzner CX33, 8GB RAM)
> 
> **Error:**
> 
> ```
> Memory index failed (main): Input is longer than the context size. Try to increase the context size or use another model that supports longer contexts.
> ```
> 
> **Session files involved:** Multiple 1-2MB session files from extended conversations.
> 
> **Observations:**
> 
> * `memory index --force --verbose` shows endless `[memory] embeddings: batch start` loops before failing
> * Index stays stuck at 24/169 files (13/158 sessions)
> * Reducing `memorySearch.chunking.tokens` from 401 to 256 didn't help
> * The chunking config appears to be ignored for session-delta sync
> 
> This blocks session memory indexing entirely for anyone with large sessions, regardless of embedding provider.

I have the exact same setup and getting the same error. My bot tried to fix it himself but kinda offed himself while doing it. 


## Links

- None detected yet
