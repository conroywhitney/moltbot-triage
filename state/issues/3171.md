---
number: 3171
title: "[Feature]: Contextual Retrieval — prepend document/section context to chunks at index time"
author: hierosir1984
created: 2026-01-28T06:13:58Z
updated: 2026-01-28T16:06:34Z
labels: ["enhancement", "extensions: memory-core"]
assignees: []
comments_count: 1
reactions_total: 0
url: https://github.com/moltbot/moltbot/issues/3171
duplicate_of: null
related_issues: [2080,2100,2542]
blocks: []
blocked_by: []
---

## Description

## Summary

The current chunking implementation (`chunkMarkdown()` in `src/memory/internal.ts`) splits documents into fixed-size text chunks with configurable overlap. Each chunk is embedded independently with no awareness of:

- **Which document it came from** (file path is metadata but not embedded)
- **What section it belongs to** (heading hierarchy is lost during chunking)
- **What the document is about** (no context is prepended)

This means a chunk like:

> "The default value is 60 and can be adjusted via the config file."

...produces an embedding with no context about *what* has a default of 60 or *which* config file. The embedding captures generic semantics but not the specific meaning in context.

This is a known retrieval failure mode. Anthropic's research on **Contextual Retrieval** (September 2024) demonstrated that prepending document/section context to chunks before embedding reduces retrieval failures by **49%** (embeddings alone) and **67%** (combined with BM25).

## Proposed solution

At index time, before embedding, prepend a short context prefix to each chunk. Two modes, configurable:

**Mode A: Header-based context (zero cost, no LLM call)**

Extract the markdown heading hierarchy and prepend it:

```
Document: memory/2026-01-28.md > Morning Check-in > Task Updates

The default value is 60 and can be adjusted via the config file.
```

Header extraction logic:

```typescript
function extractHeadingContext(
  content: string,
  chunkStartLine: number,
  filePath: string
): string {
  const lines = content.split('\n');
  const headings: string[] = [`Document: ${filePath}`];

  for (let i = 0; i < chunkStartLine - 1; i++) {
    const match = lines[i]?.match(/^(#{1,6})\s+(.+)/);
    if (match) {
      const level = match[1].length;
      headings.length = Math.min(headings.length, level);
      headings.push(match[2]);
    }
  }

  return headings.join(' > ');
}
```

**Mode B: LLM-generated context (higher quality, costs per chunk)**

Use a small/fast model to generate a 1-2 sentence context summary per chunk. This is optional and only for users who want maximum quality.

Suggested config:

```json5
{
  "memorySearch": {
    "indexing": {
      "contextualRetrieval": {
        "enabled": false,
        "mode": "headers",          // "headers" | "llm"
        "llm": {
          "provider": "openai",
          "model": "gpt-4.1-mini",
          "prompt": "Provide a short context (1-2 sentences) for this chunk..."
        }
      }
    }
  }
}
```

Key implementation detail: store both the raw chunk text (for display/snippets) and the context-enriched text (for embedding). This avoids cluttering search results with synthetic prefixes.

```sql
ALTER TABLE chunks ADD COLUMN context_text TEXT;
-- context_text is what gets embedded; text is what gets displayed
```

## Alternatives considered

- **Larger chunk sizes**: Captures more context but wastes tokens on irrelevant content and degrades embedding quality for long texts.
- **Overlapping chunks only**: Current approach. Overlap helps continuity but doesn't solve the "orphaned chunk" problem where a chunk loses all document context.
- **Document-level embeddings + chunk retrieval**: More complex architecture. Contextual retrieval achieves similar benefits with minimal changes to the existing pipeline.

## Additional context

Evidence:
- **Anthropic's Contextual Retrieval research** (September 2024):
  - Embeddings + context: 49% fewer retrieval failures
  - Embeddings + BM25 + context: 67% fewer retrieval failures
  - Blog: https://www.anthropic.com/news/contextual-retrieval
- **LlamaIndex SentenceWindowNodeParser**: Similar concept with sentence windows
- **LangChain ParentDocumentRetriever**: Stores parent context alongside chunks
- **Pinecone**: Recommends including document metadata in embeddings

Backward compatibility:
- Fully opt-in: disabled by default
- Requires reindex when enabled (context changes all embeddings — the existing reindex trigger on config change handles this automatically)
- No breaking schema changes: new `context_text` column is additive
- Header mode has zero runtime cost: no API calls, purely structural extraction
- LLM mode cost: one cheap LLM call per chunk during indexing only

Related: #2100, #2080, #2542

## Comments

### @reh3376 (2026-01-28)

I have developed a highly effective persistent long term memory application that can sidecar on any project.  It address 3 main issues:
1) Problem1 - Stateless solutions don't work for stateful work. The failure mode of most models and coding agents is not "they get an answer wrong", it is that they stop honoring commitments once context churn begins.
       - mdemg solution: the application is built around metrics that actually match most workflows.
       - State survival under context churn / rot - do decisions persist after compaction / reset!
 Verifiable evidence (whk-wms v22): 
       - In v22 codebase ingestion benchmarking you can see the divergence clearly:
       - Compaction Events: 12 (baseline - agent with standard tool call) vs 0 (MDEMG - agent armed with the mdemg skill)
            - Results:
            - Decision Persistence (DP@K): 5/100 (baseline) vs 62/100 (MDEMG)
            - Mean test scores of 3 runs each is close because test battery is evidence-heavy, but the system-level behavior isn’t close.
            - Baseline test drop significantly on auto-compact or reset
            - mdemg stays stable.  
            - Please dont take my work for it, I invite you to validate these tests yourselves (details below). 

2) Problem2 - Large mature codebases are mostly off limits to Coding agents.  Frontier models can answer questions about a big repo, but that is the easy part.  Its much harder part is having the model make correct, coherent changes across:
      - Many files
      - deep dependency chains
      - repeated refactors
      - conventions that a not explicitly defined
     This is where systems start to fail, not because the3 model is dumb, but because the memory substrate is brittle and prone to amnesia. 
     Another large codebase ingestion benchmarking example:  Zed (primarily written in Rust) 1.08MM loc.
           - Warm start mdemg tests (with learning edges beginning to form) can match baseline LLM performance within 0.5%, using 1/20th the tokens and never losing context.  
           - At this scale (1+MM loc) the differentiator isnt raw model capacity, it's weather the system can stablize state and reinforce the right paths over time.  

3) Problem3 - Current memory improvements are little more than band-aids, writing to .md files that the reset model must re-ingest doesn't fix anything, it shortens the time till next amnesia attack.  Most common interventions: 
      - RAG pipelines that retrieve snippets but remain effectively stateless
      - workflows that dump a bunch of `.md` files into context
      - longer context windows followed by aggressive compaction
      - “memory” features that decay on a timer instead of based on relevance/use

     All these approaches seem to help at first but under load they fail, failure modes are familiar:
            - context rot
            - amnesia after reset / compaction
            - drift and repeated correction
            - severe regression or worse hallucination when model cant reconcile earlier decisions
            
    Another ingest-codebase(blueseer) example with new insight (500k+ loc) Java based
           - baseline mean: 0.830
           - mdemg mean: 0.809
                 - The operational differences show what actually happened:
                 - baseline testing required handcuff to be able to complete testing:  needed 5 explicit constraints to keep on track before context reset.
                 - mdemg needed 0
           - The real differentiator was in the tail risk 
                 - baseline had  a 1/3 run catastrophic failure (severity 3 - mean score = 0)
                 - mdemg's worst was a recoverable severity 1 failure that resulted in a reduced mean score of 0.67 (a -0.15 - 0.10 reduction).
                 - Therefore even when the mean compresses, mdemg changes core iteraction:
                       - no babysitting the system
                       - reduced failure when failure does occur.   
This is already too long.  I invite you to verify for yourself at - MDEMG: [https://github.com/reh3376/mdemg]


## Links

- None detected yet
