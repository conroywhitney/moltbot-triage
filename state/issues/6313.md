---
number: 6313
title: "[Bug]: # openai-responses API Type Silent Failure with Proxied Endpoints"
author: vashquez
created: 2026-02-01T14:35:10Z
updated: 2026-02-01T14:35:10Z
labels: ["bug"]
assignees: []
comments_count: 0
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/6313
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Summary
The `openai-responses` API type in clawdbot works when connecting directly to GitHub Copilot API, but **fails silently when routing through OpenAI-compatible proxy endpoints** (like VS Code Copilot Proxy or custom converters). The agent executes successfully and receives LLM responses, but **no Discord message is sent to the user**.

---

## Issue Details

### Title
`openai-responses` API handler does not forward LLM responses to Discord when using proxied OpenAI endpoints

### Type
Bug - Core Functionality

### Severity
High - Feature is completely non-functional with proxies

### Components Affected
- `models.providers[].api: "openai-responses"` handler
- Discord message sending pipeline
- Agent response processing

---

## Description

When clawdbot is configured to use the `openai-responses` API type with a proxied OpenAI-compatible endpoint (instead of direct GitHub Copilot API), the following behavior occurs:

1. User sends message in Discord
2. Clawdbot shows "typing..." indicator (proves Discord connection works)
3. Agent executes successfully against the proxy
4. LLM returns proper response content
5. **Agent completes normally but no message appears in Discord**

The system silently fails to send the response with no errors logged.

---

## Expected Behavior

When configured with `openai-responses` API type pointing to any OpenAI-compatible endpoint:
- Agent should receive and parse LLM response correctly
- Response content should be formatted for Discord
- Discord reply should be sent to user
- User should see the bot's response message

---

## Actual Behavior

- Agent executes successfully
- LLM response received with proper content
- **Discord receives NO message**
- **No error logged** (silent failure)
- Typing indicator disappears with no reply

---

## Steps to Reproduce

1. Set up clawdbot with an OpenAI-compatible proxy endpoint:
   ```json
   {
     "models": {
       "providers": {
         "copilot": {
           "baseUrl": "http://127.0.0.1:8081/v1",
           "api": "openai-responses",
           "apiKey": "unused",
           "models": [{"id": "gpt-4o", "name": "GPT-4o"}]
         }
       }
     },
     "agents": {
       "defaults": {
         "model": {"primary": "copilot/gpt-4o"}
       }
     }
   }
   ```

2. Start VS Code Copilot Proxy or similar OpenAI-compatible proxy on port 8080
3. Start a message converter proxy that transforms messages and forwards to Copilot Proxy on port 8081
4. Send message to bot in Discord
5. Observe: Typing indicator appears, but no reply message is sent

---

## Evidence

### Converter Logs (Proxy Working Correctly)
```
[REQ-cdoem2] 2026-02-01T14:16:10.609Z POST /v1/chat/completions
[REQ-cdoem2] - Messages: 152
[REQ-cdoem2] - Model: gpt-4o
[REQ-cdoem2] - Stream: false
[REQ-cdoem2] - Payload size: 200671 bytes
[REQ-cdoem2] Forwarding to Copilot Proxy...
[REQ-cdoem2] ‚úÖ Success: 200
[REQ-cdoem2] - Response choices: 1
[REQ-cdoem2] - Content length: 39, preview: Yes, I'm here! How can I assist you? ü™º
```

### Clawdbot Logs (Agent Executes Successfully)
```json
{
  "subsystem": "agent/embedded",
  "message": "embedded run start: runId=87a89ccc-3ade-4138-9a0d-2a23fa4c721f provider=copilot model=gpt-4o thinking=off messageChannel=discord",
  "time": "2026-02-01T14:14:57.245Z"
}
```

Agent lifecycle logs show:
- ‚úÖ `embedded run start` - Agent begins execution
- ‚úÖ `embedded run prompt start` - Sends request to LLM
- ‚úÖ `embedded run prompt end: durationMs=1785` - LLM responds
- ‚úÖ `embedded run agent end` - Agent processing completes
- ‚úÖ `embedded run done: durationMs=1815 aborted=false` - Agent finishes successfully

**Missing:** Any log indicating response was sent to Discord or Discord message handling

### Network Evidence
- Converter receives HTTP POST requests from clawdbot ‚úÖ
- Converter sends proper requests to Copilot Proxy ‚úÖ
- Copilot Proxy returns 200 status with choice content ‚úÖ
- Converter returns complete response JSON to clawdbot ‚úÖ
- **Discord receives no message** ‚ùå

---

## Environment

- **Clawdbot Version:** 2026.1.24-3
- **Node.js Version:** 24.10.0
- **Discord Integration:** Enabled and working (typing indicator sends correctly)
- **API Configuration:** 
  - Provider: `openai-responses`
  - Endpoint: `http://127.0.0.1:8081/v1` (OpenAI-compatible proxy)
  - Models: Standard OpenAI chat completion format
- **Network:** OpenDNS blocking direct `api.githubcopilot.com` connections (reason for proxy requirement)

---

## Root Cause Analysis

The bug likely stems from one of these issues in clawdbot's `openai-responses` handler:

1. **Response Format Assumption:** Handler assumes response comes from GitHub Copilot API directly and may be making assumptions about response structure that differ between GitHub's endpoint and OpenAI-compatible proxies

2. **Status Code Handling:** GitHub Copilot API may return different status codes or headers than OpenAI-compatible proxies, causing silent failure in response processing

3. **Content Extraction:** The handler may be extracting/parsing the response content in a way that works with GitHub's direct API but fails with proxied endpoints

4. **Error Handling:** Missing error handling that silently swallows exceptions during Discord message sending

5. **Message Pipeline:** The response may be extracted correctly but lost during formatting or sending to Discord channel

---

## Workaround

None currently available. The `openai-responses` API type cannot be used with proxied endpoints in the current version.

### Alternative (Partial)
Use direct GitHub Copilot API connection if network allows (not viable with OpenDNS blocking):
```json
{
  "baseUrl": "https://api.githubcopilot.com"
}
```

---

## Additional Notes

### What Works
- VS Code Copilot Proxy (port 8080) - Fully functional
- Message format conversion - Successfully converts clawdbot message format to OpenAI-compatible format
- LLM inference - Copilot returns proper responses
- Agent execution - Completes without errors
- Discord connection - Typing indicator proves integration works

### What Fails
- **Clawdbot's response handling** with proxied OpenAI endpoints
- **Discord message sending** when using `openai-responses` with proxy

---

## Suggested Fix

The `openai-responses` handler should be audited to:

1. Verify response format parsing works with standard OpenAI JSON structure (not GitHub-specific)
2. Add comprehensive error logging for response processing failures
3. Implement retry logic for transient failures
4. Verify Discord message formatting pipeline handles responses from proxies
5. Test with OpenAI-compatible endpoints (Ollama, Vllm, custom proxies, etc.)

---

## Impact

Users who cannot connect directly to `api.githubcopilot.com` (due to network restrictions, firewalls, etc.) cannot use clawdbot's Copilot integration even with working proxy infrastructure.

This blocks legitimate use cases where:
- Corporate firewalls block direct API connections
- Network policies require traffic through proxies
- Users run local Copilot proxies for performance/privacy

---

## Related Issues

Similar behavior may exist with other `openai-*` API types when used with proxies. Recommend auditing entire openai handler family.

---

## Reproduction Test Case

```bash
# 1. Start VS Code Copilot Proxy (port 8080)
# 2. Start message converter proxy (port 8081) 
# 3. Configure clawdbot with:
#    - api: "openai-responses"
#    - baseUrl: "http://127.0.0.1:8081/v1"
# 4. Send Discord message
# 5. Check logs: Agent runs, LLM responds, but Discord gets nothing
```

## Comments


## Links

- None detected yet
