---
number: 6016
title: "[Bug]: Memory sync session-delta ignores chunking, cascades to trigger compaction"
author: batumilove
created: 2026-02-01T06:17:24Z
updated: 2026-02-02T00:17:00Z
labels: ["bug"]
assignees: []
comments_count: 2
reactions_total: 1
url: https://github.com/openclaw/openclaw/issues/6016
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Summary

Large session files cause memory sync to fail with "Input is longer than the context size", which unexpectedly cascades to trigger safeguard compaction in the main agent session, losing user messages.

## Bug Chain

1. Session file grows large (6.7MB in my case from extended debugging session)
2. User sends a message (voice message in this case)
3. Memory sync `session-delta` triggers
4. Session-delta sync attempts to embed the session data **without respecting chunking limits**
5. Embedding model fails: `"Input is longer than the context size"`
6. This failure somehow triggers safeguard compaction in the main session
7. User's message is lost in compaction

## Observed Logs

```
[memory] memory sync failed (session-delta): Error: Input is longer than the context size. Try to increase the context size or use another model that supports longer contexts.
[memory] memory sync failed (session-delta): Error: Input is longer than the context size...
[memory] memory sync failed (watch): Error: Input is longer than the context size...
[agent/embedded] embedded run compaction start: runId=2a3690eb-2a45-4cd2-9ff5-dd23a5a285dd
[agent/embedded] embedded run compaction retry: runId=2a3690eb-2a45-4cd2-9ff5-dd23a5a285dd
```

This pattern repeated every time the user sent a voice message.

## Expected Behavior

1. Session-delta sync should respect `memorySearch.chunking.tokens` (configured as 16000)
2. Memory sync failures should NOT cascade to trigger compaction in the main agent
3. User messages should never be lost due to background indexing failures

## Actual Behavior

- Session-delta sends the entire session to the embedding model
- Embedding model (voyage-4-large, 16k context) rejects it
- Compaction is triggered, wiping context including the just-received message

## Workaround

Removed `sessions` from `memorySearch.sources`:
```json
{
  "agents": {
    "defaults": {
      "memorySearch": {
        "sources": ["memory"]  // was ["memory", "sessions"]
      }
    }
  }
}
```

## Environment

- OpenClaw: 2026.1.29 (dev channel)
- OS: Linux 6.12.42 (x64)
- Embedding: voyage-4-large via VoyageAI
- Chunking config: `{ "tokens": 16000, "overlap": 500 }`
- Session file size: 6.7MB (~6500 lines JSONL)

## Relevant Config

```json
{
  "memorySearch": {
    "enabled": true,
    "sources": ["memory", "sessions"],
    "experimental": { "sessionMemory": true },
    "model": "voyage-4-large",
    "chunking": { "tokens": 16000, "overlap": 500 }
  },
  "compaction": {
    "mode": "safeguard",
    "memoryFlush": { "enabled": true }
  }
}
```

## Impact

- Voice messages (and likely any messages) silently lost
- User sees "context compacted" message with no indication of the cause
- Appears as if the system is ignoring their input

## Comments

### @tonydehnke (2026-02-01)

+1 Confirming this exact behavior.

**Environment:**
- OpenClaw 2026.1.30
- Embedding provider: `local` (embeddinggemma-300M-GGUF via node-llama-cpp)
- `memorySearch.chunking.tokens`: 256 (also tried 401)
- Linux (Hetzner CX33)

**Symptoms:**
1. Multiple session files 1-2MB each
2. `memory index --force` loops on `[memory] embeddings: batch start` for ~10+ minutes
3. Eventually fails: `Input is longer than the context size`
4. Index stuck at 24/169 files despite chunking config

**Key observation:** The chunking config is definitely ignored for session content. Even with `tokens: 256`, the full session content appears to be passed to the embedding model.

**Workaround for now:** Considering disabling session indexing entirely (`"sources": ["memory"]`) since memory files index fine — it's only sessions that fail.

This is blocking session memory search for anyone with extended conversations.

### @tonydehnke (2026-02-01)

**Update: Found a working workaround using cloud embeddings**

After more testing, confirming that switching local embedding models doesn't help:
- Tried `nomic-embed-text-v1.5` (GGUF) — same 2K context limit, same failure at 13/162 sessions

**What DOES work: Cloud embeddings via OpenAI-compatible API**

Config change:
```json
"memorySearch": {
  "provider": "openai",
  "remote": {
    "baseUrl": "https://api.synthetic.new/openai/v1",  // or any OpenAI-compatible endpoint
    "apiKey": "your-key"
  },
  "model": "hf:nomic-ai/nomic-embed-text-v1.5"
}
```

**Result:** All 162 session files indexed successfully (1692 chunks). The cloud provider apparently handles larger inputs than local GGUF models.

This is a workaround, not a fix — the bug still exists for local embedding users. But for anyone blocked on this, cloud embeddings work until proper session chunking is implemented.


## Links

- None detected yet
