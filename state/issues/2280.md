---
number: 2280
title: "[Feature]: Azure OpenAI as model provider"
author: wilsonwu
created: 2026-01-26T16:37:35Z
updated: 2026-01-30T11:37:53Z
labels: ["enhancement"]
assignees: []
comments_count: 13
reactions_total: 15
url: https://github.com/openclaw/openclaw/issues/2280
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Summary
I want to set a Azure OpenAI as the model provider in Clawdbot.

## Proposed solution
In setup process, I can choose Azure OpenAI and input the url and key like others.

## Comments

### @bakes82 (2026-01-27)

You will need to choose the model also as Azure Foundry supports Anthropic, Llama, OpenAI, Grok, and others.

### @wttat (2026-01-27)

> You will need to choose the model also as Azure Foundry supports Anthropic, Llama, OpenAI, Grok, and others.

I choose the openai/gpt-5.2, and input the azure openai key:
And i got this:
401 Incorrect API key provided: You can find your API key at https://platform.openai.com/account/api-keys.
do i need to setup the Endpoint?

### @cyptus (2026-01-27)

> > You will need to choose the model also as Azure Foundry supports Anthropic, Llama, OpenAI, Grok, and others.
> 
> I choose the openai/gpt-5.2, and input the azure openai key: And i got this: 401 Incorrect API key provided: You can find your API key at https://platform.openai.com/account/api-keys. do i need to setup the Endpoint?

for sure it will not work because you enter an azure api key for a connection to open ai.
a provider that connects to azure ai foundry is needed first.
it has another api interface then open ai.

### @goxia (2026-01-28)

> ## Summary
> I want to set a Azure OpenAI as the model provider in Clawdbot.
> 
> ## Proposed solution
> In setup process, I can choose Azure OpenAI and input the url and key like others.

We've been debugging all afternoon. Azure OpenAI + Teams requires custom development.


### @cyptus (2026-01-28)

@goxia actually the "Microsoft Foundry" deployments for "OpenAI" models should be API compatible with OpenAI API itself. only base-url, authentication and model choosing should be different:
https://learn.microsoft.com/en-us/azure/ai-foundry/openai/reference?view=foundry-classic

### @palla89 (2026-01-28)

Interested in this one too!

### @vladtsit (2026-01-28)

Azure OpenAI endpoints use a different URL structure than standard OpenAI, and the current custom provider implementation cannot correctly handle Azure's required format.

Azure OpenAI URL Format
Azure requires:


https://{resource}.cognitiveservices.azure.com/openai/deployments/{deployment}/chat/completions?api-version={version}
The api-version query parameter must remain at the end of the full URL.

What I Tried
1. Full URL in baseUrl:


{  "baseUrl": "https://dev-project.cognitiveservices.azure.com/openai/deployments/gpt-5.2/chat/completions?api-version=2025-04-01-preview",  "api": "openai-completions"}
Result: 404 (clawdbot likely appends /chat/completions again)

2. Without /chat/completions:


{  "baseUrl": "https://dev-project.cognitiveservices.azure.com/openai/deployments/gpt-5.2?api-version=2025-04-01-preview",  "api": "openai-completions"}
Result: 404 (query string gets malformed when path is appended)

3. Using openai-responses API:


{  "baseUrl": "https://dev-project.cognitiveservices.azure.com/openai/responses?api-version=2025-04-01-preview",  "api": "openai-responses"}
Result: 404

Direct curl verification (works perfectly)

Environment
clawdbot version: 2026.1.24-3
Node.js: v22.22.0
OS: Ubuntu (Linux)
Azure resource: Azure AI Foundry (cognitiveservices.azure.com)
Model deployment: gpt-5.2
API version: 2025-04-01-preview
Additional Configuration
Root Cause
Azure OpenAI requires:

Different auth header: api-key instead of Authorization: Bearer
Query parameter in URL: ?api-version= must be preserved at the end
Deployment in path: /openai/deployments/{name}/chat/completions
When clawdbot appends /chat/completions to a baseUrl that already contains ?api-version=..., the resulting URL becomes malformed (query string ends up in the middle of the path).

Related
PR #3144 (feat: add Azure provider support) addresses this with a URL-fix middleware
Issue #2280 (Feature: Azure OpenAI as model provider)
Discussion #3404 (Support for Enterprise Azure OpenAI Keys)
Suggested Solution
Either:

Merge PR #3144 which includes proper Azure support
Add a way to specify the complete URL without path appending (e.g., api: "raw" or appendPath: false)
Handle query strings correctly when appending paths

### @nameissainath (2026-01-28)

Issue is still not sorted for me can u provide me complete json of clawd.json 

### @joeyjoker (2026-01-29)

If you use azure foundry openai models , you can use base_url like:
https://yourendoint.cognitiveservices.azure.com/openai/v1
This format is compatible with openai.com

In sdk, you just need:
client = OpenAI(
base_url = your_endpoint.
api_key = your_key
)
BTW, this code can also use Grok models in azure foundry.
Hope this is helpful to  support azure foundry soon.

### @bakes82 (2026-01-29)

Isnt MS recommending using the OpenAI SDK and using the openai endpoints ....
https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/model-inference-to-openai-migration?view=foundry-classic&viewFallbackFrom=foundry&tabs=openai&pivots=programming-language-python

```
import os
from openai import OpenAI

client = OpenAI(
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    base_url="https://<resource>.openai.azure.com/openai/v1/",
)

Id also like to see entra support since if youre in a org you probably dont have access to the api keys

from openai import OpenAI
from azure.identity import DefaultAzureCredential, get_bearer_token_provider

token_provider = get_bearer_token_provider(
    DefaultAzureCredential(), 
    "https://cognitiveservices.azure.com/.default"
)

client = OpenAI(
    base_url="https://<resource>.openai.azure.com/openai/v1/",
    api_key=token_provider,
)
```

### @vladtsit (2026-01-29)

> If you use azure foundry openai models , you can use base_url like:
> https://yourendoint.cognitiveservices.azure.com/openai/v1
> This format is compatible with openai.com

You are absolutely right

Just tested and it works

"azure-openai": {
        "baseUrl": "https://some-foundry.openai.azure.com/openai/v1",
        "apiKey": "${AZURE_OPENAI_API_KEY}",
        "api": "openai-completions",
        "headers": {
          "api-key": "${AZURE_OPENAI_API_KEY}"
        },
        "models": [
          {
            "id": "gpt-5.2",
            "name": "GPT-5.2 (Azure)",
            "reasoning": false,
            "input": [
              "text",
              "image"
            ],
            "contextWindow": 128000,
            "maxTokens": 16384,
            "compat": {
              "maxTokensField": "max_completion_tokens"
            }
          }
        ]
      }

### @vladtsit (2026-01-29)

> Issue is still not sorted for me can u provide me complete json of clawd.json

Fallback method above works for me. Sorry for misunderstanding. 

### @gimlichael (2026-01-30)

My gateway restarts itself when integrating with Azure. Maybe because i don't have access to the premium models, and uses models such as gpt-mini and ollama 3.3?


## Links

- None detected yet
