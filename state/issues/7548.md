---
number: 7548
title: "Local GGUF memory embeddings can deadlock due to Promise.all concurrency (node-llama-cpp)"
author: Asentient
created: 2026-02-02T23:43:17Z
updated: 2026-02-02T23:43:17Z
labels: []
assignees: []
comments_count: 0
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/7548
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

### Summary
When using **local GGUF embeddings** for memory search, `openclaw memory index`/`memory_search` can hang indefinitely (repeating "batch start" with 0 chunks indexed).

Root cause appears to be concurrent embedding calls using `Promise.all(...)` against the node-llama-cpp embedding context, which can deadlock/hang when `getEmbeddingFor()` is executed concurrently.

### Environment
- OpenClaw: 2026.1.29
- MemorySearch provider: `local` (GGUF)
- Local embedding runtime: node-llama-cpp (via OpenClaw local embeddings)
- Symptom: `openclaw memory index --force --verbose` prints `batch start` repeatedly and never completes; index stays `0/x files, 0 chunks`; process may eventually get SIGKILLed.

### Fix
Change embedding generation from concurrent to sequential.

**Before (hangs):**
```js
const embeddings = await Promise.all(texts.map(async (text) => {
  const embedding = await ctx.getEmbeddingFor(text);
  return Array.from(embedding.vector);
}));
```

**After (works):**
```js
const embeddings = [];
for (const text of texts) {
  const embedding = await ctx.getEmbeddingFor(text);
  embeddings.push(Array.from(embedding.vector));
}
```

### Location
In the installed build this was at:
- `dist/memory/embeddings.js` (around lines ~54â€“61)

### Notes
After applying the sequential loop, memory indexing completed successfully (5/5 files, 55 chunks) and `memory_search` began returning results normally.

If desired, I can help test a follow-up improvement (e.g., configurable concurrency) once the deadlock is addressed.

## Comments


## Links

- None detected yet
