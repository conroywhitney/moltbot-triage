---
number: 7587
title: "[Bug]: LLM request rejected: input length and max_tokens exceed context limit:"
author: moneybible
created: 2026-02-03T00:43:34Z
updated: 2026-02-03T00:43:34Z
labels: ["bug"]
assignees: []
comments_count: 0
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/7587
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Summary
LLM request fails when input length and `max_tokens` exceed the 200,000 token context limit, resulting in a hard rejection rather than graceful handling.

## Steps to reproduce
1. Engage in an extended conversation session with the agent
2. Allow conversation context to grow to approximately 170,000+ tokens
3. Submit a request that would require additional tokens for completion
4. Error occurs when total (input + max_tokens) exceeds 200,000 limit

## Expected behavior
- Agent should gracefully handle approaching context limits
- Should automatically summarize or truncate older context when nearing limits
- Should provide user-friendly warning before hitting hard limits
- Should attempt context management before failing

## Actual behavior
Hard failure with error message: `LLM request rejected: input length and max_tokens exceed context limit: 170631 + 34048 > 200000, decrease input length or max_tokens and try again`

## Environment
- Clawdbot version: [Current version]
- OS: macOS (Darwin 24.6.0 arm64)
- Install method: [To be filled]
- Model: anthropic/claude-sonnet-4-20250514

## Logs or screenshots
```
(LLM request rejected: input length and `max_tokens` exceed context limit: 170631 + 34048 > 200000, decrease input length or `max_tokens` and try again)
```

## Additional Notes
This appears to be related to long-running sessions where context accumulates beyond the model's limit. A proactive context management system would prevent this issue.
```

## Comments


## Links

- None detected yet
