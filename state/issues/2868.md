---
number: 2868
title: "[Bug]: Unexpected high token consumption with Claude models"
author: manhhungdt06
created: 2026-01-27T16:45:35Z
updated: 2026-01-29T12:28:51Z
labels: ["bug"]
assignees: []
comments_count: 1
reactions_total: 0
url: https://github.com/moltbot/moltbot/issues/2868
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

<img width="521" height="273" alt="Image" src="https://github.com/user-attachments/assets/d448128f-7b22-43b4-8e85-0c390c6d929a" />

<img width="732" height="255" alt="Image" src="https://github.com/user-attachments/assets/d804fb17-66d5-4a44-a94a-39492f6fcbc4" />

## Summary
Switching to Claude Sonnet model causes massive token consumption (~15% of session quota) 
with no actual API calls or chat activity. Same operations with GLM result in ~1% usage.

## Steps to reproduce
1. Start session with GLM-4.7 (or any cheap model)
2. Call `session_status --model anthropic/claude-sonnet-4-20250514`
3. Check rate limit immediately after
4. Observe 15% jump in session usage (59% â†’ 74%)

## Expected behavior
Model switching should use minimal tokens for the status check only (~few hundred tokens).
Should be consistent with other model switches (GLM, DeepSeek).

## Actual behavior
Single `session_status` call with Sonnet consumed ~15% of hourly quota.
- Session context: 39k tokens
- Only action: one status check
- No chat messages sent/received
- GLM same operation: +1% usage (normal)

## Environment
- Clawdbot version: 2026.1.24-3
- OS: Linux 6.8.0-90-generic (x64)
- Install method: npm (node v22.22.0)
- Gateway: Local mode

## Hypothesis
Claude model switching may be:
1. Re-processing entire session context through model
2. Duplicating state/context during model transition
3. Pricing calculation issue in rate limiter

GLM/DeepSeek don't show this behavior, suggesting Claude-specific issue.

## Logs or screenshots
Rate limit progression:
- 22:47 - 54% (normal session)
- 22:48 - 63% (switched to Haiku, then to Sonnet)
- 22:52 - 74% (after one session_status call with Sonnet)
- 23:33 - 20% (back to Haiku)
- 23:36+ - Only +1% (switched to GLM)

## Comments

### @monkut (2026-01-29)

I just installed today and configured to use "anthropic/claude-opus-4-5".
Nearly EVERY requests results in a `rate_limit_error`:

> HTTP 429: rate_limit_error: This request would exceed the rate limit for your organization (xxxx) of 10,000 input tokens per minute. For details, refer to: https://docs.claude.com/en/api/rate-limits. You can see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later.

Is there a setting/configuration we can set to properly measure/handle throttling so we avoid rate-limit errors?


## Links

- None detected yet
