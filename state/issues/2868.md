---
number: 2868
title: "[Bug]: Unexpected high token consumption with Claude models"
author: manhhungdt06
created: 2026-01-27T16:45:35Z
updated: 2026-02-02T20:51:53Z
labels: ["bug"]
assignees: []
comments_count: 7
reactions_total: 5
url: https://github.com/openclaw/openclaw/issues/2868
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

<img width="521" height="273" alt="Image" src="https://github.com/user-attachments/assets/d448128f-7b22-43b4-8e85-0c390c6d929a" />

<img width="732" height="255" alt="Image" src="https://github.com/user-attachments/assets/d804fb17-66d5-4a44-a94a-39492f6fcbc4" />

## Summary
Switching to Claude Sonnet model causes massive token consumption (~15% of session quota) 
with no actual API calls or chat activity. Same operations with GLM result in ~1% usage.

## Steps to reproduce
1. Start session with GLM-4.7 (or any cheap model)
2. Call `session_status --model anthropic/claude-sonnet-4-20250514`
3. Check rate limit immediately after
4. Observe 15% jump in session usage (59% â†’ 74%)

## Expected behavior
Model switching should use minimal tokens for the status check only (~few hundred tokens).
Should be consistent with other model switches (GLM, DeepSeek).

## Actual behavior
Single `session_status` call with Sonnet consumed ~15% of hourly quota.
- Session context: 39k tokens
- Only action: one status check
- No chat messages sent/received
- GLM same operation: +1% usage (normal)

## Environment
- Clawdbot version: 2026.1.24-3
- OS: Linux 6.8.0-90-generic (x64)
- Install method: npm (node v22.22.0)
- Gateway: Local mode

## Hypothesis
Claude model switching may be:
1. Re-processing entire session context through model
2. Duplicating state/context during model transition
3. Pricing calculation issue in rate limiter

GLM/DeepSeek don't show this behavior, suggesting Claude-specific issue.

## Logs or screenshots
Rate limit progression:
- 22:47 - 54% (normal session)
- 22:48 - 63% (switched to Haiku, then to Sonnet)
- 22:52 - 74% (after one session_status call with Sonnet)
- 23:33 - 20% (back to Haiku)
- 23:36+ - Only +1% (switched to GLM)

## Comments

### @monkut (2026-01-29)

I just installed today and configured to use "anthropic/claude-opus-4-5".
Nearly EVERY requests results in a `rate_limit_error`:

> HTTP 429: rate_limit_error: This request would exceed the rate limit for your organization (xxxx) of 10,000 input tokens per minute. For details, refer to: https://docs.claude.com/en/api/rate-limits. You can see the response headers for current usage. Please reduce the prompt length or the maximum tokens requested, or try again later.

Is there a setting/configuration we can set to properly measure/handle throttling so we avoid rate-limit errors?

### @LeoX91 (2026-01-30)

The token usage of OpenClaw is actually ridicolous. 

<img width="1184" height="503" alt="Image" src="https://github.com/user-attachments/assets/44b86a4d-c5ca-4a2a-afe1-a609641d05db" />

Literally EVERY request fills up the context windows of the model, resulting in a GIGA costly AI assistant. It is not usable for me, and I think for no one, even with cheap-ass models like Kimi K2 Thinkig.

### @8bitsuperCPU (2026-02-01)

getting same issue with OpenAI 

### @hazy2go (2026-02-01)

Noticed the same issue, im using MAX and used 42% of my weekly usage already, barely did anything

### @BR4DY (2026-02-02)

Trying to work out if this is a bug, or just how it's setup? I setup with Claude Opus, then Sonnet, then Haiku - all 3 were hitting 429 API limit errors, just saying basic things like hello how's it going after setting up... Something doesn't seem right!

### @Arthurooooo (2026-02-02)

I got the same issue, really annoying

### @glung (2026-02-02)

Is it because there are big files in the workspace? https://docs.openclaw.ai/token-use

~I noticed I have a PDF that would take 111k tokens if injected into the system prompt. The documentation suggests that all the workspace's content is included in the system prompt~

You can inspect the token usage as documented here https://docs.openclaw.ai/concepts/context#quick-start-inspect-context. I am still not sure what the culprit is, but it was not a single large file.


## Links

- None detected yet
