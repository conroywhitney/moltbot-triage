---
number: 5457
title: "Bug: Pre-compaction memory flush uses stale token counts, can be bypassed"
author: AmbitiousRealism2025
created: 2026-01-31T14:16:01Z
updated: 2026-02-02T00:19:03Z
labels: ["bug"]
assignees: []
comments_count: 1
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/5457
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Summary
The memory flush system checks `entry.totalTokens` to decide whether to run, but this value reflects token count from the *previous* turn, not the current context including the new message. This creates a race condition where large incoming messages can push context from "below threshold" to "overflow" in one jump, completely bypassing the flush.

## Reproduction
1. Configure `softThresholdTokens: 10000` (flush triggers at 170K for 200K context)
2. Session is at 160K tokens after previous turn
3. User sends a message that triggers a large tool output (e.g., browser snapshot = 20K+ tokens)
4. Flush check sees `totalTokens = 160K` (stale) → 160K < 170K → **flush skipped**
5. Agent turn runs, actual context is now 180K+
6. Overflow triggers compaction
7. **Memory flush never ran** - context lost

## Root Cause
In `agent-runner.js`, `runMemoryFlushIfNeeded` is called before the agent turn, but uses `sessionEntry.totalTokens` which is only updated *after* turns complete.

```javascript
// memory-flush.js - shouldRunMemoryFlush()
const totalTokens = params.entry?.totalTokens;  // STALE - from previous turn
if (totalTokens < threshold) return false;      // Skips flush incorrectly
```

## Proposed Fix
Estimate incoming message tokens before the flush decision:

```javascript
// In agent-runner.js, before calling runMemoryFlushIfNeeded:
const incomingTokens = estimateTokens(commandBody) + systemPromptOverhead;
const projectedTokens = (sessionEntry?.totalTokens ?? 0) + incomingTokens;

// Pass projected tokens to flush decision
activeSessionEntry = await runMemoryFlushIfNeeded({
    ...params,
    projectedTotalTokens,
});
```

## Environment
- Clawdbot version: 2026.1.24-3
- Model: anthropic/claude-opus-4-5 (200K context)
- Config: `softThresholdTokens: 10000`, `reserveTokensFloor: 20000`

Discovered 2026-01-31 during debugging session.

## Comments

### @nathanschram (2026-02-01)

## Additional Context: Safeguard Mode Interaction

I encountered this exact issue and found an additional wrinkle when using `compaction.mode: "safeguard"`.

### The Timing Mismatch

With safeguard mode enabled, there's a fundamental timing conflict:

1. **Memory flush** checks tokens at the START of `runReplyAgent()`, before the agent turn begins
2. **Safeguard compaction** triggers DURING the agent turn via the `session_before_compact` pi-extension, when context grows mid-reply

This means:
- Memory flush sees stale tokens (e.g., 157k) → decides "we're fine, no flush needed"
- Agent starts responding, context grows
- Safeguard compaction triggers mid-reply at 200k+ tokens
- Memory flush never had a chance to run

### Evidence from Real Session

```
sessions.json state:
  compactionCount: 4
  memoryFlushCompactionCount: null  ← Never triggered despite 4 compactions!
  totalTokens: 157333

Compaction events (from session JSONL):
  tokensBefore: 203780  "fromHook": true
  tokensBefore: 241887  "fromHook": true  
  tokensBefore: 271180  "fromHook": true
  tokensBefore: 256952  "fromHook": true
```

All compactions were triggered by the safeguard hook, not by the standard compaction flow that would check memory flush first.

### Threshold Math (gpt-5.2, 400k context)

- Memory flush threshold: `400k - 20k (reserve) - 4k (soft) = 376k`
- Session tokens before reply: 157k → Well below 376k threshold
- Safeguard compaction triggers when context exceeds model limits during generation

The gap between 157k and 376k means memory flush NEVER triggers, while safeguard compaction handles the overflow internally.

### Suggested Fix Extension

The proposed fix of estimating incoming tokens is good, but for safeguard mode specifically, consider:
1. Running memory flush as part of the `session_before_compact` hook chain (before safeguard summarization)
2. Or: lowering the memory flush threshold when safeguard mode is enabled

Happy to provide more diagnostic data if helpful.


## Links

- None detected yet
