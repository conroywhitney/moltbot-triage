---
number: 1695
title: "Clawd not working with LMStudio"
author: yuicbot
created: 2026-01-25T05:06:18Z
updated: 2026-01-28T12:58:44Z
labels: ["bug"]
assignees: []
comments_count: 16
reactions_total: 3
url: https://github.com/moltbot/moltbot/issues/1695
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

**Version:** 2026.1.23-1 (also tested beta)

**Issue:** Custom provider `api` field arrives as `undefined` in `mapOptionsForApi`

**Config (models.providers.lmstudio):**
json
{
  "baseUrl": "http://127.0.0.1:1234/v1",
  "apiKey": "local",
  "api": "openai-responses",
  "models": [...]
}
**Primary model:** `lmstudio/mistralai/mistral-7b-instruct-v0.3`

**Error:**

Unhandled API in mapOptionsForApi: undefined
    at mapOptionsForApi (pi-ai/src/stream.ts:471:10)
**Notes:**
- LM Studio responds correctly via curl
- `clawdbot models status` shows correct config
- `api` field is set but not reaching stream code

## Comments

### @beboxos (2026-01-25)

i have the same issue :'( hope a solution too

### @kpiotrowicz2510 (2026-01-25)

In Clawdbot.json you have to make such configuration, works for me:
"models": [
          {
            "id"

have to be without lmstudio at start, because then the model loaded will be "lmstudio/lmstudio/model"


"models": {
    "providers": {
      "lmstudio": {
        "baseUrl": "http://192.168.8.144:1234/v1",
        "apiKey": "LMSTUDIO_KEY",
        "api": "openai-completions",
        "models": [
          {
            "id": "minimax-m2.1-gs32",
            "name": "MiniMax M2.1",
            "reasoning": false,
            "input": [
              "text"
            ],
            "cost": {
              "input": 0,
              "output": 0,
              "cacheRead": 0,
              "cacheWrite": 0
            },
            "contextWindow": 20000,
            "maxTokens": 8192
          }
        ]
      }
    }
  },
  "agents": {
    "defaults": {
      "model": {
        "primary": "lmstudio/minimax-m2.1-gs32"
      },
      "models": {
        "lmstudio/minimax-m2.1-gs32": {
          "alias": "Minimax"
        }
      },
      "maxConcurrent": 4,
      "subagents": {
        "maxConcurrent": 8
      }
    },
    "list": [
      {
        "id": "main"
      },
      {
        "id": "agent1",
        "name": "agent1",
        "workspace": "/Users/krzysiek/clawd-agent1",
        "agentDir": "/Users/krzysiek/.clawdbot/agents/agent1/agent"
      }
    ]
  },

### @beboxos (2026-01-25)

thanks that worked for me 

### @gxlopez (2026-01-25)

> thanks that worked for me

Hi, I can't seem to connect lmstudio; I've tried everything. Does it have to be Minimax? Can't I try with other models? What should I do with the models.json and auth_profiles.json files? I've checked the documentation, the Discord server, and used chatgpt. I still can't get clawbot to recognize lmstudio.

### @kpiotrowicz2510 (2026-01-25)

Change the model name when hosting a server to minimax and then it works :)

### @Dviros (2026-01-26)

Thanks @kpiotrowicz2510 you helped me!

### @amr-coding (2026-01-26)

> > thanks that worked for me
> 
> Hi, I can't seem to connect lmstudio; I've tried everything. Does it have to be Minimax? Can't I try with other models? What should I do with the models.json and auth_profiles.json files? I've checked the documentation, the Discord server, and used chatgpt. I still can't get clawbot to recognize lmstudio.

Asking the same question, can I try with other models?
Does it have to be Minimax?


Here is the what I have on my clawbot.json:

```
  "models": {
    "mode": "merge",
    "providers": {
      "deepseek": {
        "baseUrl": "http://192.168.0.172:1234/v1",
        "apiKey": "LMSTUDIO_KEY",
        "api": "openai-completions",
        "models": [
          {
            "id": "deepseek-chat",
            "name": "DeepSeek Chat"
          },
          {
            "id": "liquid/lfm2.5-1.2b",
            "name": "lm lfm",
            "input": ["text"]
          }
        ]
      }
    }
  },
  "agents": {
    "defaults": {
      "model": {
        "primary": "lmstudio/liquid/lfm2.5-1.2b"
      },
```

### @lucaxdev01 (2026-01-26)

i have the same issue :'( hope a solution too

### @gxlopez (2026-01-27)

> i have the same issue :'( hope a solution too

It's easier than I thought. The trick is to start the lmstudio server using the command line (CLI) and rename your downloaded model to "minimax". For example, I have the openai/gpt-oss-20b model downloaded on lmstudio (you can see your downloaded models in the terminal with the command "lms ls"; if they don't all appear the first time, enter the command again). Then run:

lms load openai/gpt-oss-20b --identifier="minimax"

The server will start the model with the name "minimax". With this, you can configure the clawbot.json file.

Restart the gateway, and you should be able to use any model you need, including those with vision mode.

My clawbot.json part example is (Replace the code you have with yours, making sure it fits with the rest of the code.)

```
  "models": {
    "providers": {
      "lmstudio": {
        "baseUrl": "http://127.0.0.1:1234/v1",
        "apiKey": "LMSTUDIO_KEY",
        "api": "openai-completions",
        "models": [
          {
            "id": "minimax",
            "name": "minimax",
            "reasoning": false,
            "input": [
              "text"
            ],
            "cost": {
              "input": 0,
              "output": 0,
              "cacheRead": 0,
              "cacheWrite": 0
            },
            "contextWindow": 128000,
            "maxTokens": 8192
          }
        ]
      }
    }
  },
  "agents": {
    "defaults": {
      "model": {
        "primary": "lmstudio/minimax"
      },
      "models": {
        "lmstudio/minimax": {
          "alias": "minimax"
        }
      },
      "workspace": "C:\\Users\\user\\clawd",
      "compaction": {
        "mode": "safeguard"
      },
      "maxConcurrent": 4,
      "subagents": {
      "maxConcurrent": 8
      }
    }
  },
```

And my auth-profiles.json in C:\Users\user\.clawdbot\agents\main\agent 

```
{
  "default": {
    "type": "openai-compatible",
    "apiKey": "no-need",
    "baseUrl": "http://127.0.0.1:1234/v1/"
  }
}
```

the models.json  in C:\Users\user\.clawdbot\agents\main\agent is generated/modified automatically with when gateway starts.

This works with whatsapp me (own number) and friends can chat with my local model.
 

### @hernanpopper (2026-01-27)

Does this bug affect also using a remote ollama instance? 

### @jacobpretorius (2026-01-27)

Got the same problem, can't get it to work with LM studio. Tried hosting the model with an identifier as suggested but doesn't work for me 

### @Siber704 (2026-01-27)

Had issues configuring things manually but
using Clawdbot with LM Studio right now, initilized it with Gemini and then told it to add LM Studio models from my home server and voila : ) hope this helps

### @gxlopez (2026-01-27)

> Got the same problem, can't get it to work with LM studio. Tried hosting the model with an identifier as suggested but doesn't work for me

I tried connecting to LM Studio for two days and finally got it working (I'm working on Windows 11). I recommend backing up the clawbot.json file and deleting it to create a new one with the command "clawdbot onboard," following the usual steps.

I use VS Code to modify the JSON file to see the typing errors.

### @LisaMacintosh (2026-01-28)

I have MiniMax 2.1 working from lmstudio but for some reason it keeps returning empty contents when it tries to use tools; I can only talk to it.

I don't have any issues like this when running OSS instead; anyone else having this issue?

### @nulone (2026-01-28)

I'll take this.

Found root cause: in resolveModel(), when modelRegistry.find() returns a model, provider config models.providers.*.api is ignored â€” registry model used as-is.

Fix: shallow clone + merge provider config api when model.api is missing. PR incoming.

### @nulone (2026-01-28)

PR submitted: #3322 


## Links

- None detected yet
