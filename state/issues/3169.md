---
number: 3169
title: "[Feature]: Replace linear weighted score blend with Reciprocal Rank Fusion (RRF) in hybrid search"
author: hierosir1984
created: 2026-01-28T06:13:49Z
updated: 2026-01-28T15:09:38Z
labels: ["enhancement", "extensions: memory-core"]
assignees: []
comments_count: 0
reactions_total: 0
url: https://github.com/moltbot/moltbot/issues/3169
duplicate_of: null
related_issues: [2100]
blocks: []
blocked_by: []
---

## Description

## Summary

The current hybrid search implementation in `src/memory/hybrid.ts` uses a **linear weighted score blend** to combine vector and keyword results:

```typescript
score = vectorWeight * vectorScore + textWeight * textScore
// Default: 0.7 * vectorScore + 0.3 * textScore
```

This has a well-documented problem: **score distributions from different retrievers are not comparable.** Cosine similarity scores (typically 0.6–0.95) occupy a fundamentally different range than BM25-derived scores (transformed via `1 / (1 + rank)`). A linear blend implicitly assumes these scores are on the same scale, which they are not.

Consequences:
- **Vector results dominate:** With a 0.7 weight and typically higher absolute scores, vector results almost always rank above keyword results regardless of actual relevance
- **BM25 results are suppressed:** A perfect BM25 match (rank 0 → score 1.0) produces a blended score of 0.3, while a mediocre vector match (0.7) produces 0.49 — the mediocre vector result wins
- **Tuning is fragile:** Optimal weights depend on corpus, embedding model, and query type

This is closely related to the concern raised in #2100.

## Proposed solution

Replace the linear blend with **Reciprocal Rank Fusion (RRF)**, which operates on ranks rather than scores:

```
RRF_score(d) = Σ  1 / (k + rank_i(d))
               i∈retrievers
```

Where `k` is a constant (typically 60) and `rank_i(d)` is the 1-indexed rank of document `d` in retriever `i`'s result list.

Implementation sketch:

```typescript
export function mergeRRF(params: {
  vector: HybridVectorResult[];
  keyword: HybridKeywordResult[];
  k?: number;
}): HybridMergedResult[] {
  const k = params.k ?? 60;
  const byId = new Map<string, { entry: any; score: number }>();

  for (const [rank, r] of params.vector.entries()) {
    byId.set(r.id, { entry: r, score: 1 / (k + rank + 1) });
  }

  for (const [rank, r] of params.keyword.entries()) {
    const existing = byId.get(r.id);
    const rrfScore = 1 / (k + rank + 1);
    if (existing) {
      existing.score += rrfScore;
      if (r.snippet?.length > 0) existing.entry.snippet = r.snippet;
    } else {
      byId.set(r.id, { entry: r, score: rrfScore });
    }
  }

  return Array.from(byId.values())
    .sort((a, b) => b.score - a.score)
    .map(({ entry, score }) => ({ ...entry, score }));
}
```

Suggested config:

```json5
{
  "memorySearch": {
    "query": {
      "hybrid": {
        "enabled": true,
        "fusion": "rrf",        // NEW: "rrf" | "weighted" (default could be either)
        "rrfK": 60,             // NEW: RRF constant k
        // Legacy fields still work when fusion: "weighted"
        "vectorWeight": 0.7,
        "textWeight": 0.3
      }
    }
  }
}
```

## Alternatives considered

- **Score normalization** (min-max or z-score before blending): Helps but still assumes meaningful score magnitudes across retrievers. RRF is strictly more robust.
- **Learned weights**: Requires training data. RRF is parameter-free (k=60 works well across domains).
- **Keep current approach**: Works for small corpora but degrades as memory grows and query diversity increases.

## Additional context

Evidence and industry adoption:
- **Original paper:** Cormack, Clarke, Büttcher (2009) — "Reciprocal Rank Fusion outperforms Condorcet and individual Rank Learning methods" (SIGIR 2009)
- **Elasticsearch**: uses RRF as default hybrid fusion
- **Weaviate**: implements RRF for hybrid search
- **Pinecone**: recommends RRF over linear combination
- **Azure AI Search**: uses RRF for hybrid ranking
- **Benchmarks**: RRF consistently outperforms linear blending across BEIR, MS MARCO, with typical 5-15% improvements in nDCG@10

Backward compatibility:
- Non-breaking: legacy `vectorWeight`/`textWeight` configs continue to work when `fusion: "weighted"`
- `minScore` threshold documentation would need updating since RRF scores have a different range (~0–0.033 vs 0–1)

The docs already acknowledge this — the hybrid search documentation notes "common next steps are Reciprocal Rank Fusion (RRF)" as a future improvement.

Related: #2100

## Comments


## Links

- None detected yet
