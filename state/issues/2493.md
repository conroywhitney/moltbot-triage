---
number: 2493
title: "TUI shows no output for chat.send runs (chat events not emitted)"
author: 1EchA
created: 2026-01-27T01:33:10Z
updated: 2026-02-01T19:20:00Z
labels: ["bug"]
assignees: []
comments_count: 17
reactions_total: 5
url: https://github.com/openclaw/openclaw/issues/2493
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

### Summary
TUI connects and `chat.send` returns started, but no assistant output is shown in the TUI. CLI or agent runs complete and logs show replies.

### Repro
1. Start gateway.
2. Run `clawdbot tui --url ws://127.0.0.1:18789 --session main --message \"hi\"`.
3. Observe no assistant output in TUI even though the run completes.

### Expected
TUI receives chat delta or final events and renders assistant output.

### Actual
No chat events are emitted, so TUI stays blank.

### Cause
`chat.send` never registers the run context or chat run mapping. `createAgentEventHandler` cannot resolve sessionKey for the runId, so it does not emit chat events for that run.

### Proposed Fix
In `src/gateway/server-methods/chat.ts`, when starting a `chat.send` run:
- `registerAgentRunContext(clientRunId, { sessionKey: p.sessionKey })`
- `context.addChatRun(clientRunId, { sessionKey: p.sessionKey, clientRunId })`

This mirrors behavior used in other chat entrypoints and restores chat event routing for TUI.

## Comments

### @1EchA (2026-01-27)

Additional context:
- Version: Clawdbot 2026.1.24-3 (885167d)
- OS: macOS (Apple Silicon)
- TUI: `clawdbot tui --url ws://127.0.0.1:18789 --session main --message "hi"`
- Non-TUI works: `clawdbot agent --session-id main --message "你好"` returns output in CLI
- Gateway logs show runs complete, but TUI stays blank until this fix.


### @lucasverra (2026-01-28)

<img width="1215" height="263" alt="Image" src="https://github.com/user-attachments/assets/65b6a855-30b2-4750-85de-71a1055c2c85" />

### @1EchA (2026-01-29)




> <img alt="Image" width="586.4000244140625" height="126.92500305175781" src="https://private-user-images.githubusercontent.com/11323803/541795598-65b6a855-30b2-4750-85de-71a1055c2c85.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njk2OTAwOTUsIm5iZiI6MTc2OTY4OTc5NSwicGF0aCI6Ii8xMTMyMzgwMy81NDE3OTU1OTgtNjViNmE4NTUtMzBiMi00NzUwLTg1ZGUtNzFhMTA1NWMyYzg1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTI5VDEyMjk1NVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTk4YTBmNzU4MzcxYWI2NDhmYTQyMDIzM2I2ODRiNzFmMTY1ZTI2NGJkZmIwNzE5OGFlOTFkYTliYWUyZmIxMTgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.x3IvLbGypFopZ1cbHoQ-aqVRBlon_WHwmx_ChSqK3mM">

sometimes u will see (no output) ,wait a sec then u will see the reply

### @xiuxiuxius (2026-01-29)

I also encountered the same problem

<img width="1270" height="648" alt="Image" src="https://github.com/user-attachments/assets/ceee2a85-2097-4bdf-a299-8a6f7e992b93" />

### @xiuxiuxius (2026-01-29)

> I also encountered the same problem
> 
> <img alt="Image" width="1270" height="648" src="https://private-user-images.githubusercontent.com/89114157/542208028-ceee2a85-2097-4bdf-a299-8a6f7e992b93.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njk2OTQ4NTAsIm5iZiI6MTc2OTY5NDU1MCwicGF0aCI6Ii84OTExNDE1Ny81NDIyMDgwMjgtY2VlZTJhODUtMjA5Ny00YmRmLWEyOTktOGE2ZjdlOTkyYjkzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTI5VDEzNDkxMFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg0ZDBmYjQ5MWIwZTYwODAyYzAyN2QzOGMwZmIzYTJjOGY3OGYxZWEwMGMwNjZkMTk3MTkzODVmMWM4ODBmZDAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.gWXXEXbV7qG3qqV0LWvklISkjMxBOGge8MYgXFlQdlg">

After switching to qwen, the terminal is normal

<img width="1558" height="634" alt="Image" src="https://github.com/user-attachments/assets/bed2cbef-c11f-42c6-8cb8-ced800d4511c" />

### @anajuliabit (2026-01-29)

same here 

### @User17745 (2026-01-29)

Facing the same issue. 

Did the setup using the bash script method:
`curl -fsSL https://molt.bot/install.sh | bash`

<img width="808" height="511" alt="Image" src="https://github.com/user-attachments/assets/a7b77f54-eb14-45b8-9e59-165f6a89d464" />

Thought it had to do with the LLM provider, so tried clean installs with both Gemini and GPT, but it didn't help.

Also, after exiting, tried running `moltbot -v`, but it turns out that the `moltbot` command is not recognized within the VM.

So I tried installing using the npm method as mentioned on the `https://www.molt.bot/` landing page, but it seems to still have the old `clawdbot` commands (or maybe they are just yet to rename those, I'm not sure), but it also seems to set up a separate instance of the clawdbot instead of replacing the moltbot instance that I have already supposidly installed.

Overall, I think the recent rebrand from clawdbot to moltbot may have introduced refactoring issues, resulting in the breaking of the setup script. OR, it could just be that we are all doing something wrong that we don't realize yet.

### @jalapeno777 (2026-01-29)

It's frequently an issue with reasoning with some models.   The json returned by some LLMs will put their reasoning output in a different json field than their normal output field.

### @juliandavidmr (2026-01-29)

I’m facing the same issue. I’ve reinstalled MoltBot approximately five times using various methods, utilizing the curl/bash script and then installing it globally using npm. Finally, I attempted to install it locally by downloading the source code and executing all commands using `pnpm`. Before each installation, I consistently deleted all hidden folders and created files corresponding to the previous version that was installed.

I tried using Opus 4.5 and Sonnet 4.5. The first time I installed Moltbot was when I was using ~Clowdbot~ with Sonnet 4.5, and everything worked fine. I then reinstalled all of them using the recommended installation method, but this is the time when I can no longer use Moltbot.

[Related to](https://github.com/moltbot/moltbot/issues/2493#issuecomment-3819899067):
> It's frequently an issue with reasoning with some models. The json returned by some LLMs will put their reasoning output in a different json field than their normal output field.

I tried activating/deactivating reasoning mode; I can't use Moltbot yet.

<img width="695" height="418" alt="Image" src="https://github.com/user-attachments/assets/1d3e0936-da92-484d-bd06-48414d4d31e7" />


### @1EchA (2026-01-30)

> > I also encountered the same problem我也遇到了同样的问题。
> > <img alt="Image" width="791" height="403.59375" src="https://private-user-images.githubusercontent.com/89114157/542208028-ceee2a85-2097-4bdf-a299-8a6f7e992b93.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njk2OTQ4NTAsIm5iZiI6MTc2OTY5NDU1MCwicGF0aCI6Ii84OTExNDE1Ny81NDIyMDgwMjgtY2VlZTJhODUtMjA5Ny00YmRmLWEyOTktOGE2ZjdlOTkyYjkzLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMjklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTI5VDEzNDkxMFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg0ZDBmYjQ5MWIwZTYwODAyYzAyN2QzOGMwZmIzYTJjOGY3OGYxZWEwMGMwNjZkMTk3MTkzODVmMWM4ODBmZDAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.gWXXEXbV7qG3qqV0LWvklISkjMxBOGge8MYgXFlQdlg">
> 
> After switching to qwen, the terminal is normal切换到 qwen 后，终端正常。
> 
> <img alt="Image" width="822" height="334.484375" src="https://private-user-images.githubusercontent.com/89114157/542231394-bed2cbef-c11f-42c6-8cb8-ced800d4511c.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Njk3NDAxMjEsIm5iZiI6MTc2OTczOTgyMSwicGF0aCI6Ii84OTExNDE1Ny81NDIyMzEzOTQtYmVkMmNiZWYtYzExZi00MmM2LThjYjgtY2VkODAwZDQ1MTFjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNjAxMzAlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjYwMTMwVDAyMjM0MVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTQ1NTkwYmM5NzYwNWFhN2QwZmEzNjJlYTJjMzA2NDAzZjc5NGZmMzkyYzU0MjE5ODhhNDgwNDA4NWUxNGMyODMmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.QUo2YVL5tGD7jEgQg43CkU6MOPv1cy-ZJXtTznfzygY">

不知道为什么会有这个问题，有的时候输出no output是因为他不显示思考过程，模型没来得及回复no output就出来了，等一会他会自己返回给你回答，我接入的codex偶尔会出现这种问题

### @1EchA (2026-01-30)

> It's frequently an issue with reasoning with some models. The json returned by some LLMs will put their reasoning output in a different json field than their normal output field.这通常是某些模型推理过程中会出现的问题。一些 LLM 返回的 JSON 数据会将推理结果放在与常规输出字段不同的字段中。

agree

### @ingjae (2026-01-30)

it's same at openclaw newest version 

<img width="908" height="171" alt="Image" src="https://github.com/user-attachments/assets/95e47caa-ac85-4312-8210-7bf3499e2692" />

### @ajmeese7 (2026-01-30)

Confirmed on my end as well after doing a fresh install via `curl -fsSL --proto '=https' --tlsv1.2 https://openclaw.ai/install.sh | bash -s -- --install-method git`, currently on `2026.1.29`:

<img width="1368" height="804" alt="Image" src="https://github.com/user-attachments/assets/17f4d838-2526-45f7-8d2b-63e083ab8f7a" />

### @SPIKESPIGEL404 (2026-01-31)

Same here, have anyone found the root cause & solution?

### @User17745 (2026-01-31)

Okay, guys, so I don't really know how, but I think I have gotten it to work for real this time.

What I did:

- Terminated the existing AWS EC2 and created a new one, picked `m7i-flex.large` as usual.
- Did NOT associate an Elastic IP with the instance this time (which I always used to associate earlier).
- Used the AWS web-based terminal and not the usual local SSH pem key-based access (which is also different from all my previous attempts).
- Ran the latest install bash command `curl -fsSL https://openclaw.ai/install.sh | bash`
- Used OpenRouter's `openrouter/z-ai/glm-4.5-air:free` model instead of OpenAI / Gemini API keys.
- Skipped all the **skills** and **hooks** setup.

And... it looks to be working:

<img width="1470" height="533" alt="Image" src="https://github.com/user-attachments/assets/9f8a2973-7030-45f8-9c36-96387e125f3b" />

Was this a server config issue, a network config issue, a model selection issue, an OpenClaw config issue, etc.? I don't really know, but I hope someone smarter than me will be able to point out.

### @ajmeese7 (2026-01-31)

@User17745 I did a full refresh on my local install (deleting workspaces, configs, uninstalling, etc.) and set up OpenClaw fresh with Opus 4.5 and it still doesn't work for me, so I definitely still think something is broken. These same steps worked for me twice before, with the same model and everything, so this feels like a new regression

### @User17745 (2026-02-01)

@ajmeese7 my hunch is that it has something to do with the LLM connection. For some reason, nothing other than `openrouter/z-ai/glm-4.5-air:free` has ever worked for me. Re-installed and re-configured at least 5 times at this point (since my last post).

Do you think you can get a free API key from OpenRouter and try a fresh setup with the same model to share your observation?

Also, as a side note, I think switching the model via the `configure` command is either somewhat broken, or maybe I just don't know how to use it; it just doesn't work for me. So I end up doing a fresh setup each time I want to experiment.


## Links

- None detected yet
