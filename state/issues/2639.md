---
number: 2639
title: "[Feature]: Add support for dropping unsupported parameters when using OpenAI-compatible proxies (LiteLLM)"
author: yiethan0618-lab
created: 2026-01-27T07:40:48Z
updated: 2026-01-27T07:40:48Z
labels: ["enhancement"]
assignees: []
comments_count: 0
reactions_total: 0
url: https://github.com/moltbot/moltbot/issues/2639
duplicate_of: null
related_issues: [1023,2305]
blocks: []
blocked_by: []
---

## Description

 Summary
When using Clawdbot with an OpenAI-compatible proxy (like LiteLLM), the agent fails with the error:
LLM request rejected: store: Extra inputs are not permitted
This happens because Clawdbot's Pi agent sends the `store` parameter (an OpenAI-specific feature), but LiteLLM/other proxies don't support it and return a validation error.
## Steps to Reproduce
1. Configure a custom OpenAI-compatible provider in `~/.clawdbot/clawdbot.json`:
```json5
{
  "models": {
    "providers": {
      "my-proxy": {
        "baseUrl": "https://my-litellm-proxy.com",
        "apiKey": "sk-xxx",
        "api": "openai-completions",
        "models": [
          {
            "id": "claude-opus-4-5",
            "name": "Claude Opus 4.5",
            "input": ["text", "image"],
            "contextWindow": 200000,
            "maxTokens": 8192
          }
        ]
      }
    }
  },
  "agents": {
    "defaults": {
      "model": { "primary": "my-proxy/claude-opus-4-5" }
    }
  }
}
2. Run the agent:
clawdbot agent --session-id main --message "Hello"
3. Error occurs:
LLM request rejected: store: Extra inputs are not permitted
Expected Behavior
Clawdbot should either:
1. Provide a config option to disable/drop unsupported parameters (like store) for custom providers
2. Automatically detect when a proxy doesn't support certain parameters and omit them
3. Add a dropUnsupportedParams: true or similar option in the provider config
Proposed Solution
Add a configuration option like:
{
  "models": {
    "providers": {
      "my-proxy": {
        "baseUrl": "https://my-litellm-proxy.com",
        "apiKey": "sk-xxx",
        "api": "openai-completions",
        "dropParams": ["store"],  // Option 1: explicit list
        // or
        "strictOpenAI": false,    // Option 2: boolean flag
        // or  
        "litellmProxy": true,     // Option 3: preset for LiteLLM compatibility
        "models": [...]
      }
    }
  }
}
Use Case
Many users run LiteLLM as a proxy to:
- Route requests to different backends (Anthropic, OpenAI, local models)
- Implement fallback/load balancing
- Use corporate API gateways with unified authentication
These proxies are OpenAI-compatible but don't support all OpenAI-specific parameters.
Environment
- Clawdbot version: 2026.1.24-3
- OS: macOS
- Proxy: LiteLLM-based API gateway
Related Issues
- #2305 - Feature Request: Custom baseUrl for model providers (OpenAI-compatible proxies)
- #1023 - Bug: Having error with LiteLLM
Workaround
Currently, the only workaround is to configure drop_params=True on the LiteLLM server side, but this isn't always possible when users don't have access to the proxy server configuration.

## Comments


## Links

- None detected yet
