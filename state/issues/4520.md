---
number: 4520
title: "safeguard compaction mode always produces \"Summary unavailable\" on large context windows"
author: al-munazzim
created: 2026-01-30T08:49:31Z
updated: 2026-01-30T08:49:31Z
labels: []
assignees: []
comments_count: 0
reactions_total: 1
url: https://github.com/openclaw/openclaw/issues/4520
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Environment
- Clawdbot `2026.1.24-3`, npm global install (Linux x64)
- Model: Claude Opus 4.5 (200k context window)
- Config: `agents.defaults.compaction.mode: "safeguard"`

## Problem

With `safeguard` compaction mode, **every** compaction summary comes back as:

> "Summary unavailable due to context limits. Older messages were truncated."

This is 100% reproducible — 4 out of 4 compactions in the same session transcript all failed to produce a summary. The summary has **never** successfully generated.

## Evidence from transcript

| Timestamp | tokensBefore | Summary generated? |
|-----------|-------------|---------|
| 2026-01-27T20:49:10Z | 181,292 | ❌ "Summary unavailable" |
| 2026-01-27T20:49:11Z | 128 | ❌ "Summary unavailable" |
| 2026-01-30T08:13:36Z | 185,166 | ❌ "Summary unavailable" |
| 2026-01-30T08:19:14Z | 191 | ❌ "Summary unavailable" |

Note: even the compaction with only 128 tokens fails, suggesting the issue is not purely about context fullness.

## Impact

- The pre-compaction memory flush (`memoryFlush.enabled: true`) fires correctly, but the agent receives no usable summary to persist
- All conversation context before compaction is permanently lost
- The agent's memory system (daily notes, MEMORY.md) cannot compensate because the flush turn also lacks context to save
- Effectively, `safeguard` mode silently destroys conversation history instead of preserving it

## Expected behavior

Safeguard mode should produce a meaningful compaction summary. The docs describe it as enabling "chunked summarization for very long histories," which implies it should handle large contexts better than default mode — not worse.

## Possible root cause

At ~181-185k tokens (out of 200k context window), there may not be enough headroom for the model to generate a summary. The chunked summarization may be failing silently and falling back to the generic "unavailable" message. The fact that even a 128-token context fails suggests there may also be a code path issue (e.g., the chunked summarizer always returning the fallback).

## Workaround

Switching to `compaction.mode: "default"` (not yet verified if this produces working summaries — will update).

## Comments


## Links

- None detected yet
