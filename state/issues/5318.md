---
number: 5318
title: "[Bug]: # GLM-4.7 Context Issues: Two Real-World Cases"
author: Nibblemor
created: 2026-01-31T10:15:28Z
updated: 2026-02-03T01:32:39Z
labels: ["bug"]
assignees: []
comments_count: 3
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/5318
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Case 1: PDF Processing Overflow

**Problem:**
Processing 313KB PDF directly with `zai/glm-4.7` → `Context overflow: prompt too large for the model`

**Root Cause:**
Passing large file content directly to model exceeds token limits (system + user + file + response).

**Solution:**
```python
import PyPDF2

def extract_text_from_pdf(pdf_path, output_path):
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        content = []
        for i, page in enumerate(reader.pages, 1):
            text = page.extract_text()
            if text.strip():
                content.append(f"## Page {i}\n\n{text}\n\n---\n\n")
        with open(output_path, 'w', encoding='utf-8') as out:
            out.write(''.join(content))
```

**Pattern:** Document → Extract (tool) → Analyze (model if needed)

---

## Case 2: Gateway Context Window Saturation

**Problem:**
Gateway stops responding during long conversations when context window fills up - no reply sent.

**Root Cause:**
Model reaches max context capacity during active session, blocking responses.

**Solution:**
```yaml
# Adjust context limits in gateway config
models:
  glm-4.7:
    maxTokens: 180  # Reduced from 200 (10% buffer)
```

**Implementation:**
- Set maxTokens 10% below model's rated capacity
- Implement proactive context cleanup before overflow
- Clear oldest messages when approaching limit

**Pattern:** Monitor → Prune → Maintain buffer

---

## Best Practices for GLM-4.7

1. **Large inputs:** Extract with tools first (PyPDF2, pdfplumber)
2. **Long conversations:** Set maxTokens at 80-90% of rated capacity
3. **Context management:** Proactive cleanup before saturation
4. **Monitoring:** Track token usage during sessions

## Key Takeaways
- **Extraction ≠ Cognition:** Use tools for extraction, LLM for analysis
- **Buffer is essential:** 10-20% headroom prevents failures
- **Proactive > Reactive:** Clean before overflow, not after

---

**Date:** Jan 31, 2026 | **Model:** zai/glm-4.7 | **Category:** Best Practices / Troubleshooting

## Comments

### @trevtravtrev (2026-02-01)

can you show your exact json setting file for this portion? having trouble getting it to work.

### @Nibblemor (2026-02-01)

# Memory Flush and Auto-Compaction Not Triggering - session.json compactionCount stays at 0

## Summary

The memory flush pre-compaction process and auto-compaction are not triggering automatically, despite correct configuration. Sessions grow until context overflow occurs, with `compactionCount` staying at 0. Only manual `/compact` commands work.

## Environment

- **OpenClaw Version:** 2026.1.30
- **Model:** zai/glm-4.7 (200k context window)
- **OS:** macOS (Darwin 24.6.0, arm64)
- **Node:** v24.3.0
- **Channel:** Telegram

## Configuration

```json
{
  "agents": {
    "defaults": {
      "model": {
        "primary": "zai/glm-4.7"
      },
      "workspace": "/Users/myusername/clawd",
      "contextTokens": 190000,
      "memorySearch": {
        "enabled": true,
        "provider": "local",
        "local": {
          "modelPath": "hf:nomic-ai/nomic-embed-text-v1-GGUF/nomic-embed-text-v1.Q5_K_M.gguf"
        }
      },
      "contextPruning": {
        "mode": "cache-ttl",
        "ttl": "1h"
      },
      "compaction": {
        "mode": "safeguard",
        "reserveTokensFloor": 20000,
        "memoryFlush": {
          "enabled": true,
          "softThresholdTokens": 3000,
          "prompt": "Write any lasting notes to memory/YYYY-MM-DD.md; reply with NO_REPLY if nothing to store.",
          "systemPrompt": "Session nearing compaction. Store durable memories now."
        }
      }
    }
  }
}
```

## Expected Behavior

According to [Session Management documentation](https://docs.openclaw.ai/reference/session-management-compaction):

1. When session crosses `contextWindow - reserveTokensFloor - softThresholdTokens` (177k tokens)
2. Memory flush should execute as a silent turn
3. Write to `memory/YYYY-MM-DD.md`
4. Reply with `NO_REPLY`
5. Auto-compaction should trigger
6. Reduce context size

**Expected threshold:**
```
contextWindow (200k) - reserveTokensFloor (20k) - softThresholdTokens (3k) = 177k
```

## Actual Behavior

1. ❌ Memory flush NEVER executes automatically
2. ❌ Auto-compaction NEVER triggers
3. ❌ `compactionCount` stays at 0
4. ❌ No entries written to `memory/YYYY-MM-DD.md` during session growth
5. ✅ Manual `/compact` command works correctly
6. ✅ Manual memory writes work

### Evidence

**Session shows:**
```
Session: agent:main:main
Context: 75k/190k (39%)
Compactions: 0
```

**Memory file verified:**
- Checked `/Users/myusername/clawd/memory/2026-02-01.md`
- Last entry: 16:50 (manual entry)
- Session continued growing until 75k tokens
- No automatic memory flush occurred
- `compactionCount: 0` in sessions.json

**Manual test:**
- User executed `/compact` manually
- Compaction worked: 75k → 15k
- But no memory flush was triggered beforehand

## Steps to Reproduce

1. Start a new session
2. Have a long conversation (50+ messages)
3. Use tools that generate large outputs (web_fetch, read, etc.)
4. Let session grow beyond 177k tokens
5. Check `sessions.json` → `compactionCount: 0`
6. Check `memory/YYYY-MM-DD.md` → no automatic entries
7. Eventually get "Context overflow: prompt too large for the model" error

## Related Issues

- **#5433** - Auto-compaction overflow recovery not triggering (1 day old)
  - Shows exact same symptom: `compactionCount: 0`
  - Context exceeds limit without triggering

- **#3154** - Context overflow does not trigger automatic session recovery (5 days old)
  - Same problem reported
  - No auto-recovery mechanism

## Hypothesis

**Possible root causes:**

1. **Broken monitoring:** OpenClaw not correctly monitoring `contextTokens` after each turn
2. **Threshold mismatch:** Pi runtime using different threshold than OpenClaw expects
3. **Missing event handler:** Pi runtime emits compaction events but OpenClaw not listening
4. **Counter bug:** `contextTokens` counter in sessions.json is "best-effort" and may be stale/incorrect

According to documentation:
> "contextTokens in the store is a runtime estimate/reporting value; don't treat it as a strict guarantee."

This suggests the counter may not accurately reflect real context usage.

## Impact

- **Severity:** High - affects all long-running sessions
- **Frequency:** Every session exceeds threshold without auto-recovery
- **Workaround:** Manual `/compact` commands required
- **Data loss risk:** Context overflow without memory flush means important information may be lost

## Additional Context

**Attempted fixes:**
1. Enabled `contextPruning` with `cache-ttl` mode (1h TTL)
2. Increased `contextTokens` from 128k → 190k to align with model's 200k window
3. Verified configuration is correct per documentation

**Result:** Memory flush and auto-compaction still not triggering automatically.

**Current workaround:**
```bash
/compact "Focus on decisions, tasks, and pending items"
```

## Documentation References

- [Session Management Deep Dive](https://docs.openclaw.ai/reference/session-management-compaction)
- [Memory Concepts](https://docs.openclaw.ai/concepts/memory)
- [Compaction Concepts](https://docs.openclaw.ai/concepts/compaction)


### @trevtravtrev (2026-02-03)

Do you have a workaround for this yet that isn't manual? Mine is happening in the main session due to heartbeats, so about every 2 hours my main session completely breaks due to heartbeats somehow accumulating. I've been playing around with different configs and nothing has worked so far.


## Links

- None detected yet
