---
number: 4298
title: "OpenAI-compatible streaming: missing terminal chunk before [DONE] can crash some clients"
author: perryraskin
created: 2026-01-30T01:30:20Z
updated: 2026-01-30T01:30:34Z
labels: []
assignees: []
comments_count: 0
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/4298
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

### Bug
When using the gateway OpenAI-compatible **streaming** endpoint (SSE), Moltbot currently emits one or more `chat.completion.chunk` events with `finish_reason: null` and then immediately sends `data: [DONE]`.

Some OpenAI-compatible clients expect a **terminal chunk** (same shape as a normal chunk, but with an empty `delta` and `finish_reason: "stop"`) *before* `[DONE]`. When that terminal chunk is missing, those clients can throw / crash while finalizing the stream.

### Where
`src/gateway/openai-http.ts` (SSE streaming path in `handleOpenAiHttpRequest`)

### Expected
Before writing `[DONE]`, send a final chunk like:
```json
{
  "object": "chat.completion.chunk",
  "choices": [{ "index": 0, "delta": {}, "finish_reason": "stop" }]
}
```
Then send `[DONE]`.

### Current behavior
We send `[DONE]` directly on lifecycle `end/error` or in the `finally` cleanup, without a terminal chunk.

### Proposed fix
Add a helper to build the terminal chunk and centralize shutdown in an `endStream()` function that:
- ensures we only close once
- writes the terminal chunk exactly once (if it hasnâ€™t already been sent)
- then writes `[DONE]`, unsubscribes, and ends the response

Sketch (from local patch):
```ts
function buildFinalChunk({ runId, model }: { runId: string; model: string }) {
  return {
    id: runId,
    object: "chat.completion.chunk",
    created: Math.floor(Date.now() / 1000),
    model,
    choices: [{ index: 0, delta: {}, finish_reason: "stop" }],
  };
}

let sentTerminalChunk = false;
const endStream = () => {
  if (closed) return;
  closed = true;
  if (!sentTerminalChunk) {
    sentTerminalChunk = true;
    writeSse(res, buildFinalChunk({ runId, model }));
  }
  unsubscribe();
  writeDone(res);
  res.end();
};
```

Then call `endStream()` from lifecycle `end/error` and from the main `finally`.

### Notes
This keeps our output compatible with strict OpenAI streaming parsers while remaining fine for tolerant clients.

## Comments


## Links

- None detected yet
