---
number: 7351
title: "Feature request: per-model context budget for fallback chain"
author: mkoslacz
created: 2026-02-02T18:38:55Z
updated: 2026-02-02T18:38:55Z
labels: []
assignees: []
comments_count: 0
reactions_total: 2
url: https://github.com/openclaw/openclaw/issues/7351
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Problem

When using a fallback chain with models of different context window sizes, there's no way to automatically compress/prune context to fit smaller models.

## Use case

**Primary model:** claude-opus-4-5 (200k context)
**Desired emergency fallback:** local Ollama model (8k context)

Current session context: ~88k tokens. When primary fails and fallback triggers, the 8k model receives 88k tokens → immediate context overflow error.

## Current workarounds

- `contextPruning` only trims tool results (~20-30% reduction), not enough for 200k→8k
- `compaction` works reactively when context is full, not preemptively before fallback
- Only option is to exclude small-context models from fallback chain entirely

## Proposed solution

Add per-model context budget in fallback config:

```json
{
  "agents": {
    "defaults": {
      "model": {
        "fallbacks": [
          "openai/gpt-5-mini",
          {
            "model": "ollama/llama3.1:8b",
            "contextBudget": 8000,
            "pruneStrategy": "aggressive"
          }
        ]
      }
    }
  }
}
```

When falling back to a model with `contextBudget`, OpenClaw would:
1. Run aggressive compaction
2. Prune all tool results
3. Keep only last N messages that fit the budget
4. Send to the smaller model

This would enable true offline/emergency fallback to local models with small context windows.

## Comments


## Links

- None detected yet
