---
number: 3775
title: "[Bug]: Bootstrap files (SOUL.md, USER.md) not injected into system prompt for openai-completions API"
author: adriangrassi
created: 2026-01-29T05:20:07Z
updated: 2026-01-29T16:22:24Z
labels: []
assignees: []
comments_count: 2
reactions_total: 1
url: https://github.com/openclaw/openclaw/issues/3775
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

# [Bug]: Bootstrap files (SOUL.md, USER.md) not injected into system prompt for openai-completions API

## Description

When using a local LLM via Ollama with the `openai-completions` API, bootstrap files (SOUL.md, USER.md, IDENTITY.md) are **not injected** into the system prompt's Project Context section. The same configuration works correctly with Anthropic providers.

## Steps to Reproduce

1. Configure Ollama provider with `api: "openai-completions"`:
```json
{
  "models": {
    "providers": {
      "ollama": {
        "baseUrl": "http://127.0.0.1:11434/v1",
        "apiKey": "ollama-local",
        "api": "openai-completions",
        "models": [{
          "id": "qwen2.5:32b",
          "name": "qwen2.5:32b",
          "contextWindow": 131072,
          "maxTokens": 8192
        }]
      }
    }
  },
  "agents": {
    "defaults": {
      "model": { "primary": "ollama/qwen2.5:32b" },
      "workspace": "/path/to/workspace"
    }
  }
}
```

2. Create workspace files:
   - `SOUL.md` - Agent personality
   - `USER.md` - User profile with name, timezone, etc.
   - `IDENTITY.md` - Agent identity

3. Run agent and ask about user info:
```bash
clawdbot agent --agent main -m "What is my name and timezone?"
```

## Expected Behavior

Agent should know user's name and timezone from USER.md content injected into Project Context (works correctly with Anthropic).

## Actual Behavior

- Agent does NOT have USER.md content in its context
- Agent tries to use `memory_get` tool to fetch SOUL.md, USER.md, IDENTITY.md
- `memory_get` fails with `"error": "path required"` because these aren't memory paths
- Model hallucinates or says it doesn't have the information
- Input tokens are consistently ~4096 regardless of conversation length

## Session Transcript Evidence

```json
{"type":"message","message":{"role":"assistant","content":[
  {"type":"toolCall","name":"memory_get","arguments":{"path":"SOUL.md"}},
  {"type":"toolCall","name":"memory_get","arguments":{"path":"USER.md"}}
],"usage":{"input":4096,"output":68}}}

{"type":"message","message":{"role":"toolResult","content":[
  {"type":"text","text":"{\"path\": \"USER.md\", \"disabled\": true, \"error\": \"path required\"}"}
]}}
```

## Verification

Switching to Anthropic provider with same workspace:
```bash
clawdbot models set anthropic/claude-sonnet-4-5
clawdbot agent --agent main -m "What is my name and timezone?"
```

Result: Agent correctly responds with user's name and timezone from USER.md.

## Environment

- **Clawdbot version:** 2026.1.24-3
- **OS:** Windows 11
- **Ollama version:** Latest
- **Model:** qwen2.5:32b (also tested with llama3.1:8b)

## Analysis

The system prompt building code in `system-prompt.js` correctly includes `contextFiles` (lines 419-434), and `openai-completions.js` correctly adds system messages when `context.systemPrompt` exists (lines 398-402). However, somewhere in the chain between `buildEmbeddedSystemPrompt` and the actual API call, the system prompt content appears to not be reaching Ollama.

Direct curl calls to Ollama with system prompts work correctly:
```bash
curl http://127.0.0.1:11434/v1/chat/completions -d '{
  "model": "qwen2.5:32b",
  "messages": [
    {"role": "system", "content": "User is John in America/New_York timezone"},
    {"role": "user", "content": "What is my timezone?"}
  ]
}'
# Returns correct answer
```

## Workaround

Use Anthropic provider instead of Ollama for agents that need bootstrap file context.

## Comments

### @NuCl34R (2026-01-29)

Can confirm, ollama models seems clueless about who they are, they do not recognize being moltbot at all. Makes them unusable.

### @adriangrassi (2026-01-29)

## Update: Root Cause Found

After further investigation, we discovered there are **two separate issues**:

### Issue 1: `contextFiles` not passed to SDK (PR #3838)
The original issue where `contextFiles: []` was hardcoded instead of passing the resolved context files. This was fixed in PR #3838.

### Issue 2: Ollama silently truncates context (NEW - Issue #4028)
Even after applying the PR #3838 fix, bootstrap files were still missing. The root cause is that **Ollama silently truncates the context to its default `num_ctx` (typically 4096 tokens)**, regardless of the `contextWindow` configured in clawdbot.

**Evidence from Ollama logs:**
```
level=WARN msg="truncating input prompt" limit=4096 prompt=10573 keep=4 new=4096
```

The full system prompt (including Project Context with bootstrap files) was being sent, but Ollama truncated it before the model saw it.

### Workaround
Create a custom Ollama model with `num_ctx` baked in via Modelfile:
```bash
cat > Modelfile << 'EOF'
FROM qwen2.5:32b
PARAMETER num_ctx 16384
EOF
ollama create qwen2.5-16k -f Modelfile
```

Then update clawdbot config to use the new model.

### Related Issue
See #4028 for the Ollama context truncation issue and suggested fix (passing `num_ctx` via the OpenAI-compatible API).


## Links

- None detected yet
