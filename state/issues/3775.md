---
number: 3775
title: "[Bug]: Bootstrap files (SOUL.md, USER.md) not injected into system prompt for openai-completions API"
author: adriangrassi
created: 2026-01-29T05:20:07Z
updated: 2026-01-29T10:27:47Z
labels: []
assignees: []
comments_count: 1
reactions_total: 1
url: https://github.com/moltbot/moltbot/issues/3775
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

# [Bug]: Bootstrap files (SOUL.md, USER.md) not injected into system prompt for openai-completions API

## Description

When using a local LLM via Ollama with the `openai-completions` API, bootstrap files (SOUL.md, USER.md, IDENTITY.md) are **not injected** into the system prompt's Project Context section. The same configuration works correctly with Anthropic providers.

## Steps to Reproduce

1. Configure Ollama provider with `api: "openai-completions"`:
```json
{
  "models": {
    "providers": {
      "ollama": {
        "baseUrl": "http://127.0.0.1:11434/v1",
        "apiKey": "ollama-local",
        "api": "openai-completions",
        "models": [{
          "id": "qwen2.5:32b",
          "name": "qwen2.5:32b",
          "contextWindow": 131072,
          "maxTokens": 8192
        }]
      }
    }
  },
  "agents": {
    "defaults": {
      "model": { "primary": "ollama/qwen2.5:32b" },
      "workspace": "/path/to/workspace"
    }
  }
}
```

2. Create workspace files:
   - `SOUL.md` - Agent personality
   - `USER.md` - User profile with name, timezone, etc.
   - `IDENTITY.md` - Agent identity

3. Run agent and ask about user info:
```bash
clawdbot agent --agent main -m "What is my name and timezone?"
```

## Expected Behavior

Agent should know user's name and timezone from USER.md content injected into Project Context (works correctly with Anthropic).

## Actual Behavior

- Agent does NOT have USER.md content in its context
- Agent tries to use `memory_get` tool to fetch SOUL.md, USER.md, IDENTITY.md
- `memory_get` fails with `"error": "path required"` because these aren't memory paths
- Model hallucinates or says it doesn't have the information
- Input tokens are consistently ~4096 regardless of conversation length

## Session Transcript Evidence

```json
{"type":"message","message":{"role":"assistant","content":[
  {"type":"toolCall","name":"memory_get","arguments":{"path":"SOUL.md"}},
  {"type":"toolCall","name":"memory_get","arguments":{"path":"USER.md"}}
],"usage":{"input":4096,"output":68}}}

{"type":"message","message":{"role":"toolResult","content":[
  {"type":"text","text":"{\"path\": \"USER.md\", \"disabled\": true, \"error\": \"path required\"}"}
]}}
```

## Verification

Switching to Anthropic provider with same workspace:
```bash
clawdbot models set anthropic/claude-sonnet-4-5
clawdbot agent --agent main -m "What is my name and timezone?"
```

Result: Agent correctly responds with user's name and timezone from USER.md.

## Environment

- **Clawdbot version:** 2026.1.24-3
- **OS:** Windows 11
- **Ollama version:** Latest
- **Model:** qwen2.5:32b (also tested with llama3.1:8b)

## Analysis

The system prompt building code in `system-prompt.js` correctly includes `contextFiles` (lines 419-434), and `openai-completions.js` correctly adds system messages when `context.systemPrompt` exists (lines 398-402). However, somewhere in the chain between `buildEmbeddedSystemPrompt` and the actual API call, the system prompt content appears to not be reaching Ollama.

Direct curl calls to Ollama with system prompts work correctly:
```bash
curl http://127.0.0.1:11434/v1/chat/completions -d '{
  "model": "qwen2.5:32b",
  "messages": [
    {"role": "system", "content": "User is John in America/New_York timezone"},
    {"role": "user", "content": "What is my timezone?"}
  ]
}'
# Returns correct answer
```

## Workaround

Use Anthropic provider instead of Ollama for agents that need bootstrap file context.

## Comments

### @NuCl34R (2026-01-29)

Can confirm, ollama models seems clueless about who they are, they do not recognize being moltbot at all. Makes them unusable.


## Links

- None detected yet
