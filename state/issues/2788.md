---
number: 2788
title: "Feature Request: Recursive Language Model (RLM) integration for unbounded context"
author: ularity
created: 2026-01-27T14:27:46Z
updated: 2026-01-28T05:58:49Z
labels: ["enhancement"]
assignees: []
comments_count: 0
reactions_total: 0
url: https://github.com/moltbot/moltbot/issues/2788
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Summary

Consider integrating Recursive Language Models (RLM) as described in the recent arXiv paper (arxiv.org/abs/2512.24601) for handling arbitrarily long conversations without context window limitations.

## The Problem

Current memory architecture relies on:
1. **Compaction** â€” summarizing old context (lossy)
2. **File-based memory** â€” writing important stuff to markdown (requires manual/prompted writes)
3. **Session resets** â€” daily context wipes

This works, but long conversations still hit context limits and compaction loses detail.

## The RLM Approach

RLMs treat the full prompt as an **external environment variable** that the model queries programmatically:

- Store conversation history as variables in a REPL-like environment
- Model uses regex/queries to find relevant chunks on-demand
- Recursive sub-calls can handle 10M+ tokens without retraining
- No information loss from compaction

From the paper:
> "RLM(GPT-5) was the only approach to achieve and maintain perfect performance at 1,000 document scale, while base models showed clear performance degradation."

## Potential Integration

- Replace/augment compaction with RLM-style context retrieval
- Session transcripts become queryable environment variables
- Model decides what context to load per-request
- Could dramatically improve long-running agent sessions

## References

- Paper: https://arxiv.org/abs/2512.24601
- Blog: https://alexzhang13.github.io/blog/2025/rlm/
- Prime Intellect: https://www.primeintellect.ai/blog/rlm

Would love to see this explored for future versions. The "paradigm of 2026" indeed! ðŸ¦€

## Comments


## Links

- None detected yet
