---
number: 4350
title: "Model fallback not triggering when primary hits rate limits"
author: fwends
created: 2026-01-30T03:54:29Z
updated: 2026-01-30T06:13:05Z
labels: []
assignees: []
comments_count: 1
reactions_total: 1
url: https://github.com/openclaw/openclaw/issues/4350
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

When the primary model (e.g. openai-codex/gpt-5.1) hits rate limits, the fallback chain is not being used.

Config:
```json
"model": {
  "primary": "openai-codex/gpt-5.1",
  "fallbacks": ["anthropic/claude-opus-4-5", "anthropic/claude-sonnet-4-5", "anthropic/claude-haiku-4-5"]
}
```

Expected: When Codex returns 429/rate limit, automatically try the next fallback (Opus).
Actual: Stays stuck on the rate-limited model.

## Comments

### @Glucksberg (2026-01-30)

ðŸ”— **Related:** This appears to be the same class of issue as #4260 (provider-level cooldown blocking cross-provider fallback). PR #4312 addresses this by allowing fallback models from different providers to be tried even when the original provider is in cooldown.

Worth testing if #4312 resolves this scenario as well.


## Links

- None detected yet
