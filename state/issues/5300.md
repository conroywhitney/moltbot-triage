---
number: 5300
title: "EmbeddingGemma produces non-normalized vectors, breaking semantic search"
author: CamelSprout
created: 2026-01-31T09:18:06Z
updated: 2026-02-02T00:19:41Z
labels: ["bug"]
assignees: []
comments_count: 0
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/5300
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Summary

The local embedding provider using `embeddinggemma-300M-Q8_0.gguf` produces **non-L2-normalized vectors**, causing semantic search to return irrelevant results (same files regardless of query).

## Environment

- **OpenClaw version:** v2026.1.24-3
- **Model:** `hf_ggml-org_embeddinggemma-300M-Q8_0.gguf`
- **Provider:** `local` (node-llama-cpp)

## Problem

Embeddings should be L2-normalized (magnitude = 1.0) for cosine similarity to work correctly. Instead, EmbeddingGemma outputs vectors with magnitudes ranging from **10-15**:

```
# Expected (normalized)
[0.21, -0.03, -0.04, 0.10, ...]  magnitude â‰ˆ 1.0

# Actual (unnormalized)
[2.35, 3.45, 0.63, 4.30, ...]    magnitude â‰ˆ 12.5
```

This causes cosine similarity to be dominated by vector magnitude rather than semantic direction, resulting in the same high-magnitude chunks ranking first for all queries.

## Impact

- 100% of chunks affected (all 2821 in my database)
- Semantic search returns irrelevant results
- Same file (`tasks/homelab.md`) returned for unrelated queries like "musik spotify", "Ã¸konomi budget", "carnivore kÃ¸d"

## Workaround

Manual post-processing normalization fixes the issue:

```python
import sqlite3, json, math

def normalize(vec):
    mag = math.sqrt(sum(x*x for x in vec))
    return [x/mag for x in vec] if mag > 0 else vec

conn = sqlite3.connect("~/.clawdbot/memory/main.sqlite")
for id, emb in conn.execute("SELECT id, embedding FROM chunks"):
    normalized = normalize(json.loads(emb))
    conn.execute("UPDATE chunks SET embedding=? WHERE id=?", 
                 (json.dumps(normalized), id))
conn.commit()
```

## Suggested Fix

Normalize embeddings after model inference in the local embedding provider:

```typescript
// After getting embedding from node-llama-cpp
const magnitude = Math.sqrt(embedding.reduce((sum, x) => sum + x * x, 0));
const normalized = embedding.map(x => x / magnitude);
```

## Notes

- Attempted switching to `Qwen3-Embedding-8B-Q8_0.gguf` but it requires more VRAM than available
- The normalization fix needs to be applied after every `memory index --force` until this is patched

---

ðŸ¤– Generated with [Claude Code](https://claude.ai/code)

## Comments


## Links

- None detected yet
