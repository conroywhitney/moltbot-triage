---
number: 4899
title: "[Bug]: 10 second gateway latency overhead vs direct Ollama (local models)"
author: GalaxyBrain1
created: 2026-01-30T20:43:03Z
updated: 2026-01-31T06:02:38Z
labels: ["bug"]
assignees: []
comments_count: 3
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/4899
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Summary
I have been testing how OpenClaw would work on laptops, and there is significant latency when using OpenClaw.
OpenClaw gateway adds significant latency (~10 seconds) compared to direct Ollama inference, even with minimal thinking mode and optimized settings.

## Steps to reproduce:
1. Use any Ollama model (tested with falcon-h1r-7b, ~7.6B params, Q4_K_M)
2. Enable flash attention for Ollama along with Q8 quant for KV cache
3. Test direct Ollama speed: `ollama run <model>` → ~2.5 seconds per response
4. Test OpenClaw TUI (second prompt to eliminate cold start) → ~12+ seconds

## What is happening:
Direct Ollama: ~2.5 seconds average
Through OpenClaw: ~12+ seconds average (same prompt, same model)

The reasoning trace length and VRAM usage appear similar between both methods, so the overhead seems to come from the gateway processing itself rather than model inference.

### Attempted optimizations (no improvement):
- `thinkingDefault: minimal`
- `skipBootstrap: true`
- `typingMode: never`
- `debounceMs: 0`

## Environment:
- OpenClaw version: 2026.1.29
- OS: Windows 11 Home (WSL2)
- Install method: npx
- GPU: RTX 5070 Laptop (8GB VRAM)
- Model VRAM usage: ~6.5GB

## Comments

### @GalaxyBrain1 (2026-01-30)

I actually don't think it is a gateway latency, more so a full slowdown of the model. If I ask more complicated questions, it seems exponentially slower.

### @Vector-Cross (2026-01-31)

How do you configure and enable the local Ollama model?

### @GalaxyBrain1 (2026-01-31)

Heres my config json: {
  "models": {
    "providers": {
      "ollama": {
        "baseUrl": "http://127.0.0.1:11434/v1",
        "apiKey": "ollama-local",
        "api": "openai-completions",
        "models": [
          {
            "id": "your-model:latest",
            "name": "your-model",
            "reasoning": false,
            "contextWindow": 32768,
            "maxTokens": 4096
          }
        ]
      }
    }
  },
  "agents": {
    "defaults": {
      "model": {
        "primary": "ollama/your-model:latest"
      }
    }
  },
  "gateway": {
    "port": 18789,
    "mode": "local"
  }
}

As far as I know, I think this is pretty standard.


## Links

- None detected yet
