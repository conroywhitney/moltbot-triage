---
number: 7073
title: "[Feature]: Support Kimi K2.5 Cache"
author: vorbei
created: 2026-02-02T10:47:58Z
updated: 2026-02-02T21:18:17Z
labels: ["enhancement"]
assignees: []
comments_count: 1
reactions_total: 1
url: https://github.com/openclaw/openclaw/issues/7073
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Summary

The token usage more than expect with long contexts.

## Proposed solution

Kimi API supports Context caching via context_id, but current support is not available in openclaw.

```json                                                                                                                                                
   "kimi-for-coding": {                                                                                                                                 
     "cost": {                                                                                                                                          
       "input": 0,                                                                                                                                      
       "output": 0,                                                                                                                                     
       "cacheRead": 0,                                                                                                                                  
       "cacheWrite": 0   ← cache not using.
     }                                                                                                                                                  
   }                                                                                                                                                    
 ```  

## Comments

### @AkashaBot (2026-02-02)

Makes sense — Kimi’s `context_id` cache can dramatically cut *re-sending* long history and reduce both latency + billed tokens.

A concrete way to add this in OpenClaw (provider-agnostic):

## Proposed implementation (OpenClaw)
1) **Session-level context cache state**
- Store `{ providerKey, model, contextId, expiresAt }` on the conversation/session state.
- Only reuse when *the prompt prefix is identical* (system + tools schema + any static prelude). Otherwise start a new context.

2) **Provider capability flag**
- Add a capability like `supportsContextIdCache: true` for Kimi.
- In request building: if we have a valid `contextId`, send it; else omit.

3) **Cost accounting**
- If the provider returns cache metrics (read/write), map them into OpenClaw’s cost model (`cacheRead` / `cacheWrite`).
- If not returned explicitly, we can at least surface a boolean “cacheHit” in debug logs.

4) **Invalidation rules**
- Invalidate when:
  - system prompt changes
  - tool definitions/schema changes
  - model changes
  - provider returns “unknown/expired context_id”

## Questions / clarifications
- Which API path are you using for Kimi (direct Moonshot/Kimi API vs OpenRouter)?
  - OpenRouter may not expose `context_id` the same way, so we may need a direct-provider implementation first.
- Do you have an example request/response showing where `context_id` is returned and how it should be sent back?

Happy to open a PR once we confirm the exact wire format + where OpenClaw should persist the `context_id`.



## Links

- None detected yet
