---
number: 5769
title: "Ollama (and other local models): streaming breaks tool calling ‚Äî need stream:false fallback"
author: code-compliant
created: 2026-01-31T23:31:11Z
updated: 2026-02-02T19:37:30Z
labels: ["bug"]
assignees: []
comments_count: 2
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/5769
duplicate_of: null
related_issues: [9632,12557]
blocks: []
blocked_by: []
---

## Description

## Summary

When using Ollama-hosted models (e.g. Mistral Small 3.2 24B) via the `openai-completions` provider, **tool calling silently fails** because OpenClaw always sends `stream: true`. Ollama's streaming implementation does not properly emit `tool_calls` delta chunks ‚Äî the model decides to call a tool, but the streaming response returns empty content with `finish_reason: "stop"`, losing the tool call entirely.

This means **no local model can use tools through OpenClaw**, despite the models supporting tool calling perfectly in non-streaming mode.

## Evidence

### Non-streaming (works perfectly):
```bash
curl -s http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"mistral-small:24b","messages":[{"role":"user","content":"List open PRs"}],"tools":[{"type":"function","function":{"name":"exec","description":"Run a command","parameters":{"type":"object","properties":{"command":{"type":"string"}},"required":["command"]}}}]}'
```
Response: proper `tool_calls` array with `finish_reason: "tool_calls"` ‚úÖ

### Streaming (broken):
Same request with `"stream": true` added:
```
data: {"choices":[{"delta":{"role":"assistant","content":""},"finish_reason":"stop"}]}
data: [DONE]
```
Tool call completely lost. Empty response. ‚ùå

## Root Cause

In `@mariozechner/pi-ai/dist/providers/openai-completions.js`, line ~316:
```js
const params = {
    model: model.id,
    messages,
    stream: true,  // ‚Üê hardcoded, no way to disable
};
```

This is a [known Ollama limitation](https://github.com/ollama/ollama/issues/9632) affecting multiple models (Mistral, Qwen, etc.) ‚Äî tracked in ollama/ollama#9632 and ollama/ollama#12557. The recommended workaround from the Ollama community is to use `stream: false` when tools are present.

## Impact

- **All Ollama models** configured as custom providers cannot use tools (exec, web_search, browser, etc.)
- Cron jobs and subagents configured with local models get narrative essays instead of tool execution
- Heartbeat sessions with local models cannot perform any actions

## Proposed Fix

Add a per-provider or per-model config option to disable streaming, or auto-detect when the provider is Ollama (baseUrl contains `:11434` or provider is explicitly `ollama`) and fall back to non-streaming when tools are present in the request.

Something like:
```js
const shouldStream = !(context.tools?.length && isOllamaProvider(model));
const params = {
    model: model.id,
    messages,
    stream: shouldStream,
};
```

Or a model-level config option:
```json
{
  "id": "mistral-small:24b",
  "streamToolCalls": false
}
```

## Environment

- **OpenClaw:** v2026.1.29
- **Ollama:** v0.15.1
- **Model:** mistral-small:24b (Mistral Small 3.2)
- **OS:** Linux 6.14.0-37-generic (x64)
- **Provider config:** `api: "openai-completions"`, `baseUrl: "http://localhost:11434/v1"`

## Comments

### @jokelord (2026-02-02)

I have successfully applied the tool calling patch via LLM.üëá
https://github.com/jokelord/openclaw-local-model-tool-calling-patch

### @mdkrush (2026-02-02)

Same issue here... 


## Links

- None detected yet
