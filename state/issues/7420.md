---
number: 7420
title: "Feature Proposal: RLM-Style Query Delegation as Alternative to Compaction"
author: initiator1
created: 2026-02-02T20:06:40Z
updated: 2026-02-02T20:17:00Z
labels: []
assignees: []
comments_count: 0
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/7420
duplicate_of: null
related_issues: [2597,4121,4520,5224,5429,6622,6658,6757,7175]
blocks: []
blocked_by: []
---

## Description

## Summary

Propose adding RLM-style (Recursive Language Model) query patterns as an alternative to context compaction for preserving information fidelity.

## Problem

Context compaction via summarization loses information. Multiple issues document this pain:
- #5429: Lost 2 days of context to silent compaction
- #2597: Context/state lost after unexpected compaction
- #4520, #5224: "Summary unavailable" failures
- #4121: Summarization silently fails

For agents managing complex, long-term domains (life management, multi-project coordination), this information loss compounds over time.

## Proposed Solution: RLM-Style Query Delegation

Based on the [Recursive Language Models paper](https://arxiv.org/abs/2512.24601) (Zhang, Kraska, Khattab - Dec 2025), instead of summarizing context, treat long content as external variables that sub-agents query on demand.

### Key Insight from the Paper

> "Arbitrarily long user prompts should not be fed into the neural network directly but should instead be treated as part of the environment that the LLM is tasked to symbolically and recursively interact with."

### How it works with OpenClaw

Instead of:
```
[load MEMORY.md into context] → compaction → [lossy summary]
```

Do:
```
[keep MEMORY.md on disk] → spawn sub-agent query → [precise extraction] → discard sub-agent
```

OpenClaw already has the primitives:
- `sessions_spawn` for sub-agent delegation
- File system for external memory
- Cheaper/faster models (Haiku 4.5) for retrieval tasks

### Prototype Results

Tested with a real OpenClaw instance:

| Query | File | Time | Result |
|-------|------|------|--------|
| Breakfast supplements | MEMORY.md | 12s | ✅ Precise extraction (14 items) |
| Policy lookup | TOOLS.md | 7s | ✅ Exact quote retrieved |

**Main session context used: 0 bytes** — sub-agents loaded the files, extracted precisely, returned results.

### Model Comparison (UPDATE: Haiku 4.5 is optimal)

We tested Haiku 4.5 vs Sonnet 4.5 for retrieval tasks:

| Model | Time | Quality | Relative Cost |
|-------|------|---------|---------------|
| **Haiku 4.5** | **9s** | ✅ Excellent | 1x |
| Sonnet 4.5 | 19s | ✅ Excellent | ~3x |

**Finding:** For simple retrieval (read file → extract answer → return), Haiku 4.5 is **2x faster** and **3x cheaper** with identical quality. Recommend Haiku for retrieval, reserving Sonnet/Opus for complex reasoning.

### Trade-offs

| Aspect | Compaction | RLM Pattern |
|--------|------------|-------------|
| Latency | Instant | ~9-15s per query |
| Info Loss | High | Zero |
| Cost | Lower | Higher per query (but Haiku makes it sustainable) |
| Reliability | Degrades | Consistent |

## Possible Implementation Paths

1. **Pattern Documentation** — Document this as a recommended pattern in docs
2. **Built-in Query Tool** — `memory_query(file, question)` that auto-spawns sub-agent
3. **Hybrid Compaction** — Compaction that delegates to sub-agents instead of summarizing
4. **Native RLM Integration** — Deeper integration with the RLM library

## Related Issues

- #6622, #7175, #6658: Pre-compaction hooks (complementary approach)
- #6757: Self-compact tool (timing control)
- #2597, #5429: Context loss bugs (this addresses root cause)

## References

- [Recursive Language Models Paper](https://arxiv.org/abs/2512.24601)
- [RLM GitHub](https://github.com/alexzhang13/rlm)
- [RLM Blogpost](https://alexzhang13.github.io/blog/2025/rlm/)

---

Would love feedback from maintainers on whether this approach aligns with OpenClaw's direction. Happy to contribute a PR for documentation or implementation.

## Comments


## Links

- None detected yet
