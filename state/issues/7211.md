---
number: 7211
title: "Local model provider (Ollama/custom OpenAI-compatible) not picked up for sub-agent inference"
author: sathwikreddy0615
created: 2026-02-02T15:25:13Z
updated: 2026-02-02T15:25:13Z
labels: []
assignees: []
comments_count: 0
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/7211
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Bug Report

### Summary
Custom local model providers (Ollama or any OpenAI-compatible endpoint) are configured correctly in `clawdbot.json` but never used for actual inference. Sub-agents spawned with `model: "local/qwen2.5:7b"` always fall back to the default Anthropic model despite `modelApplied: true` in the spawn response.

### Environment
- **Clawdbot:** 2026.1.24-3 (885167d)
- **OS:** Ubuntu 24.04, Linux 6.14.0-37-generic (x64)
- **Node:** v22.22.0
- **Ollama:** Running with qwen2.5:7b, ministral-3:3b, llama2
- **Default model:** anthropic/claude-opus-4-5

### Steps to Reproduce

1. Install and run Ollama with tool-capable models:
   ```bash
   ollama pull qwen2.5:7b
   ```

2. Configure local provider in `clawdbot.json` following [OpenClaw docs](https://docs.openclaw.ai/gateway/local-models):
   ```json5
   {
     models: {
       mode: "merge",
       providers: {
         local: {
           baseUrl: "http://127.0.0.1:11434/v1",
           apiKey: "ollama-local",
           api: "openai-responses",
           models: [
             {
               id: "qwen2.5:7b",
               name: "Qwen 2.5 7B",
               reasoning: false,
               input: ["text"],
               cost: { input: 0, output: 0, cacheRead: 0, cacheWrite: 0 },
               contextWindow: 32768,
               maxTokens: 8192
             }
           ]
         }
       }
     }
   }
   ```

3. Restart gateway.

4. Verify Ollama endpoint works:
   ```bash
   # Both endpoints respond correctly
   curl http://127.0.0.1:11434/v1/models           # lists models
   curl http://127.0.0.1:11434/v1/responses -XPOST  # responds to queries
   curl http://127.0.0.1:11434/v1/chat/completions  # responds with tool support
   ```

5. Check model catalog:
   ```bash
   clawdbot models list --json
   # Only shows anthropic/claude-opus-4-5 — local models missing
   ```

6. Spawn a sub-agent with local model:
   ```
   sessions_spawn(model: "local/qwen2.5:7b", task: "Say hello")
   # Returns: { modelApplied: true }
   ```

7. Check transcript — model is `claude-opus-4-5`, not `qwen2.5:7b`.

### Configurations Tried
All of these result in the same behavior (local models not appearing in catalog, falling back to Claude):

- **Explicit `ollama` provider** with `api: "openai-completions"` and manual model list
- **Auto-discovery** (`OLLAMA_API_KEY` set, no explicit `models.providers.ollama`)
- **Custom provider name `local`** with `api: "openai-responses"` (per OpenClaw docs)
- **`models.mode: "merge"`** set in all cases
- **Auth profile** added to `auth-profiles.json` for ollama provider

### Expected Behavior
- Local models should appear in `clawdbot models list`
- Sub-agents spawned with `model: "local/qwen2.5:7b"` should use the local endpoint
- `modelApplied: true` should mean the model is actually used, not just set as metadata

### Actual Behavior
- `clawdbot models list` only shows hosted Anthropic models
- All sub-agent inference falls back to `claude-opus-4-5` regardless of model override
- `modelApplied: true` is misleading — model is set on session but ignored at inference time
- No errors or warnings in logs about failed provider registration

### Workaround
Using `anthropic/claude-haiku-4-5` as a cheaper sub-agent model works correctly.

## Comments


## Links

- None detected yet
