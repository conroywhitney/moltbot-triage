---
number: 3746
title: "[Feature] Support for Llama cpp config"
author: 0xrushi
created: 2026-01-29T03:57:04Z
updated: 2026-01-29T03:57:33Z
labels: []
assignees: []
comments_count: 0
reactions_total: 1
url: https://github.com/moltbot/moltbot/issues/3746
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

This is already working, but I wanted to share the configuration in case anyone finds it helpful.

**Note:** Use `api: "openai-completions"` for standard OpenAI-compatible proxies (llama.cpp, vLLM, LiteLLM). Only use `openai-responses` for specialized APIs like MiniMax that return the non-standard responses format.

```
{
  "meta": {
    "lastTouchedVersion": "2026.1.24-3",
    "lastTouchedAt": "2026-01-29T03:31:15.462Z"
  },
  "wizard": {
    "lastRunAt": "2026-01-29T02:21:08.114Z",
    "lastRunVersion": "2026.1.24-3",
    "lastRunCommand": "onboard",
    "lastRunMode": "local"
  },
  "models": {
    "providers": {
      "llamacpp": {
        "baseUrl": "http://127.0.0.1:8080/v1",
        "apiKey": "1234",
        "api": "openai-responses",
        "models": [
          {
            "id": "gpt-oss-20b-f16",
            "name": "gpt-oss-20b-f16",
            "reasoning": false,
            "input": [
              "text"
            ],
            "cost": {
              "input": 0,
              "output": 0,
              "cacheRead": 0,
              "cacheWrite": 0
            },
            "contextWindow": 128000,
            "maxTokens": 8192
          }
        ]
      }
    }
  },
  "agents": {
    "defaults": {
      "model": {
        "primary": "llamacpp/gpt-oss-20b-f16"
      },
      "workspace": "/home/alpha/clawd",
      "maxConcurrent": 4,
      "subagents": {
        "maxConcurrent": 8
      }
    }
  },
  "messages": {
    "ackReactionScope": "group-mentions"
  },
  "commands": {
    "native": "auto",
    "nativeSkills": "auto"
  },
  "hooks": {
    "internal": {
      "enabled": true,
      "entries": {
        "command-logger": {
          "enabled": true
        }
      }
    }
  },
  "channels": {},
  "gateway": {
    "port": 18789,
    "mode": "local",
    "bind": "loopback",
    "auth": {
      "mode": "token",
      "token": "undefined"
    },
    "tailscale": {
      "mode": "off",
      "resetOnExit": false
    }
  },
  "skills": {
    "install": {
      "nodeManager": "npm"
    }
  },
  "plugins": {
    "entries": {
      "telegram": {
        "enabled": true
      }
    }
  }
}

```

## Comments


## Links

- None detected yet
