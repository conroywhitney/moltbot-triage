---
number: 6926
title: "[Bug]: Gateway/TUI crash: spawn docker ENOENT on macOS with Ollama-only setup and Docker explicitly disabled."
author: gxclark
created: 2026-02-02T06:19:31Z
updated: 2026-02-02T09:03:34Z
labels: ["bug"]
assignees: []
comments_count: 0
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/6926
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

Summary
Gateway/TUI crash: spawn docker ENOENT on macOS with Ollama-only setup and Docker explicitly disabled.

What went wrong?
Using the TUI with a local Ollama provider (no Docker installed) causes the gateway to crash with Error: spawn docker ENOENT even when OPENCLAW_DISABLE_DOCKER=1, OPENCLAW_DISABLE_TAILSCALE=1, and OPENCLAW_DISABLE_SKILLS=1 are set on the gateway process. The TUI shows tokens 0/200k (0%) and never sends a request to the model.

Steps to reproduce
Environment and metadata

As non-admin user openclaw:

bash
openclaw --version        # 2026.1.30 (76b5208)
node --version            # v25.5.0
sw_vers                   # macOS 26.1 (arm64)
jq '.meta' ~/.openclaw/openclaw.json
{
  "lastTouchedVersion": "2026.1.30",
  "lastTouchedAt": "2026-02-02T06:01:56.000Z"
}
Ollama models:

ollama list
qwen3:32b present (along with qwen3-coder:30b, deepseek-coder-v2:16b)
Verify model works directly from openclaw:

curl http://127.0.0.1:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3:32b",
    "messages": [
      {"role": "user", "content": "Say a short hello from Ollama only, no reasoning."}
    ],
    "max_tokens": 128,
    "temperature": 0.2
  }'
Response includes:

"message": {
  "role": "assistant",
  "content": "Hello from Ollama!",
  "reasoning": "..."
}
Start gateway with Docker/Tailscale/skills disabled

As openclaw:

OPENCLAW_DISABLE_DOCKER=1 \
OPENCLAW_DISABLE_TAILSCALE=1 \
OPENCLAW_DISABLE_SKILLS=1 \
openclaw gateway

Gateway log:

06:14:14 [canvas] host mounted at http://127.0.0.1:18789/__openclaw__/canvas/ (root /Users/openclaw/.openclaw/canvas)
06:14:14 [heartbeat] started
06:14:14 [gateway] agent model: ollama/qwen3:32b
06:14:14 [gateway] listening on ws://127.0.0.1:18789 (PID 77345)
06:14:14 [gateway] listening on ws://[::1]:18789
06:14:14 [gateway] log file: /tmp/openclaw/openclaw-2026-02-02.log
06:14:14 [browser/service] Browser control service ready (profiles=2)
06:14:14 Registered hook: boot-md -> gateway:startup
06:14:14 Registered hook: command-logger -> command
06:14:14 Registered hook: session-memory -> command:new
06:14:14 [hooks] loaded 3 internal hook handlers
06:14:18 [ws] webchat connected conn=97cf9742-80ac-4355-8d29-b4983ba96ee4 remote=127.0.0.1 client=openclaw-control-ui webchat vdev
In another shell:

openclaw status --all
Gateway: local · ws://127.0.0.1:18789 (local loopback) · reachable ~20ms · auth token.

Skills: 6 eligible · 0 missing · /Users/openclaw/.openclaw/workspace.

Tailscale: off (diagnostics may show ENOENT).

Trigger via TUI

Still as openclaw:

openclaw tui
agent: main (Main (openclaw@meiji.local))
prompt: "please tell me your model name and verion number"
TUI output:

openclaw tui - ws://127.0.0.1:18789 - agent main (Main (openclaw@meiji.local)) - session main

session agent:main:main

please tell me your model name and verion number

⠙ hobnobbing… -  0s | connected
agent main (Main (openclaw@meiji.local)) | session main (openclaw-tui) | ollama/qwen3:32b | tokens 0/200k (0%)

...

gateway disconnected: closed | idle
agent main (Main (openclaw@meiji.local)) | session main (openclaw-tui) | ollama/qwen3:32b | tokens 0/200k (0%)
At the same time, the gateway process logs:

06:15:18 [openclaw] Uncaught exception: Error: spawn docker ENOENT
    at Process.ChildProcess._handle.onexit (node:internal/child_process:285:19)
    at onErrorNT (node:internal/child_process:483:16)
    at processTicksAndRejections (node:internal/process/task_queues:90:21)
Expected behavior
With Docker explicitly disabled via OPENCLAW_DISABLE_DOCKER=1 and no Docker binary installed, a simple TUI chat prompt should route directly to the configured Ollama model (ollama/qwen3:32b) and return a response, as proven by the direct curl API call.

Optional Docker-based skills/sandboxes should not be invoked in this configuration; they should be skipped or treated as unavailable, and must not cause the gateway to attempt spawn docker or crash.

Actual behavior
The gateway crashes with Error: spawn docker ENOENT when handling a plain text TUI prompt, even though Docker is disabled and not installed.

The TUI session displays ollama/qwen3:32b | tokens 0/200k (0%), indicating no tokens were ever sent to the model.

The crash appears to originate in a path that unconditionally tries to spawn docker (likely browser/sandbox tooling), ignoring the OPENCLAW_DISABLE_* environment flags.

Environment
Clawdbot/OpenClaw version: 2026.1.30 (76b5208) (openclaw --version).

OS: macOS 26.1 (25B78) on Apple silicon (arm64).

Kernel: Darwin meiji.local 25.1.0 … arm64.

Node: v25.5.0.

Install method: npm install -g openclaw@latest as admin user (gxclark), using Node/Homebrew under /opt/homebrew. Gateway is run as a separate non-admin user openclaw.

Provider: Ollama, local on http://127.0.0.1:11434, with qwen3:32b configured as the primary model under models.providers.ollama.

Docker: not installed (Homebrew docker removed; which docker returns nothing).

Gateway run-time env (for the crashing process):

OPENCLAW_DISABLE_DOCKER=1
OPENCLAW_DISABLE_TAILSCALE=1
OPENCLAW_DISABLE_SKILLS=1
NODE_NO_WARNINGS=1        # to silence unrelated punycode deprecation noise
Logs or screenshots

Gateway startup and crash:

06:14:14 [canvas] host mounted at http://127.0.0.1:18789/__openclaw__/canvas/ (root /Users/openclaw/.openclaw/canvas)
06:14:14 [heartbeat] started
06:14:14 [gateway] agent model: ollama/qwen3:32b
06:14:14 [gateway] listening on ws://127.0.0.1:18789 (PID 77345)
06:14:14 [gateway] listening on ws://[::1]:18789
06:14:14 [gateway] log file: /tmp/openclaw/openclaw-2026-02-02.log
06:14:14 [browser/service] Browser control service ready (profiles=2)
06:14:14 Registered hook: boot-md -> gateway:startup
06:14:14 Registered hook: command-logger -> command
06:14:14 Registered hook: session-memory -> command:new
06:14:14 [hooks] loaded 3 internal hook handlers
06:14:18 [ws] webchat connected conn=97cf9742-80ac-4355-8d29-b4983ba96ee4 remote=127.0.0.1 client=openclaw-control-ui webchat vdev
06:15:18 [openclaw] Uncaught exception: Error: spawn docker ENOENT
    at Process.ChildProcess._handle.onexit (node:internal/child_process:285:19)
    at onErrorNT (node:internal/child_process:483:16)
    at processTicksAndRejections (node:internal/process/task_queues:90:21)
TUI session (abridged):

openclaw tui - ws://127.0.0.1:18789 - agent main (Main (openclaw@meiji.local)) - session main

session agent:main:main

please tell me your model name and verion number

⠙ hobnobbing… • 0s | connected
agent main (Main (openclaw@meiji.local)) | session main (openclaw-tui) | ollama/qwen3:32b | tokens 0/200k (0%)

...

gateway disconnected: closed | idle
agent main (Main (openclaw@meiji.local)) | session main (openclaw-tui) | ollama/qwen3:32b | tokens 0/200k (0%)

Ollama verification (for reference):

{
  "id": "chatcmpl-811",
  "object": "chat.completion",
  "created": 1770012672,
  "model": "qwen3:32b",
  "system_fingerprint": "fp_ollama",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Hello from Ollama!",
        "reasoning": "Okay, the user wants a short hello from Ollama without any reasoning. ..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 23,
    "completion_tokens": 65,
    "total_tokens": 88
  }
}

## Comments


## Links

- None detected yet
