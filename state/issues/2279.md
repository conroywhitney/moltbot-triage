---
number: 2279
title: "Ollama models return '(no output)' — enforceFinalTag incorrectly applied to all Ollama models"
author: bkpaine1
created: 2026-01-26T16:37:01Z
updated: 2026-01-29T07:21:25Z
labels: ["bug"]
assignees: []
comments_count: 9
reactions_total: 1
url: https://github.com/moltbot/moltbot/issues/2279
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Bug Description

All Ollama models (glm-4.7-flash, gpt-oss, deepseek-r1:70b, etc.) return `(no output)` when used through ClawdBot TUI or WhatsApp. Anthropic models work fine in the same session.

## Root Cause

`isReasoningTagProvider()` in `dist/utils/provider-utils.js` returns `true` for **all** Ollama models (line 14: `normalized === "ollama"`). This causes two problems:

### 1. `enforceFinalTag` is set to `true` for all Ollama runs

In `auto-reply/reply/get-reply-run.js:253`:
```js
...(isReasoningTagProvider(provider) ? { enforceFinalTag: true } : {}),
```

This means `stripBlockTags()` in `pi-embedded-subscribe.js` enforces strict `<final>` tag extraction (lines 296-298):
```js
if (!everInFinal) {
    return "";
}
```

Since Ollama models don't output `<final>` tags, **all text content is discarded**.

### 2. System prompt injects `<think>/<final>` format instructions

In `agents/system-prompt.js:202-207`, when `reasoningTagHint` is true:
```
"ALL internal reasoning MUST be inside <think>...</think>."
"Format every reply as <think>...</think> then <final>...</final>, with no other text."
```

Most Ollama models don't reliably follow this instruction, resulting in malformed output that gets stripped.

## Why this is wrong for Ollama

Ollama's OpenAI-compatible endpoint (`/v1/chat/completions`) already handles reasoning natively via the `reasoning` field in streaming chunks:

```json
{"choices":[{"delta":{"content":"","reasoning":"The user said hello"}}]}
...
{"choices":[{"delta":{"content":"Hello! How can I help?"}}]}
```

The `pi-ai` library correctly maps `reasoning` → `thinking` blocks and `content` → `text` blocks. There is **no need** for `<think>/<final>` tag enforcement because reasoning separation happens at the API level.

## Reproduction

```bash
# Ollama returns valid content through the OpenAI SDK
node -e "
import OpenAI from 'openai';
const c = new OpenAI({baseURL:'http://localhost:11434/v1', apiKey:'ollama'});
const s = await c.chat.completions.create({model:'glm-4.7-flash:latest', messages:[{role:'user',content:'say hello'}], stream:true});
let t='';
for await (const ch of s) { if(ch.choices[0]?.delta?.content) t+=ch.choices[0].delta.content; }
console.log(t); // 'Hello! How can I help you today?'
"
# But ClawdBot shows (no output) because enforceFinalTag strips everything
```

## Suggested Fix

Remove `"ollama"` from `isReasoningTagProvider()` in `dist/utils/provider-utils.js`:

```diff
-    if (normalized === "ollama" ||
-        normalized === "google-gemini-cli" ||
+    if (normalized === "google-gemini-cli" ||
         normalized === "google-generative-ai") {
```

Ollama's native API-level reasoning separation (via the `reasoning` field) makes tag-based enforcement unnecessary and actively harmful.

## Environment

- ClawdBot: v2026.1.24-3
- Ollama: running on localhost:11434
- Models tested: glm-4.7-flash, gpt-oss, deepseek-r1:70b
- Platform: Ubuntu, AMD Strix Halo APU
- All models produce valid responses via direct API calls but show `(no output)` in ClawdBot

## Workaround

Manually edit `~/.npm-global/lib/node_modules/clawdbot/dist/utils/provider-utils.js` and remove `"ollama"` from the `isReasoningTagProvider()` function, then restart the gateway service.

## Comments

### @morissonmaciel (2026-01-27)

I can confirm that this workaround solution worked, including the cloud models.
For those who installed `npm` with `Homebrew`, the correct file path is the following:

```bash
/opt/homebrew/lib/node_modules/clawdbot/dist/utils/provider-utils.js
```

Currently I am using these models with no issues at all:

```json
"models": {
  "providers": {
    "ollama": {
      "baseUrl": "http://127.0.0.1:11434/v1",
      "apiKey": "ollama-local",
      "api": "openai-completions",
      "models": [
        {
          "id": "gpt-oss:20b-cloud",
          "name": "GPT-20B Cloud (Ollama)",
          "reasoning": true,
          "input": ["text", "image"],
          "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
          "contextWindow": 128000,
          "maxTokens": 128000
        },
        {
          "id": "qwen3-coder:480b-cloud",
          "name": "Qwen3-Coder Cloud (Ollama)",
          "reasoning": true,
          "input": ["text", "image"],
          "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
          "contextWindow": 260000,
          "maxTokens": 260000
        }
      ]
    }
  }
},
```

I hope someone finds a way to enable the `image` input  as well for Ollama models.

### @paulmorabito (2026-01-27)

I raised the bug marked as duplicate. I made the code change above, restarted and it didn’t work for me. Am using a different model (qwen2.5:32b-instruct-q4_K_M) to the ones tested above.

### @scottaltham-payroc (2026-01-27)

I have the same issue. Updated the provider-utils.js, restarted the gateway and its made no difference. I ask the ollama model a question (deepseek r1 7b in my case), the bot looks like its typing, then nothing happens.

### @javiersgjavi (2026-01-28)

Is there any solution for this? I think this might be an issue when the TUI tries to load the ollama message. I have analysed the GPU performance and it is working when I send a message. Also, when I try to exit the TUI and enter again, sometimes I can see the previous message.

### @bkpaine1 (2026-01-28)

> Is there any solution for this? I think this might be an issue when the TUI tries to load the ollama message. I have analysed the GPU performance and it is working when I send a message. Also, when I try to exit the TUI and enter again, sometimes I can see the previous message.

Me too.  I haven't found the full cause I am trying.


### @egenc (2026-01-28)

not even doing a simple terminal commands either. Looks like open source models are not optimized for this. Would be great if finetuning possible for agentic operations

### @bbrietzke (2026-01-28)

Can confirm with ollama/glm-4.7-flash:latest that the above fix does not work.

### @iamsuperark (2026-01-29)

my path is /usr/local/lib/node_modules/clawdbot/dist/utils/provider-utils.js, remove the ollama section, with glm-4.7-flash model, still don't work. no response in webchat, in telegram, respone is "Cannot read properties of undefined (reading 'includes')", which could be a hint to anyone?

### @iamsuperark (2026-01-29)

> my path is /usr/local/lib/node_modules/clawdbot/dist/utils/provider-utils.js, remove the ollama section, with glm-4.7-flash model, still don't work. no response in webchat, in telegram, respone is "Cannot read properties of undefined (reading 'includes')", which could be a hint to anyone?

Problem resolved after i check the clawdbot.json model setion. the root cause maybe during installation i configured the ollama with model "deepseek-r1:32b", and after installation i changed the model to glm-4.7-flesh, may may lead to the mismatch in the "model" and "agent", originally

"models": {
    "providers": {
      "ollama": {
        "baseUrl": "http://127.0.0.1:11434/v1",
        "apiKey": "ollama-local",
        "api": "openai-completions",
        "models": [
          {
            "id": "deepseek-r1:32b",
            "name": "deepseek-r1:32b",_            <<- this the model i used during installaltion, which maybe auto-selected by clawdbot
            "reasoning": false,
            "input": [
              "text"
            ],
            "cost": {
              "input": 0,
              "output": 0,
              "cacheRead": 0,
              "cacheWrite": 0
            },
            "contextWindow": 131072,
            "maxTokens": 16384
          }
        ]
      }
    }
  },
  "agents": {
    "defaults": {
      "model": {
        "primary": "ollama/glm-4.7-flash"           <<-- this is the model i changed after installation, don't know why clawdbot choose to defined it here.
      },
      "models": {
        "ollama/glm-4.7-flash": {}
      },
      "workspace": "/Users/yujie/clawd",
      "compaction": {
        "mode": "safeguard"
      },
      "maxConcurrent": 4,
      "subagents": {



after i changed the model name in the "models" like the following, it works.

"models": {
    "providers": {
      "ollama": {
        "baseUrl": "http://127.0.0.1:11434/v1",
        "apiKey": "ollama-local",
        "api": "openai-completions",
        "models": [
          {
            "id": "glm-4.7-flash",
            "name": "glm-4.7-flash",
            "reasoning": false,
            "input": [
              "text"
            ],
            "cost": {
              "input": 0,
              "output": 0,
              "cacheRead": 0,
              "cacheWrite": 0
            },
            "contextWindow": 131072,
            "maxTokens": 16384

thanks guys, but i dind't change the /usr/local/lib/node_modules/clawdbot/dist/utils/provider-utils.js to original state for further test, not sure it's also part of the root cause. 



## Links

- None detected yet
