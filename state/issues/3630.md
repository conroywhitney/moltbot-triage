---
number: 3630
title: "[Bug]: Embedded agent never generates output (0 tokens, no error) - Clawdbot 2026.1.24-3 + Ollama (OpenAI-compatible)"
author: garaujo-edumobi
created: 2026-01-28T23:24:45Z
updated: 2026-01-28T23:24:45Z
labels: ["bug"]
assignees: []
comments_count: 0
reactions_total: 0
url: https://github.com/moltbot/moltbot/issues/3630
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Environment

- Clawdbot version: `2026.1.24-3`
- OS: Ubuntu (linux 6.14.0-37-generic x64)
- Node: `22.22.0`
- Gateway: local, `ws://127.0.0.1:18789`
- Ollama host: `192.168.15.20:11434`
- Ollama models installed:
    - `qwen2.5-coder:7b` (4.7 GB)
    - `minimax-m2:cloud`
    - `qwen3:8b`
- Channel in use: Telegram (OK)
- TUI: `clawdbot tui` connected and stable


***

## Summary of the problem

Even though:

- Ollama is running and responding correctly via the OpenAI-compatible `/v1/chat/completions` endpoint, and
- Clawdbot shows the agent using `ollama/qwen2.5-coder:7b`,

the embedded agent **never returns any generative output** in the TUI or Telegram.

Each run shows as started and completed (`aborted=false`), but:

- 0 tokens are consumed, and
- there is **no error** in the Clawdbot logs for the final run,
- Ollama logs show **no new requests** when the run is triggered from Clawdbot.

From the user perspective: TUI shows the message (“teste”), then `(no output)`, and status `connected | idle | tokens 0/200k`.

***

## Configuration details (sanitized)

### Provider configuration (Ollama)

```json
{
  "models": {
    "providers": {
      "ollama": {
        "baseUrl": "http://192.168.15.20:11434/v1",
        "apiKey": "<redacted>",
        "api": "openai-completions",
        "models": [
          {
            "id": "ollama/qwen2.5-coder:7b",
            "name": "qwen2.5-coder:7b",
            "reasoning": false,
            "input": ["text"],
            "cost": {
              "input": 0,
              "output": 0,
              "cacheRead": 0,
              "cacheWrite": 0
            },
            "contextWindow": 32000,
            "maxTokens": 320000
          }
        ]
      }
    }
  }
}
```

Notes:

- Using OpenAI-compatible API: `"api": "openai-completions"`.
- Environment variables for Ollama (URL, key) match this config.
- `clawdbot doctor --fix` has been run; configuration now passes validation.


### Agent configuration (simplified)

The main agent is `main`, using the Ollama model:

- `clawdbot status` shows:
    - `Agents: 1 total`
    - `main` was `PENDING` earlier, then bootstrapped.
    - TUI header shows:
`agent main | session main (clawdbot-tui) | ollama/qwen2.5-coder:7b | tokens 0/200k`.
- Model for the agent was set via a JSON block like:

```bash
clawdbot config set agents '{
  "defaults": {
    "model": { "primary": "ollama/qwen2.5-coder:7b" },
    "maxConcurrent": 4,
    "subagents": { "maxConcurrent": 8 }
  }
}' --json
```


Sandbox / Docker:

- Previously, runs failed with timeouts trying to pull a Docker image for sandboxing.
- Sandbox-related config keys that did not match the schema were removed and fixed.
- The behavior described below is after sandbox errors were resolved.

***

## Ollama endpoint is working (verified separately)

From the Clawdbot host:

```bash
curl http://192.168.15.20:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5-coder:7b",
    "messages": [
      { "role": "user", "content": "Diga \"teste\" em português" }
    ]
  }'
```

Example response:

```json
{
  "id": "chatcmpl-516",
  "object": "chat.completion",
  "created": 1769639821,
  "model": "qwen2.5-coder:7b",
  "system_fingerprint": "fp_ollama",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Obrigado! Aqui está sua palavra em português:\n\"Teste\""
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 38,
    "completion_tokens": 19,
    "total_tokens": 57
  }
}
```

So the Ollama OpenAI-compatible API is functional, and the model is correctly installed and generating.



***

## Clawdbot logs during a failing run

From `clawdbot tui`, user sends:

```text
teste
```

TUI:

```text
(no output)
connected | idle
agent main | session main (clawdbot-tui) | ollama/qwen2.5-coder:7b | tokens 0/200k (0%)
```

Clawdbot log snippet for that run:

```text
23:13:31 debug diagnostic {"subsystem":"diagnostic"} lane enqueue: lane=session:agent:main:main queueSize=1
23:13:31 debug diagnostic {"subsystem":"diagnostic"} lane dequeue: lane=session:agent:main:main waitMs=4 queueSize=0
23:13:31 debug diagnostic {"subsystem":"diagnostic"} lane enqueue: lane=main queueSize=1
23:13:31 debug diagnostic {"subsystem":"diagnostic"} lane dequeue: lane=main waitMs=0 queueSize=0
23:13:31 debug agent/embedded {"subsystem":"agent/embedded"} embedded run start: runId=8c0df31b-0abe-4058-8067-bb9e47b2bb59 sessionId=cc194fed-1a81-42b4-ace9-71252aa2f9fb provider=ollama model=qwen2.5-coder:7b thinking=off messageChannel=webchat
23:13:31 debug diagnostic {"subsystem":"diagnostic"} session state: sessionId=cc194fed-1a81-42b4-ace9-71252aa2f9fb sessionKey=unknown prev=idle new=processing reason="run_started" queueDepth=0
23:13:31 debug diagnostic {"subsystem":"diagnostic"} run registered: sessionId=cc194fed-1a81-42b4-ace9-71252aa2f9fb totalActive=1
23:13:31 debug agent/embedded {"subsystem":"agent/embedded"} embedded run prompt start: runId=8c0df31b-0abe-4058-8067-bb9e47b2bb59 sessionId=cc194fed-1a81-42b4-ace9-71252aa2f9fb
23:13:31 debug agent/embedded {"subsystem":"agent/embedded"} embedded run agent start: runId=8c0df31b-0abe-4058-8067-bb9e47b2bb59
23:13:31 debug agent/embedded {"subsystem":"agent/embedded"} embedded run agent end: runId=8c0df31b-0abe-4058-8067-bb9e47b2bb59
23:13:31 debug agent/embedded {"subsystem":"agent/embedded"} embedded run prompt end: runId=8c0df31b-0abe-4058-8067-bb9e47b2bb59 sessionId=cc194fed-1a81-42b4-ace9-71252aa2f9fb durationMs=302
23:13:31 debug diagnostic {"subsystem":"diagnostic"} session state: sessionId=cc194fed-1a81-42b4-ace9-71252aa2f9fb sessionKey=unknown prev=processing new=idle reason="run_completed" queueDepth=0
23:13:31 debug diagnostic {"subsystem":"diagnostic"} run cleared: sessionId=cc194fed-1a81-42b4-ace9-71252aa2f9fb totalActive=0
23:13:31 debug agent/embedded {"subsystem":"agent/embedded"} embedded run done: runId=8c0df31b-0abe-4058-8067-bb9e47b2bb59 sessionId=cc194fed-1a81-42b4-ace9-71252aa2f9fb durationMs=364 aborted=false
23:13:31 debug diagnostic {"subsystem":"diagnostic"} lane task done: lane=main durationMs=365 active=0 queued=0
23:13:31 debug diagnostic {"subsystem":"diagnostic"} lane task done: lane=session:agent:main:main durationMs=367 active=0 queued=0
```

Key points:

- Run is marked as complete (`aborted=false`).
- No error is logged.
- No indication that a request was actually sent to the Ollama endpoint.
- Tokens remain at 0.

***

## Ollama logs during the same period

From `journalctl -f -u ollama`:

- The last logged call to `/v1/chat/completions` is from the earlier manual `curl`.
- When triggering runs via `clawdbot tui`, **no new log entries** appear for POST `/v1/chat/completions`.

Example (earlier, working call from curl):

```text
[GIN] 2026/01/28 - 19:37:01 | 200 |  5.808198153s | 192.168.122.200 | POST     "/v1/chat/completions"
```

The “teste” run around `23:13:31` has no corresponding log line.

***

## Previously seen errors (now resolved)

Earlier, before cleaning up config and sandbox:

- Runs failed with timeouts trying to pull Docker images:

```text
Error: failed to copy: httpReadSeeker: failed open: failed to do request: Get "https://docker-images-prod...": dial tcp ...: i/o timeout
```

- Setting sandbox-related keys and unsupported model keys caused validation errors like:

```text
Config validation failed: agents.defaults.sandbox.mode: Invalid input
Config validation failed: models.providers.ollama.models.0: Unrecognized keys: "local", "auth", "tags"
```


These have been fixed (invalid keys removed, doctor run). The current state is “clean” from the config-validator perspective.

***

## Current behavior (concise)

- `clawdbot tui` connects and shows the agent model `ollama/qwen2.5-coder:7b`.
- Sending a message results in:
    - No output in TUI.
    - Tokens remain 0.
- Logs show a full embedded run lifecycle with `provider=ollama` and `aborted=false`, but no error.
- Ollama receives no requests for these runs.
- Ollama works fine when called directly via `curl`.

***

## Expected behavior

Given:

- A configured `ollama` provider using `"api": "openai-completions"`.
- A reachable `/v1/chat/completions` endpoint that works via curl.
- An agent whose `primary` model is `ollama/qwen2.5-coder:7b`.

I would expect:

- Clawdbot to send user messages to the Ollama endpoint.
- Ollama to generate tokens.
- The TUI and Telegram channel to display the assistant’s response.

***

## Actual behavior

- Clawdbot internally completes the run (no error, `aborted=false`).
- No HTTP request appears to be sent to Ollama for that run.
- No tokens are generated.
- No response is shown to the user.

***

## Questions / request

- Is this a known issue with version `2026.1.24-3` when using a custom `ollama` provider with `api: "openai-completions"`?
- Is there additional configuration required so that the embedded agent actually forwards prompts to the provider (e.g., different `api` type, model id format, or some flag to enable generative runs)?
- Should a model that still shows as `default,missing` in `clawdbot models list` be considered “non-invocable” even when the provider is configured and reachable, and if so, how should it be marked as fully available?

## Comments


## Links

- None detected yet
