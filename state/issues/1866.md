---
number: 1866
title: "[Bug]: Add tool calling support for `openai-completions` API mode"
author: sekharmalla
created: 2026-01-25T17:02:47Z
updated: 2026-02-02T17:33:21Z
labels: ["bug"]
assignees: []
comments_count: 12
reactions_total: 3
url: https://github.com/openclaw/openclaw/issues/1866
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

### Problem
When using a local vLLM server with `api: "openai-completions"`, Clawdbot doesn't send tool definitions to the model, so tool calling never works. The model just responds with text instead of making tool calls.

### Environment
- Clawdbot: 2026.1.23-1
- vLLM: latest (vllm/vllm-openai:latest)
- Model: Qwen/Qwen2.5-7B-Instruct-AWQ with `--enable-auto-tool-choice --tool-call-parser hermes`

### Evidence
vLLM's `/v1/chat/completions` endpoint **does support tools** when called directly:
```bash
curl -s http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model":"Qwen/Qwen2.5-7B-Instruct-AWQ",
    "messages":[{"role":"user","content":"Call ping with ok=true"}],
    "tools":[{"type":"function","function":{"name":"ping","parameters":{"type":"object","properties":{"ok":{"type":"boolean"}}}}}],
    "tool_choice":"auto"
  }' | jq '.choices[0].message.tool_calls'

# Returns:
[{"id":"chatcmpl-tool-xxx","type":"function","function":{"name":"ping","arguments":"{\"ok\": true}"}}]
```

But Clawdbot with `api: "openai-completions"` never sends tools, so the model just replies with text.

### Request
Add an option to send tools with `openai-completions` mode, or add a new API mode like `openai-chat-with-tools` that:
1. Uses `/v1/chat/completions` endpoint
2. Includes tool definitions in the request
3. Parses `tool_calls` from the response

This would enable local vLLM + Clawdbot browser automation.

## Comments

### @yanghu (2026-01-27)

+1, same situation with local models served in TabbyAPI. tool calls returned as text and clawdbot not calling the tools. 

### @bkutasi (2026-01-27)

Seems like we need better local support. Did anyone get it working with llama.cpp?

### @z-x-x136 (2026-01-28)

+1

### @derhamderham (2026-01-28)

+1

### @ZyuzinDmitry (2026-01-28)

+1

### @JD-kriswu (2026-01-29)

+1

### @sdultsin (2026-01-29)

+1

### @w84death (2026-01-31)

+1

Same with ollama/glm-4.7-flash. Sends tool call as text message to discord instead of actually doing that call in the system.

### @malicorX (2026-02-01)

+1

having the problem with a bunch of LLMs

### @wangrobin0956 (2026-02-02)

+1

### @jokelord (2026-02-02)

I have successfully applied the tool calling patch via LLM.ðŸ‘‡
https://github.com/jokelord/openclaw-local-model-tool-calling-patch

### @Yiklek (2026-02-02)

+1
vllm Qwen3-14B-FP8


## Links

- None detected yet
