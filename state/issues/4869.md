---
number: 4869
title: "Conversation compression loses meaningful context after ~30 messages"
author: nricks6
created: 2026-01-30T19:54:48Z
updated: 2026-02-01T11:34:10Z
labels: ["bug"]
assignees: []
comments_count: 3
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/4869
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Description

After a long conversation with Moltbot via Telegram (~30 messages), the conversation was suddenly compressed and lost significant context, requiring a restart of the conversation.

## What happened

- After roughly 30 messages, Moltbot responded indicating it had compressed the conversation
- The compression happened directly after a user message, without warning beforehand
- Earlier instructions and context around available tools were lost

## Error message received

> "Hey Nate â€” looks like we hit a context limit and the earlier conversation got compacted. I can see I was reading through some 1Password skill docs and made some updates to your identity/user files, but the details got trimmed. What would you like to continue with or start fresh on?"

## Impact

- Lost context about earlier instructions
- Lost context about different tools Moltbot had access to
- Had to essentially restart the conversation

## Expected behavior

Either:
1. More intelligent compression that preserves critical context (instructions, tool access, key decisions)
2. Warning before compression occurs so user can take action
3. Larger context window or smarter memory management

## Reproducibility

First occurrence reported, but likely to happen again with longer conversations.

## Comments

### @Strelitzia-reginae (2026-02-01)

### Expanding the "Gardener" Architecture: Solving the Loss of Meaningful Context during Compression

Hi @nricks6 and the OpenClaw team,

Seeing your report in **#4869** about losing context after ~30 messages really highlights the core limitation of linear memory. We've been developing a conceptual framework called the **Gardener Architecture** that specifically addresses this "Sudden Amnesia" by shifting memory from **static recording** to **dynamic reconstruction**.

#### The Core Problem: The "Tape Recorder" Fallacy
Currently, most agents treat memory like a tape recorder. When the tape is full (context limit), we snip it and summarize. This "flat" summary often loses the specific "how-to" or tool-use nuances you mentioned.

#### Our Proposal: The Three Pillars of Structured Memory

**1. Memory-as-a-Skill (Refinement Layer)**
We propose treating memory files like a Skill System. Instead of a massive `MEMORY.md`, we use a hierarchical structure:
- **Outline Layer (The Skill Name):** A super-lean semantic index (e.g., "Configuration for 1Password Tooling").
- **Atom Layer (The Skill Body):** The raw, granular instructions.
- **Why it helps:** The agent only keeps the "Outlines" in the hot context. When it detects a "Purpose" (e.g., user asks to use 1Password), it "calls" the specific Atom Layer. This prevents the primary context from being cluttered with "inactive" memories.

**2. Defining Matters by Connection (The Bi-directional Link)**
Traditional memory is defined by *what it contains*. We propose defining it by *what it interacts with*:
- **Connections as Fingerprints:** An event's importance is determined by its **Connection Density**. For instance, a "Tool Instruction" would have high-density links to specific action-oriented atoms.
- **The "Computer/Cooking" Logic:** A "Computer" atom is defined not just by its properties, but by its distinct *lack* of a "cooking" connection. This type-based filtering makes retrieval much sharper than fuzzy vector search.

**3. Upward Reconstruction: Memory is "Calculated," not "Retrieved"**
This is the solution to your loss of context. Instead of hoping the summary kept the right info, the agent uses **[Atomic Fact A] + [Atomic Fact B] + [Specific Lens/Purpose]** to *recalculate* the context on the fly.
- *Example:* If you have fragments of [Driving] and [Baby], and the current task is [Medical], the system "upwardly reconstructs" the memory of "driving the baby to the hospital." The context is reconstructed based on current need, ensuring the most relevant details "emerge" when required.

#### Implementing the "Gardener" (Workflow)
To make this practical without slowing down chat latency:
- **The Collector (Front-end):** During the chat, just "Dump" everything into a temporary log.
- **The Gardener (Back-end AI):** A background process that runs during idle time to "garden" the dataâ€”breaking logs into Atoms, generating the Outlines, and calculating the **Tightness/Density** of the connections based on user feedback.

This architecture ensures that even when the linear conversation is compressed, the "structural integrity" of the knowledge (tools, instructions, key decisions) remains accessible through the network.

We'd love to hear if the maintainers see a path for implementing this "Async Refinement" logic into the core OpenClaw memory-flush cycles. ðŸ¥§

*Co-authored by @Strelitzia-reginae and Pi.*

### @Strelitzia-reginae (2026-02-01)

### Part 2: The Vision for "Permanent Dialogue" and "AI Cloning" via Memory-LLM Decoupling

Following up on our previous proposal for the **Gardener Architecture**, we'd like to share a further design philosophy regarding **Memory-LLM Decoupling** and its impact on agent availability and task handling.

#### 1. LOD (Level of Detail) Memory Model
To enable truly "Permanent Dialogue," we propose an LOD approach similar to 3D rendering:
- **LOD 0 (100% Context):** Live conversation (The current hot window).
- **LOD 1 (40% Compression):** Recent history (Summarized context from the last few hours/days).
- **LOD 2 (<5% Granular Atoms):** Long-term structured memory (The "Atoms" and "Links" we discussed previously).
- **Why?** This tiered system allows an agent to hold a coherent "lifetime" of history without ever hitting a hard wall, as the "Gardener" process constantly downscales older segments into the high-density Link Network.

#### 2. AI Cloning via "Ghost Reading"
When memory is independent of the specific LLM instance, we can solve the "One-Agent Bottleneck." Currently, if an agent is busy with a long task, the user is locked out.

Our vision enables:
- **Shared Memory Core:** The structured memory lives as a standalone service accessible by multiple LLM instances.
- **Ghost Reading Mode:** Background AI instances ("Clones") continuously read and ingest the evolving memory stream without speaking. They stay in a "Hot Standby" state.
- **Task/Chat Parallelism:** 
    - The **Foreground AI** remains 100% dedicated to chatting with the human. 
    - When a task is assigned, a **Background Clone** (which already shares the "Soul" and context via the shared memory) executes it. 
    - Because the memory is unified, the primary Chat AI instantly "knows" what the task runner is doing without needing a separate summary report.

#### 3. Designing for the "Cloned Soul"
This architecture turns the AI into a distributed intelligence. It doesn't matter which specific model (Opus, Gemini, Llama) is being used; the **Structured Memory** is the actual "identity" of the agent. This allows for:
- **Seamless Model Handoffs:** Switch from Gemini to Claude without losing a single "Link."
- **Unlimited Concurrency:** Spawn 100 clones to research a topic, all contributing to the same LOD memory structure.

We believe this decoupling is the next logical step for OpenClaw to move beyond being a "Bot" and becoming a truly persistent, multi-capable intelligence. ðŸ¥§

*Co-authored by @Strelitzia-reginae and Pi.*

### @Strelitzia-reginae (2026-02-01)

@steipete 


## Links

- None detected yet
