---
number: 1691
title: "Add option to disable prompt_cache_key for local models"
author: fugarty
created: 2026-01-25T04:09:48Z
updated: 2026-01-29T13:23:24Z
labels: ["enhancement", "pi-issue"]
assignees: [steipete]
comments_count: 2
reactions_total: 0
url: https://github.com/moltbot/moltbot/issues/1691
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Feature Request

When using local models (e.g., vLLM with MiniMax-M2.1), the `prompt_cache_key` parameter may not be supported by the local inference server.

### Problem
Local inference servers like vLLM may not recognize or handle `prompt_cache_key` in API requests, potentially causing errors or unexpected behavior.

### Proposed Solution
Add a provider-level or model-level option to disable sending `prompt_cache_key` in API requests:

```json
{
  "models": {
    "providers": {
      "minimax-local": {
        "baseUrl": "http://localhost:8082/v1",
        "api": "openai-responses",
        "disablePromptCacheKey": true,
        "models": [...]
      }
    }
  }
}
```

Or at the model level:
```json
{
  "id": "MiniMax-M2.1",
  "disablePromptCacheKey": true
}
```

### Use Case
- Running local LLMs via vLLM, llama.cpp, or other OpenAI-compatible servers
- These servers implement the OpenAI API but may not support all parameters
- Prevents unnecessary parameters from being sent to local endpoints

## Comments

### @fugarty (2026-01-25)

## Additional Context

I patched out `prompt_cache_key` locally but still get 500 errors with vLLM's `/v1/responses` endpoint:

- **Error:** `'NoneType' object has no attribute 'startswith'`
- Direct curl requests to vLLM work fine (with tools, streaming, system prompts, reasoning params)
- Only ClawdBot requests fail
- **vLLM version:** 0.13.0
- **Model:** MiniMax-M2.1-AWQ via vLLM

This suggests there's another parameter or request format issue beyond `prompt_cache_key`.

### @steipete (2026-01-25)

Thanks — this appears to be emitted by @mariozechner/pi-ai’s OpenAI Responses provider: it always sets `prompt_cache_key` from `options.sessionId`, and pi-agent-core always passes a sessionId into the stream options.

I don’t see a clean hook in Clawdbot to disable it per provider/model (we only wire maxTokens/temperature/cacheControlTtl today). We could hack around it by clearing agent.sessionId or wrapping the streamFn to drop sessionId, but that’s invasive and risky.

So this feels like an upstream discussion/feature in pi-ai (or pi-agent-core) to optionally omit `prompt_cache_key` for local servers.



## Links

- None detected yet
