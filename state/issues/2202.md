---
number: 2202
title: "[Bug]: Gateway sends empty messages on persistent API errors (429/402) instead of user-facing explanation"
author: 26tajeen
created: 2026-01-26T12:04:36Z
updated: 2026-01-29T08:41:42Z
labels: ["bug"]
assignees: []
comments_count: 7
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/2202
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Problem

When the upstream API returns persistent errors (e.g., 429 rate limit, 402 payment required, credit exhaustion), the gateway retries correctly but then sends **empty messages** to the user rather than explaining what happened.

From the user's perspective, the assistant simply stops responding ‚Äî or worse, sends blank/ghost messages. There's no indication that credits are exhausted or rate limits have been hit.

## Observed Behavior

Session history shows:
- User sends message
- API returns `429 rate_limit_error: "This request would exceed your account's rate limit"`
- Gateway retries 4 times (all fail with same error)
- Gateway sends response with `content: []`, `stopReason: "error"`
- User receives empty/blank message
- User has no idea what's happening

## Expected Behavior

When retries are exhausted on a 429/402/similar error, the gateway should send a helpful message to the user:

> ‚ö†Ô∏è API rate limit reached. Your account may need more credits. Check console.anthropic.com or try again later.

This should be configurable (e.g., `errorMessages.rateLimitExhausted`) but have sensible defaults.

## Impact

- Users think the assistant has crashed or is ignoring them
- No actionable information provided
- Poor UX during a recoverable situation

## Suggested Fix

In the response handling path, detect when:
1. `stopReason === 'error'`
2. Error is a rate limit (429) or payment issue (402)
3. Retries exhausted

Then inject a user-facing error message rather than sending empty content.

## Comments

### @0xJonHoldsCrypto (2026-01-26)

Had this same issue a few times yesterday on larger payloads - had to quit entirely in order to continue as it tied things up for a bit. This also contributes to tons of usage if the token requirements for the query are large they are effectively being replayed 4x after a 429. 

### @0xJonHoldsCrypto (2026-01-26)

Also, asking the agent to add fallback has a high likelihood of replacing your auth incorrectly - if you do make sure to instruct using config.patch for surgical updates.

### @scottsanpedro (2026-01-26)

I blew through a ton of tokens. This 

From what we saw, the loop was:

Initial 429 error - hit the TPM rate limit
Clawdbot's retry logic kicked in - but instead of exponential backoff, it immediately retried
Each retry failed with 429 (still rate limited)
Each failed retry counted against quota - burning tokens just to get the error response
Repeat hundreds of times in seconds

Looking at the session log, we saw messages like:
"stopReason":"error"
"errorMessage":"{\"error\":{\"code\":429,\"message\":\"Resource has been exhausted...
Then immediately another attempt, another 429, over and over.
The bug is in Clawdbot - it should:

Detect 429 errors
Back off exponentially (wait 1s, 2s, 4s, 8s, etc.)
Have a max retry limit
NOT keep hammering a rate-limited API

Instead it just kept firing requests as fast as possible, each one eating tokens. Classic retry storm.

### @leslieleefook (2026-01-27)

Adding a specific request: when surfacing 429 errors to the agent/user, please include the **\Retry-After\ header** (or \x-ratelimit-reset\) from the upstream API response.

**Use case:** Agents need to know *when* to retry, not just that they were rate limited. This enables:
- Exponential backoff with accurate timing
- Communicating reset time to users
- Smart work scheduling (pause background tasks, resume at reset)

**Example desired output:**
\\\
‚ö†Ô∏è RATE LIMITED
Retry-After: 847 seconds (14m 7s)
Reset: 2026-01-27 04:35 AST
\\\

Currently the agent only sees the error message but not the headers, making it impossible to implement proper rate limit resilience strategies.

---
Submitted via Clawdbot ü¶û

### @26tajeen (2026-01-27)

# Bug Report: Silent LLM Failures - No Error Feedback to User

## Summary
When LLM providers fail (quota exceeded, rate limits, etc.), clawdbot's failover system silently swallows errors without notifying the user. This creates a poor user experience where messages appear to be ignored with no indication of what went wrong.

## Problem Description

### Current Behavior
1. User sends a message via webchat
2. All configured LLM providers fail (e.g., OpenAI quota exceeded, Kimi rate limited)  
3. Agent run completes with `run_completed / aborted=false`
4. **No error message is sent back to the user**
5. User sees complete silence - no indication anything went wrong
6. `chat.send` returns successfully (status ‚úì) because message queueing succeeded
7. Errors only appear in `gateway.err.log` and detailed log files, invisible to end user

### Expected Behavior
When all LLM providers fail, the system should:
1. Send an error message back to the user via websocket
2. Clearly indicate what went wrong (e.g., "Failed to get response: insufficient quota")
3. Provide actionable information when possible

## Technical Details

### Error Flow
- FailoverError goes to logs only
- No error event pushed back to webchat client over websocket
- User interface has no indication of failure
- Creates illusion that system is functioning while silently failing

### Example Log Traces
```
[timestamp] agent run provider=openai model=gpt-5.2 completed in 800ms with no response
[timestamp] FailoverError: [detailed error] ‚Üí gateway.err.log only
```

### Discovery Process
- OpenAI quota errors were completely invisible in logs due to built-in provider failover
- Only direct `curl` testing of OpenAI API revealed `insufficient_quota` response
- Kimi 429 errors were visible in `gateway.err.log` but didn't surface to user
- No connection made between multiple provider failures due to silent failure mode

## Impact
- **Severe UX degradation**: Users don't know if messages were ignored, lost, or system failed
- **Difficult debugging**: Users can't self-diagnose common issues like quota limits
- **False reliability signals**: System appears healthy while actually non-functional
- **Support burden**: Users must contact support for issues they could resolve themselves

## Proposed Solution
1. **Error Event System**: Push error events back over websocket to client
2. **User-Friendly Messages**: Transform technical errors into actionable user messages
3. **Graceful Degradation**: Show partial functionality when some providers work
4. **Retry Hints**: Suggest when users might retry (e.g., "rate limit, try again in X minutes")

## Reproduction
1. Configure clawdbot with providers that will fail (expired keys, exceeded quotas)
2. Send message via webchat
3. Observe: message appears sent, but no response and no error indication
4. Check logs: errors present in `gateway.err.log` but not surfaced to user

## Environment
- clawdbot version: [current]
- Interface: webchat
- Providers tested: OpenAI (quota exceeded), Kimi (rate limited)

---

**Priority: High** - Silent failures are a fundamental UX anti-pattern that undermines user confidence in the system.

### @aviwolf-nz (2026-01-27)

I think I‚Äôm seeing the same issue, but from the Telegram channel.
When OpenAI returns a persistent 429 rate-limit, Moltbot retries (I consistently see 4 retries, matching this issue).
In the web UI this often results in an empty response, but in Telegram the retry/error path appears to be treated as a valid assistant output.
The result is multiple assistant messages for a single user prompt (rate-limit error text and/or near-duplicate replies), all originating from what looks like a single logical request in the OpenAI logs.
This suggests retry handling is leaking into the assistant output layer, and different channels surface it differently.

### @tonydehnke (2026-01-29)

+1 on this. I'm experiencing the same issue with Synthetic (Kimi K2.5) provider.

**Observed behavior:**
- Agent starts processing, typing indicator shows
- Model API hangs silently (no response, no error)
- After 2 minutes, typing TTL expires
- User receives nothing - no message, no error explanation
- Logs show `embedded run tool end` but no subsequent `embedded run end` or API error

**From the logs:**
```
08:16:10 [tools] exec failed: ... (tool error - expected)
08:16:58 embedded run tool end: ...
08:18:58 typing TTL reached (2m); stopping typing indicator
```

No error logged between the tool ending and the typing TTL. The model API request seems to hang indefinitely with no timeout/error handling.

**What would help:**
1. Log when model API requests start (provider, model, request ID)
2. Log API errors with HTTP status, error body, duration
3. Log explicit timeouts with how long the request waited
4. Surface a user-facing message when retries are exhausted (as you suggested)

This would let users distinguish between:
- Provider API down/slow (their issue to report)
- Rate limiting (user needs to check credits)
- Configuration issues (user can fix)

Currently it's impossible to tell what went wrong.


## Links

- None detected yet
