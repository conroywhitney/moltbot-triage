---
number: 6691
title: "[Feature Request] Context-aware dynamic skill loading/unloading to reduce token cost"
author: bellarivialabs
created: 2026-02-01T23:27:12Z
updated: 2026-02-02T00:15:51Z
labels: ["enhancement"]
assignees: []
comments_count: 1
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/6691
duplicate_of: null
related_issues: [1594,1949]
blocks: []
blocked_by: []
---

## Description

## Summary

Add support for dynamically loading and unloading skills during a session based on conversation context, to reduce token costs without sacrificing functionality.

## Background

This is a common pain point in the community:
- Discussion #1949: Users report "burning through $$$" even with Haiku
- Issue #1594: User proposed "vectorizing context and using a search tool on that hot memory"
- Current workarounds: session resets, model switching, `disable-model-invocation`

The existing `disable-model-invocation: true` flag is a partial solution but:
- Requires manual `/command` invocation
- Model loses awareness of the capability
- No context-based auto-loading

## Problem

Skills are snapshotted at session start and remain fixed. This forces a trade-off:
- **Load all skills** â†’ High token cost (~15k+ input tokens with 11 skills)
- **Use `disable-model-invocation`** â†’ Lose context-aware suggestions

## Desired Behavior

```
User: "What's the weather in Vancouver?"
â†’ Detect weather intent â†’ Auto-inject weather skill â†’ Respond

User: "Thanks. Now help me with code..."
â†’ Detect topic shift â†’ Remove weather skill â†’ Continue with lower token cost
```

## Proposed Implementation

### Option A: Intent-based auto-loading (SKILL.md frontmatter)

```yaml
---
name: weather
auto-load-triggers: ["weather", "temperature", "forecast"]
auto-unload-after: 3  # turns without trigger
---
```

### Option B: Skill groups in config

```json5
{
  "skills": {
    "groups": {
      "core": ["voice-call", "summarize"],     // always loaded
      "weather": ["weather", "goplaces"],      // on-demand
      "dev": ["github", "session-logs"]        // on-demand
    },
    "autoLoadTriggers": {
      "weather": ["weather", "forecast"],
      "dev": ["code", "git", "repository"]
    }
  }
}
```

### Option C: Session commands

```
/load weather      # inject into current session
/unload weather    # remove from current session  
/skills            # show currently loaded
```

## Benefits

- 30-50% token reduction for typical conversations
- No functionality loss (skills remain available)
- Compatible with existing `disable-model-invocation` users

## Environment

- OpenClaw version: 2026.1.26
- Platform: Docker on EC2 (Linux)
- 11 bundled skills, ~15k input tokens per request

## Related

- #1949 - Burning through tokens (discussion)
- #1594 - Context dragging (issue)
- Existing partial solution: `disable-model-invocation` in SKILL.md frontmatter

## Comments

### @bellarivialabs (2026-02-01)

cc @steipete â€” Would love your thoughts on this approach. I've been running OpenClaw on EC2 with 11 skills and noticed ~15k input tokens per request. The `disable-model-invocation` flag helps but loses context-awareness. 

A lightweight intent-based loader (Option A) could solve this without major architectural changes. Happy to help test or contribute if there's interest! ðŸ¦ž


## Links

- None detected yet
