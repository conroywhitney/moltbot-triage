---
number: 4785
title: "Using with Ollama feels stupid?"
author: zcode-apps
created: 2026-01-30T17:28:14Z
updated: 2026-01-30T17:28:14Z
labels: []
assignees: []
comments_count: 0
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/4785
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

I've been using OpenClaw via Claude Code until now. Since today, I've connected it to a local Ollama endpoint (gpt:oss:20b) on an RTX 5070 Ti 16GB, but the agent has become so unresponsive that it can't even state its name.

`  "models": {
    "providers": {
      "ollama": {
        "baseUrl": "http://192.168.0.178:11434/v1",
        "api": "openai-completions",
        "models": [
          {
            "id": "gpt-oss:20b",
            "name": "GPT-20B (Ollama)",
            "reasoning": true,
            "input": ["text", "image"],
            "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },
            "contextWindow": 128000,
            "maxTokens": 128000
          }
        ]
      }
    }
  },`
Models are guaranteed to be loaded into the GPU
Have I forgotten anything?

## Comments


## Links

- None detected yet
