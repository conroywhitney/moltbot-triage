---
number: 3475
title: "[Bug]: Kimi/Moonshot OpenAI-compatible models fail silently (direct API works, Clawdbot hangs)"
author: yukiodinson-coder
created: 2026-01-28T17:13:03Z
updated: 2026-02-02T16:49:17Z
labels: ["bug"]
assignees: []
comments_count: 11
reactions_total: 4
url: https://github.com/openclaw/openclaw/issues/3475
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

<pre><code class="language-markdown">## Summary
Kimi models (both `moonshot` and `kimi-code` providers) configured as OpenAI-compatible endpoints fail silently in Clawdbot. Direct API calls via curl work perfectly, but Clawdbot never receives/processes responses.

## Environment
- Clawdbot version: 2026.1.24-3
- OS: Ubuntu Linux
- Channel: Telegram

## Providers configured
```json
"moonshot": {
  "baseUrl": "https://api.moonshot.ai/v1",
  "api": "openai-completions"
}
"kimi-code": {
  "baseUrl": "https://api.kimi.com/coding/v1", 
  "api": "openai-completions"
}
</code></pre>
<h2>Models tested</h2>
<ul>
<li><code>moonshot/kimi-k2.5</code></li>
<li><code>moonshot/kimi-k2-0905-preview</code></li>
<li><code>kimi-code/kimi-for-coding</code></li>
<li><code>kimi-code/kimi-k2.5-thinking</code></li>
</ul>
<h2>Behavior</h2>

Test | Result
-- | --
Direct curl to API | ‚úÖ Works, returns response
Subagent spawn | ‚ùå Hangs forever (startedAt set, never completes)
Main session | ‚ùå Fails silently, falls back to next model


<h2>Reproduction</h2>
<ol>
<li>Configure moonshot or kimi-code provider with valid API key</li>
<li>Spawn subagent with <code>model: "moonshot/kimi-k2.5"</code></li>
<li>Subagent shows <code>totalTokens: 0</code>, empty messages array, never completes</li>
</ol>
<h2>Direct API test (works)</h2>
<pre><code class="language-bash">curl -X POST "https://api.moonshot.ai/v1/chat/completions" \
  -H "Authorization: Bearer $API_KEY" \
  -d '{"model": "kimi-k2.5", "messages": [{"role": "user", "content": "Hi"}]}'
# Returns valid response with reasoning_content + content
</code></pre>
<h2>Notes</h2>
<ul>
<li>API responses include <code>reasoning_content</code> field (chain-of-thought) ‚Äî may need special handling</li>
<li>Both providers exhibit identical behavior</li>
<li>Auth profiles verified correct, API keys work via curl</li>
</ul>

Also here is the logs from the gateway:
 8:06:30 PM
error
[clawdbot] Unhandled promise rejection: Error: Unhandled API in mapOptionsForApi: undefined
    at mapOptionsForApi (file:///home/yuki/.npm-global/lib/node_modules/clawdbot/node_modules/@mariozechner/pi-ai/src/stream.ts:471:10)
    at streamSimple (file:///home/yuki/.npm-global/lib/node_modules/clawdbot/node_modules/@mariozechner/pi-ai/src/stream.ts:218:26)
    at streamAssistantResponse (file:///home/yuki/.npm-global/lib/node_modules/clawdbot/node_modules/@mariozechner/pi-agent-core/src/agent-loop.ts:233:25)
    at runLoop (file:///home/yuki/.npm-global/lib/node_modules/clawdbot/node_modules/@mariozechner/pi-agent-core/src/agent-loop.ts:141:20)
    at file:///home/yuki/.npm-global/lib/node_modules/clawdbot/node_modules/@mariozechner/pi-agent-core/src/agent-loop.ts:51:3

## Comments

### @antymijaljevic (2026-01-28)

Tried to setup Kimi 2.5 model via Openrouter provider. Getting "not whitelisted" even config is valid.

### @eliotgevers (2026-01-28)

I tried to setup kimi k2.5 with openrouter and it fails. It is also not visible under the /models list

### @stewartcelani (2026-01-28)

Every way I try the gateway crashes due to "mapOptiosnForApi" receiving undefined.

One attempt I got the Kimi Code to at least reject me because I wasn't using approved client but I've tried a few different ways now. ü§∑‚Äç‚ôÇÔ∏è

### @varoudis (2026-01-29)

running kimi2.5 with sglang:dev (as per official instruction) I does "work", but I get reasoning token in normal chat and I get error with simple tool calling. 

### @axatbhardwaj (2026-01-29)

any update on this ? , when is it going to be fixed or how ? or maybe i can contribute?

### @sus2790 (2026-01-29)

Isn't a kimi problem, effects to ALL custom provider.

### @grizzdank (2026-01-30)

## Additional findings: Multi-turn tool calling fails with reasoning_content

I've been debugging Kimi K2.5 via OpenRouter and found two specific failure modes:

### Bug 1: kimi-k2.5 ‚Äî reasoning_content replay fails on turn 3+

**Error:**
```
"thinking is enabled but reasoning_content is missing in assistant tool call message at index 2"
```

**Pattern:**
- Turn 1: Kimi responds with `reasoning_content` + tool calls ‚úÖ
- Turn 2: Tool results returned, Kimi responds with reasoning + more tool calls ‚úÖ  
- Turn 3: OpenRouter rejects the request üí•

The issue is that Kimi produces `reasoning_content` by default (even with `reasoning: false` in config). When replaying conversation history, OpenRouter expects `reasoning_content` in ALL assistant messages once it appears in any. The history replay doesn't format this correctly.

**Workaround attempted:** Setting `"reasoning": true` in model config ‚Äî did NOT fix it.

### Bug 2: kimi-k2-thinking ‚Äî Tool calls embedded in thinking text

**Behavior:**
```json
{
  "stopReason": "stop",
  "thinking": "...Let me read the file: <|tool_calls_section_begin|> <|tool_call_begin|> functions.read:2..."
}
```

kimi-k2-thinking embeds tool call syntax inside reasoning content as special tokens (`<|tool_calls_section_begin|>` etc.), which aren't parsed as actual tool calls. Session ends prematurely.

### Evidence

Simple 2-turn tasks (mkdir ‚Üí write file ‚Üí text response) succeed. Complex coding tasks requiring 5+ tool call rounds fail consistently.

Successful session pattern:
- Turn 1: reasoning + exec ‚úÖ
- Turn 2: reasoning + write ‚úÖ
- Turn 3: reasoning + **text only** ‚úÖ (no tool call)

Failing session pattern:
- Turn 1: reasoning + tool calls ‚úÖ
- Turn 2: reasoning + tool calls ‚úÖ
- Turn 3: reasoning + tool calls üí•

### Related

This is the same class of bug as #841 (Gemini thoughtSignature replay) ‚Äî reasoning/thinking content not being sanitized or formatted correctly when replaying history to providers that have specific expectations.

### Environment
- OpenClaw version: 2026.1.29
- Provider: OpenRouter
- Models: `moonshotai/kimi-k2.5`, `moonshotai/kimi-k2-thinking`


### @grizzdank (2026-01-30)

Actually this is an upstream issue with the way Pi is dealing with these calls, an issue and PR exists already: [https://github.com/badlogic/pi-mono/pull/987](https://github.com/badlogic/pi-mono/pull/987). Looks like its waiting on merge while some stuff settles down since its a new model. The Pi PR indicates this is broken at the Openrouter provider level in the way its dealing with this specific model api calls.


### @therealZpoint-bot (2026-02-02)

Confirming this affects tool use broadly, not just subagent spawning.

**Environment:**
- Clawdbot Gateway (latest)
- Provider: `kimi-coding`
- Model: `kimi-k2.5`

**Behavior:**
- Even simple tool calls (like `Read` for file reads) cause output to stop completely
- No error message, just silence after tool invocation
- Session becomes unresponsive and requires model switch + context reset to recover

**Workaround:**
- Switch to different provider (e.g., Gemini Flash)
- Reset context to clear the corrupted tool call state

This suggests the `mapOptionsForApi` issue affects the entire tool-use code path, not just streaming responses. The provider works fine for pure text completions but breaks on any tool invocation.

### @gregjrothwell (2026-02-02)

Additional data point: `reasoning: false` in the model config does **not** suppress `reasoning_content` from the Kimi K2.5 API response. The field is always present regardless of client-side config.

With `max_tokens` too low, Kimi returns `"content": ""` (empty) with all tokens consumed by `reasoning_content`, which Clawdbot interprets as a failure/hang. With sufficient `max_tokens` (~500+), both fields are populated and the API works fine via curl.

Gateway error log shows: `FailoverError: The AI service is temporarily overloaded. Please try again in a moment.` ‚Äî which is misleading; the API isn't overloaded, Clawdbot is just failing to parse the response.

**Workaround status**: No client-side workaround found. The `openai-completions` API handler in Clawdbot needs to handle `reasoning_content` in the response body.

### @gregjrothwell (2026-02-02)

**Update: workaround confirmed working.**

Setting `"reasoning": false` in the model config + increasing `"maxTokens"` to 16384 resolves the issue for `kimi-k2.5`. Clawdbot successfully uses the model as primary via Telegram with this config:

```json
{
  "id": "kimi-k2.5",
  "name": "Kimi K2.5 (262k)",
  "reasoning": false,
  "input": ["text", "image"],
  "contextWindow": 262144,
  "maxTokens": 16384
}
```

It appears that with `reasoning: false`, Clawdbot ignores the `reasoning_content` field and reads `content` directly, which is populated correctly when sufficient `maxTokens` are available. The original `moonshot-v1-128k` with `maxTokens: 4096` was likely too low for Clawdbot's payload + reasoning overhead to produce any `content`.


## Links

- None detected yet
