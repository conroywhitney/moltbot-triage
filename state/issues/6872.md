---
number: 6872
title: "[Feature]: xai support for responses and native tools: x_search, web_search, code_exec"
author: ivelin
created: 2026-02-02T04:27:26Z
updated: 2026-02-02T15:51:41Z
labels: ["enhancement"]
assignees: []
comments_count: 1
reactions_total: 4
url: https://github.com/openclaw/openclaw/issues/6872
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

## Summary

I know this issues has been brought up multiple times before and PRs have been suggested but all have been shut down so far.


Grok 4.1 Fast feels artificially handicapped as compared to the Grok app use. Quite possible because openclaw does not allow configuring the xAI API to use [responses API](https://docs.x.ai/docs/guides/chat#responses-api) and native server side tools:
- web_search - parallel and much faster than client side brave search
- x_search - real time, up to date information - super helpful for fast moving openclaw with constant stream of x updates
- sandboxed code exec - very useful for grounding calculations, timeseries research , charts/visualisation , etc. 

## Proposed solution

Ideally enable native xAI provider via Vercel's AI SDK, which is referenced in the official xAI docs.

## Alternatives considered

If not, then allow the extra config options in the openclaw.json schema for /responses end point and native tools via existing OpenRouter and Vercel Providers.

## Additional context

This is a prompt-style PR as requested by Peter in his podcast appearances.

**What this PR does**  
Extends support for xAI/Grok (building on existing partial integrations like Grok as web_search provider) by adding Responses API usage to the xAI provider config. This enables Grok models (e.g., grok-4.1-fast) to use native server-side tools (web_search, x_search, code_execution, etc.) with autonomous multi-step loops handled by xAI servers.

It keeps everything under a single `"xai"` provider entry in `openclaw.json` â€” no new top-level provider type. Adds config flags to toggle Responses mode and hybrid behavior, so users can opt into native server-side while still using OpenClaw's client-side tools/skills (browser, canvas, nodes, cron, sessions, ClawHub, etc.) when needed.

**Why this is useful**  
- Builds incrementally on current xAI/Grok usage (OpenAI-compat + recent search provider additions) instead of forking provider logic.  
- Unlocks full Grok agentic power (low-latency native tools, 2M context) without breaking existing setups or requiring a separate provider.  
- Enables **hybrid workflows**: native server-side for research/code â†’ client-side OpenClaw tools for local actions â†’ back to Grok.  
- Improves Grok reliability on agentic tasks (vs. pure client-side limitations) while preserving local ecosystem.  
- Backward compatible: standard xAI calls unchanged; Responses is opt-in.

**How to test**  
1. Update/add to `~/.openclaw/openclaw.json` (example below).  
2. Native-only test: `openclaw agent --model xai/grok-4.1-fast "Search recent X posts about OpenClaw and run code to count mentions"` â†’ uses native x_search + code_execution.  
3. Hybrid test: `"Search web for latest OpenClaw skills, install one via ClawHub, then list sessions"` â†’ native web_search â†’ client-side ClawHub/sessions tools.  
4. Disable Responses: set `"useResponsesApi": false` â†’ falls back to standard chat/completions + full client-side tools.

**Config example** (under `"models.providers"`):
```json
"xai": {
  "provider": "xai",                          // or "openai" compat if preferred
  "apiKey": "xai-YourKeyHere",
  "baseUrl": "https://api.x.ai/v1",           // or Vercel proxy: https://gateway.ai.vercel.com/v1/xai
  "models": {
    "default": "grok-4.1-fast",
    "reasoning": "grok-4.1-fast-reasoning"
  },
  "useResponsesApi": true,                    // toggle for native server-side mode
  "tools": {
    "native": {                               // xAI natives (only active when useResponsesApi: true)
      "web_search": { "allowedDomains": [] },
      "x_search": { "allowedXHandles": [] },
      "code_execution": true
    },
    "clientSide": ["browser", "canvas", "nodes", "cron", "sessions_list", "clawhub_install"]  // exposed as custom tools
  },
  "reasoningEffort": "high",                  // optional Responses flag
  "hybrid": true                              // opt-in for mixing
}
```

**Changes overview**  
- Detect `"useResponsesApi": true` in xAI provider config â†’ switch to Responses API endpoint via Vercel AI SDK (`@ai-sdk/xai`). Install: `pnpm add @ai-sdk/xai ai`.  
- Wrapper logic (e.g., extend existing xAI/OpenAI compat handler or add to `src/providers/xai.js`): use `xai.responses(model)` + `generateText`/`streamText` when enabled.  
- Pass native tool configs + custom client-side schemas (from OpenClaw's tool registry/TOOLS.md).  
- Hybrid loop: If Responses returns client-side tool calls â†’ execute locally via agent runtime â†’ feed results back (continue Responses or fallback).  
- Prioritize native tools for speed; fallback/delegate to OpenClaw skills when matched or hybrid disabled.  
- Security: allowlists for natives and client tools; no auto-enable.  
- Fallback: when `"useResponsesApi": false`, use standard `/chat/completions` path.

**Testing status**  
- Lightly tested: native tools work via direct xAI; hybrid prototype (native search â†’ client browser) succeeds manually.  
- Not battle-tested in multi-agent/high-load â€” feedback appreciated on edges.  
- Verified macOS/Linux; no regressions to existing providers/tools.

**AI-assisted notes**  
- This PR / plan was vibe-coded / assisted by Grok 4.1 (xAI).  
- Prompt logs / reasoning available on request (based on xAI docs + Vercel SDK examples).  
- I understand the changes: toggles within existing provider, adds SDK integration, secure hybrid mapping, preserves fallbacks.

**Related issues / motivation**  
- Builds on recent PRs like feat(tools): add Grok (xAI) as web_search provider.  
- No core issue for full Responses/hybrid yet (repo search: "grok", "xai", "responses").  
- Community demand (Discord/X) for better Grok beyond compat mode.  
- Refs: xAI Responses docs (https://docs.x.ai/docs/responses-api), Vercel AI SDK xAI provider, Advanced mixing server/client tools.

Happy to iterate â€” mark as draft if needed. Let's make xAI/Grok shine natively in OpenClaw! ðŸ¦žðŸ¤–

## Comments

### @glittle (2026-02-02)

Yes! We need grok powering more openclaw bots! Here are some thoughts on this: [X Post](https://x.com/glenlittle/status/2018314654720602173)


## Links

- None detected yet
