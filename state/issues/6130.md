---
number: 6130
title: "Telegram: raw audio binary data embedded as text/plain in session context causes prompt overflow"
author: bonquiz
created: 2026-02-01T09:31:11Z
updated: 2026-02-03T00:01:46Z
labels: ["bug"]
assignees: []
comments_count: 2
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/6130
duplicate_of: null
related_issues: [6068]
blocks: []
blocked_by: []
---

## Description

## Summary

When a Telegram voice message (OGG/Opus) is received, the raw binary audio data is embedded directly into the session context as `text/plain`. This causes massive token inflation, as binary data gets tokenized as hundreds of thousands of garbage tokens.

## Impact

A single 13-second voice note creates a ~440KB session entry. When tokenized, this can produce 200,000–600,000 tokens of binary garbage, exceeding Claude's 200k token context limit and causing silent delivery failures (agent gets 400 error, user sees typing indicator but never receives a response).

## Evidence

Session log showing repeated `prompt is too long` errors across a single day:

```
07:07 UTC → 501,890 tokens (max 200,000)
07:08 UTC → 482,720 tokens
08:23 UTC → 639,302 tokens
09:16 UTC → 410,635 tokens
```

The user message entry for a 13-second voice note:
- Session entry size: **448,051 bytes** (438 KB)
- Content includes: transcript (correct) + raw OGG binary embedded as `<file name="...ogg" mime="text/plain">`

## Expected Behavior

Voice messages should include only:
- The transcript text
- A file reference/path (not the binary content)

The raw audio binary should never be inlined as text in the session prompt.

## Environment

- OpenClaw version: 2026.1.30
- Node: v22.22.0
- Channel: Telegram (long-polling)
- Model: anthropic/claude-opus-4-5
- TTS config: `messages.tts.auto: "inbound"`

## Workaround

- Enable `contextPruning` with `mode: "cache-ttl"` and `hardClear.enabled: true` to trim old tool results
- Auto-compaction helps but cannot fix single user messages that exceed the model limit
- Session reset (`/new`) when context is bloated

## Related

- #6068 (Telegram voice caption overflow)

## Comments

### @bonquiz (2026-02-01)

## Source Code Analysis (fresh install, v2026.1.30)

The root cause is in `dist/media-understanding/apply.js`:

### 1. MIME type misclassification
Telegram voice messages (OGG/Opus) arrive with `mime="text/plain"` instead of `audio/ogg`. This appears to happen upstream in the attachment pipeline before media-understanding runs.

### 2. Permissive text/* filter (line ~230)
```javascript
if (mimeType.startsWith("text/")) {
    allowedMimes.add(mimeType);
}
```
Any file with a `text/*` MIME type is automatically allowed for inline embedding, regardless of actual file content.

### 3. Binary content embedded as text (line ~273)
```javascript
blocks.push(`<file name="${xmlEscapeAttr(safeName)}" mime="${xmlEscapeAttr(mimeType)}">\n${blockText}\n</file>`);
```
The raw binary OGG data is then inserted as a `<file>` block directly into the session prompt.

### Reproduction
This was reproduced on a **completely fresh installation** (new user, new config, new workspace — zero carryover from any prior setup):
- OpenClaw v2026.1.30
- Node v22.22.0
- Config created: 2026-02-01T10:40:53Z (verified via `stat` birth timestamp)
- No prior session data or config migration

### Suggested fix
Either:
1. Fix the MIME type detection for Telegram voice messages to return `audio/ogg` instead of `text/plain`
2. Add a binary content guard before the `text/*` filter (e.g., check for null bytes or valid UTF-8)
3. Exclude known audio extensions (`.ogg`, `.opus`, `.mp3`) from the text/* passthrough

### @mac-110 (2026-02-03)

**+1 on this issue** — experiencing the exact same problem.

### Our findings

A 10-second voice message creates ~15-20k tokens of binary garbage vs ~50 tokens for the actual transcribed text. That's a **300x overhead**.

### Enhancement suggestion

Beyond fixing the MIME type detection, OpenClaw could benefit from a **local pre-transcription option**:

1. Audio message received (Telegram/Signal/etc.)
2. Transcribe locally using something like `parakeet-mlx` (Nvidia Parakeet V3, runs great on Apple Silicon) or `mlx-whisper`
3. Only send the transcript text to the LLM — never the binary

This would:
- Eliminate token waste entirely
- Reduce API costs
- Work offline (no Whisper API calls needed)
- Be significantly faster

We've tested Parakeet V3 locally on an M4 Mac Mini — transcription is real-time with excellent quality (6% WER, better than Whisper).

Happy to help test or contribute a PR if there's interest in this approach.


## Links

- None detected yet
