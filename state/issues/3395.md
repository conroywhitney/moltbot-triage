---
number: 3395
title: "Feature: Lazy loading for skills prompt injection"
author: cryptosquanch
created: 2026-01-28T15:04:06Z
updated: 2026-01-28T15:08:47Z
labels: ["enhancement"]
assignees: []
comments_count: 0
reactions_total: 0
url: https://github.com/openclaw/openclaw/issues/3395
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

# Feature: Lazy loading for skills prompt injection

## Problem

Currently, the entire `<available_skills>` XML block is injected into every system prompt, regardless of whether skills will be used. With 40+ skills installed, this consumes ~3,700 tokens per request (~14,700 characters).

For a typical session with 100 messages, this adds up to **~370,000 tokens** of overhead.

## Current Behavior

```
<available_skills>
  <skill>
    <name>weather</name>
    <description>Get current weather and forecasts...</description>
    <location>/path/to/SKILL.md</location>
  </skill>
  <!-- 40+ more skills -->
</available_skills>
```

This block is injected in `buildWorkspaceSkillsPrompt()` via `formatSkillsForPrompt()`.

## Proposed Solution

Add a `skills.promptMode` config option:

```yaml
skills:
  promptMode: full | compact | lazy  # default: full (current behavior)
```

### Modes

1. **full** (default): Current behavior - inject all skill metadata
2. **compact**: Inject only skill names and one-line descriptions (~50% reduction)
3. **lazy**: No upfront injection. Add a `list_skills` tool that returns available skills on demand

### Implementation Notes

For `lazy` mode:
- Add `list_skills` tool to the tools array
- Tool returns skill names and descriptions when called
- Model can then `read` the specific SKILL.md as needed
- Reduces per-message overhead to near zero for skill-agnostic conversations

## Benefits

- Significant token savings for heavy skill users
- Delayed compaction (longer conversations within budget)
- Same cost = more messages
- No breaking changes (opt-in via config)

## Comments


## Links

- None detected yet
