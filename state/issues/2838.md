---
number: 2838
title: "[Feature]: Add and configure vLLM and Ollama as first-class model providers"
author: PYTHON01100100
created: 2026-01-27T15:49:29Z
updated: 2026-01-29T11:54:17Z
labels: ["enhancement"]
assignees: []
comments_count: 4
reactions_total: 3
url: https://github.com/moltbot/moltbot/issues/2838
duplicate_of: null
related_issues: []
blocks: []
blocked_by: []
---

## Description

Clawdbot currently supports model providers, but configuring local inference engines such as vLLM and Ollama is not straightforward or fully documented. Users running local LLMs (GPU / on-prem / WSL) face friction when attempting to integrate these providers reliably.

Adding official support for vLLM and Ollama as first-class providers would significantly improve local deployment, performance, and developer experience.

## Summary

Clawdbot currently supports model providers, but configuring local inference engines such as vLLM and Ollama is not straightforward or fully documented. Users running local LLMs (GPU / on-prem / WSL) face friction when attempting to integrate these providers reliably.

Adding official support for vLLM and Ollama as first-class providers would significantly improve local deployment, performance, and developer experience.

## Proposed solution

Add built-in support and configuration helpers for:

Ollama provider

Define baseUrl, models, contextWindow, and maxTokens

Support OpenAI-compatible /v1/chat/completions

Auto-detect local models (e.g. qwen3:8b)

Clear validation errors and examples

vLLM provider

Support OpenAI-compatible vLLM endpoints

Allow configuration of:

served-model-name

contextWindow

maxTokens

GPU / batching options

Enable seamless switching between cloud and local models

CLI / config improvements:

clawdbot models add --provider ollama|vllm

Better schema validation errors

Example configs in docs

## Alternatives considered

Manually configuring models via generic OpenAI-compatible providers
→ Works partially, but leads to unclear errors, missing validation, and poor UX.

External proxy layers
→ Adds unnecessary complexity for users already running vLLM or Ollama locally.

## Comments

### @JamesMowery (2026-01-28)

I'm also having issues getting this working with llama.cpp. I'm hitting the endpoint correctly and it's number crunching, but the responses I get back in the chat UI are just blank.

Tested with gemma3, qwen3 vl, glm 4.7 flash, and all have the same results.

<img width="1376" height="698" alt="Image" src="https://github.com/user-attachments/assets/fcde8f92-fd05-4455-88aa-6f18887220c9" />

Not seeing any errors in the moltbot logs.

My models config if helpful (I use port 8081):

```
"models": {
    "providers": {
      "openai": {
        "baseUrl": "http://192.168.0.101:8081/v1",
        "apiKey": "localfirst",
        "auth": "api-key",
        "api": "openai-responses",
        "models": [
          {
            "id": "openai/Qwen3-VL-30B-A3B-Instruct-UD-Q5KXL",
            "name": "Qwen3-VL-30B-A3B-Instruct-UD-Q5KXL",
            "api": "openai-responses",
            "reasoning": false,
            "input": [
              "text"
            ],
            "cost": {
              "input": 0,
              "output": 0,
              "cacheRead": 0,
              "cacheWrite": 0
            },
            "contextWindow": 102800,
            "maxTokens": 9046
          }
        ]
      }
    }
  },
```

### @zycer (2026-01-29)

`openai-responses` does not support tool calling and must be switched to `openai-completions`. However, Clawdbot’s request parameters are not fully compatible with vLLM’s `/v1/chat/completions`  API, which results in HTTP 400 errors.

<img width="1401" height="799" alt="Image" src="https://github.com/user-attachments/assets/291095fd-5937-46e3-8819-b23e917b479b" />

### @scottybo (2026-01-29)

Would be great to get this running with a local LLM. I'm on a DGX Spark, with vLLM running gpt-oss-120b

### @BahamutRU (2026-01-29)

Ollama is bad practice and worst software.
Better clear llama.cpp support.
It is fully working at this moment?


## Links

- None detected yet
